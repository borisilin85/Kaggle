{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67f1ef572ea1f522bcdc6a775bf629ee740d87ac"
   },
   "source": [
    "# Bayesian global optimization with gaussian processes for finding (sub-)optimal parameters of LightGBM\n",
    "\n",
    "As many of fellow kaggler asking how did I get LightGBM parameters for the kernel [Customer Transaction Prediction](https://www.kaggle.com/fayzur/customer-transaction-prediction) I published. So, I decided to publish a kernel to optimize parameters. \n",
    "\n",
    "\n",
    "\n",
    "In this kernel I use Bayesian global optimization with gaussian processes for finding optimal parameters. This optimization attempts to find the maximum value of an black box function in as few iterations as possible. In our case the black box function will be a function that I will write to optimize (maximize) the evaluation function (AUC) so that parameters get maximize AUC in training and validation, and expect to do good in the private. The final prediction will be **rank average on 5 fold cross validation predictions**.\n",
    "\n",
    "Continue to the end of this kernel and **upvote it if you find it is interesting**.\n",
    "\n",
    "![image.jpg](https://i.imgur.com/XKS1oqU.jpg)\n",
    "\n",
    "Image taken from : https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7409352edc21584713b1225028f087c6989e94be"
   },
   "source": [
    "## Notebook  Content\n",
    "0. [Installing Bayesian global optimization library](#0) <br>    \n",
    "1. [Loading the data](#1)\n",
    "2. [Black box function to be optimized (LightGBM)](#2)\n",
    "3. [Training LightGBM model](#3)\n",
    "4. [Rank averaging](#4)\n",
    "5. [Submission](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "## 0. Installing Bayesian global optimization library\n",
    "\n",
    "Let's install the latest release from pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "81c2a7386319cae39c1cf83d39394d530f549128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bayesian-optimization) (1.16.1)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bayesian-optimization) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bayesian-optimization) (0.20.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import rankdata\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f5624fd428106888ec023312d3479d324ef0eac9"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:\\\\PythonScripts\\\\Kaggle\\\\dont_overfitt\\\\train.csv')\n",
    "\n",
    "test_df = pd.read_csv('C:\\\\PythonScripts\\\\Kaggle\\\\dont_overfitt\\\\test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7365ac9a050f611cb284bbb47519e04ac1ee19f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>...</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>1.825</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>1.864</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-1.927</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>1.763</td>\n",
       "      <td>1.449</td>\n",
       "      <td>-1.097</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-1.859</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.009</td>\n",
       "      <td>-2.296</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>1.528</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-1.078</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.405</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.579</td>\n",
       "      <td>2.929</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.717</td>\n",
       "      <td>-1.542</td>\n",
       "      <td>-1.847</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.238</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-1.891</td>\n",
       "      <td>-1.531</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>2.072</td>\n",
       "      <td>0.946</td>\n",
       "      <td>-1.105</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.933</td>\n",
       "      <td>-1.410</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>1.740</td>\n",
       "      <td>-1.504</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-1.551</td>\n",
       "      <td>-1.415</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>0.796</td>\n",
       "      <td>-2.464</td>\n",
       "      <td>-1.424</td>\n",
       "      <td>1.230</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>1.851</td>\n",
       "      <td>1.292</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>1.318</td>\n",
       "      <td>1.146</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>2.227</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.870</td>\n",
       "      <td>1.420</td>\n",
       "      <td>-1.675</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.768</td>\n",
       "      <td>2.563</td>\n",
       "      <td>0.638</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.407</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.017</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>1.906</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.609</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>-2.352</td>\n",
       "      <td>-1.661</td>\n",
       "      <td>0.498</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>0.907</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.817</td>\n",
       "      <td>1.372</td>\n",
       "      <td>1.187</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>0.253</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.413</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.655</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-1.730</td>\n",
       "      <td>1.132</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.039</td>\n",
       "      <td>1.489</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-2.851</td>\n",
       "      <td>-1.021</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-1.892</td>\n",
       "      <td>0.203</td>\n",
       "      <td>2.174</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>-1.053</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-1.109</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>1.250</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>-1.318</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>2.457</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.569</td>\n",
       "      <td>-1.320</td>\n",
       "      <td>-1.516</td>\n",
       "      <td>-2.145</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-1.049</td>\n",
       "      <td>-1.125</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.617</td>\n",
       "      <td>1.253</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-1.793</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>2.907</td>\n",
       "      <td>1.085</td>\n",
       "      <td>2.144</td>\n",
       "      <td>1.540</td>\n",
       "      <td>0.584</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.098</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>1.382</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-1.519</td>\n",
       "      <td>0.619</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.866</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.238</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>-2.721</td>\n",
       "      <td>1.659</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>1.719</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-1.320</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-1.102</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-1.082</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.472</td>\n",
       "      <td>1.315</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.633</td>\n",
       "      <td>1.221</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.133</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-2.144</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.813</td>\n",
       "      <td>1.966</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.195</td>\n",
       "      <td>-0.799</td>\n",
       "      <td>1.117</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-1.634</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.610</td>\n",
       "      <td>-1.822</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>1.188</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>1.914</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-1.495</td>\n",
       "      <td>1.787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-1.194</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.066</td>\n",
       "      <td>1.005</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.329</td>\n",
       "      <td>1.213</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>1.299</td>\n",
       "      <td>0.850</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.793</td>\n",
       "      <td>-1.470</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-2.254</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.504</td>\n",
       "      <td>1.500</td>\n",
       "      <td>-1.160</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>-1.151</td>\n",
       "      <td>1.764</td>\n",
       "      <td>1.307</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-1.234</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1.470</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-2.015</td>\n",
       "      <td>-1.258</td>\n",
       "      <td>0.630</td>\n",
       "      <td>1.158</td>\n",
       "      <td>0.971</td>\n",
       "      <td>-1.489</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.917</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>0.887</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.583</td>\n",
       "      <td>1.267</td>\n",
       "      <td>-1.667</td>\n",
       "      <td>-2.771</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>1.312</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.932</td>\n",
       "      <td>2.064</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1.215</td>\n",
       "      <td>2.012</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>1.121</td>\n",
       "      <td>1.333</td>\n",
       "      <td>0.211</td>\n",
       "      <td>1.753</td>\n",
       "      <td>0.053</td>\n",
       "      <td>1.274</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.359</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.352</td>\n",
       "      <td>1.095</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-1.044</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-1.658</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>1.786</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-1.223</td>\n",
       "      <td>2.273</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-2.032</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.924</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>1.896</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>1.920</td>\n",
       "      <td>-1.244</td>\n",
       "      <td>-1.704</td>\n",
       "      <td>0.167</td>\n",
       "      <td>1.088</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.972</td>\n",
       "      <td>-1.554</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-2.677</td>\n",
       "      <td>-1.528</td>\n",
       "      <td>0.613</td>\n",
       "      <td>-1.269</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-1.025</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.923</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.847</td>\n",
       "      <td>-0.807</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.593</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>1.910</td>\n",
       "      <td>0.837</td>\n",
       "      <td>-1.252</td>\n",
       "      <td>1.492</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>0.355</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>1.527</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>0.533</td>\n",
       "      <td>1.020</td>\n",
       "      <td>-1.488</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-1.476</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.857</td>\n",
       "      <td>-1.796</td>\n",
       "      <td>2.540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>-1.636</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>0.948</td>\n",
       "      <td>1.670</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>1.662</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.269</td>\n",
       "      <td>1.873</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-1.181</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-1.391</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.773</td>\n",
       "      <td>-1.234</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.815</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>2.342</td>\n",
       "      <td>0.779</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>-2.330</td>\n",
       "      <td>2.158</td>\n",
       "      <td>2.165</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-2.269</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>1.059</td>\n",
       "      <td>0.483</td>\n",
       "      <td>2.470</td>\n",
       "      <td>1.459</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>1.074</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>1.086</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>0.432</td>\n",
       "      <td>1.345</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-1.602</td>\n",
       "      <td>-0.727</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.780</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>-1.122</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>2.535</td>\n",
       "      <td>-1.045</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.373</td>\n",
       "      <td>0.456</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>1.381</td>\n",
       "      <td>1.843</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.726</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-1.035</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.335</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-1.010</td>\n",
       "      <td>1.048</td>\n",
       "      <td>-1.442</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.836</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.716</td>\n",
       "      <td>-0.764</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-1.308</td>\n",
       "      <td>2.127</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>1.854</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-1.171</td>\n",
       "      <td>2.798</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-1.048</td>\n",
       "      <td>1.078</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-2.241</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.423</td>\n",
       "      <td>1.070</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>1.320</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-1.096</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-1.133</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>0.645</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>-1.458</td>\n",
       "      <td>1.629</td>\n",
       "      <td>-1.112</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>1.116</td>\n",
       "      <td>0.272</td>\n",
       "      <td>1.100</td>\n",
       "      <td>-0.811</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.312</td>\n",
       "      <td>1.848</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>0.290</td>\n",
       "      <td>1.188</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>1.103</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>0.863</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-1.108</td>\n",
       "      <td>-1.151</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>...</td>\n",
       "      <td>1.477</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-1.730</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.656</td>\n",
       "      <td>1.259</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-1.561</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.505</td>\n",
       "      <td>1.410</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-1.029</td>\n",
       "      <td>-1.382</td>\n",
       "      <td>1.350</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.168</td>\n",
       "      <td>1.069</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-1.878</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.059</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.899</td>\n",
       "      <td>1.496</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>1.176</td>\n",
       "      <td>1.852</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.350</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.795</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.814</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>1.495</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>1.251</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-1.215</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>2.725</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.845</td>\n",
       "      <td>-1.226</td>\n",
       "      <td>1.527</td>\n",
       "      <td>-1.701</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.864</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>1.282</td>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>1.020</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-1.574</td>\n",
       "      <td>-1.618</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.347</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.509</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>2.198</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.494</td>\n",
       "      <td>1.478</td>\n",
       "      <td>-1.412</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-1.312</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>1.042</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>1.656</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-1.437</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>-1.739</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-1.102</td>\n",
       "      <td>2.371</td>\n",
       "      <td>0.554</td>\n",
       "      <td>1.173</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>1.528</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>2.054</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-1.328</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.666</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-1.016</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>1.556</td>\n",
       "      <td>1.138</td>\n",
       "      <td>2.066</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>-1.172</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.357</td>\n",
       "      <td>1.626</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>1.717</td>\n",
       "      <td>-1.424</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.732</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>1.246</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-1.058</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.663</td>\n",
       "      <td>-1.218</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>1.333</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-1.906</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.527</td>\n",
       "      <td>1.438</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.742</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-1.806</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.093</td>\n",
       "      <td>1.960</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-1.003</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.210</td>\n",
       "      <td>2.099</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.693</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-1.096</td>\n",
       "      <td>-0.728</td>\n",
       "      <td>0.842</td>\n",
       "      <td>1.914</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.169</td>\n",
       "      <td>1.273</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.904</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>3.432</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-1.517</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>-1.830</td>\n",
       "      <td>1.408</td>\n",
       "      <td>2.319</td>\n",
       "      <td>1.704</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>1.014</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>1.845</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.415</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.246</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target      0      1      2      3      4      5      6      7      8  \\\n",
       "0   0     1.0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276 -2.246   \n",
       "1   1     0.0  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  0.004   \n",
       "2   2     1.0 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  0.137   \n",
       "3   3     1.0  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  0.503   \n",
       "4   4     1.0  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509 -0.012   \n",
       "\n",
       "       9     10     11     12     13     14     15     16     17     18  \\\n",
       "0  1.825 -0.912 -0.107  0.305  0.102  0.826  0.417  0.177 -0.673 -0.503   \n",
       "1 -0.291  2.907  1.085  2.144  1.540  0.584  1.133  1.098 -0.237 -0.498   \n",
       "2  0.183  0.459  0.478 -0.425  0.352  1.095  0.300 -1.044  0.270 -1.038   \n",
       "3  0.274  0.335 -1.148  0.067 -1.010  1.048 -1.442  0.210  0.836 -0.326   \n",
       "4  2.198  0.190  0.453  0.494  1.478 -1.412  0.270 -1.312 -0.322 -0.688   \n",
       "\n",
       "      19     20     21     22     23     24     25     26     27     28  \\\n",
       "0  1.864  0.410 -1.927  0.102 -0.931  1.763  1.449 -1.097 -0.686 -0.250   \n",
       "1  0.283 -1.100 -0.417  1.382 -0.515 -1.519  0.619 -0.128  0.866 -0.540   \n",
       "2  0.144 -1.658 -0.946  0.633 -0.772  1.786  0.136 -0.103 -1.223  2.273   \n",
       "3  0.716 -0.764  0.248 -1.308  2.127  0.365  0.296 -0.808  1.854  0.118   \n",
       "4 -0.198 -0.285  1.042 -0.315 -0.478  0.024 -0.190  1.656 -0.469 -1.437   \n",
       "\n",
       "      29     30     31     32     33     34     35     36     37     38  \\\n",
       "0 -1.859  1.125  1.009 -2.296  0.385 -0.876  1.528 -0.144 -1.078 -0.403   \n",
       "1  1.238 -0.227  0.269 -0.390 -2.721  1.659  0.106 -0.121  1.719  0.411   \n",
       "2  0.055 -2.032 -0.452  0.064  0.924 -0.692 -0.067 -0.917  1.896 -0.152   \n",
       "3  0.380  0.999 -1.171  2.798  0.394 -1.048  1.078  0.401 -0.486 -0.732   \n",
       "4 -0.581 -0.308 -0.837 -1.739  0.037  0.336 -1.102  2.371  0.554  1.173   \n",
       "\n",
       "      39     40     41     42     43     44     45     46     47     48  \\\n",
       "0  0.005  1.405 -0.044 -0.458  0.579  2.929  0.833  0.761  0.737  0.669   \n",
       "1 -0.303 -0.307  0.380  0.503 -1.320  0.339 -1.102 -0.947  0.267  0.695   \n",
       "2  1.920 -1.244 -1.704  0.167  1.088  0.068  0.972 -1.554  0.218 -2.677   \n",
       "3 -2.241 -0.193  0.336  0.009  0.423  1.070 -0.861  1.320 -0.976 -1.096   \n",
       "4 -0.122  1.528 -1.220  2.054 -0.318 -0.445  0.344  0.161  0.830 -1.328   \n",
       "\n",
       "      49     50     51     52     53     54     55     56     57     58  \\\n",
       "0  0.717 -1.542 -1.847 -0.445  1.238 -0.840 -1.891 -1.531 -0.396 -0.927   \n",
       "1  0.167  0.188 -1.082 -0.872  0.660  0.051  0.303 -0.553 -0.771  0.588   \n",
       "2 -1.528  0.613 -1.269  0.516 -0.714 -0.347 -1.025  1.340  0.923 -0.071   \n",
       "3 -0.912  0.548  0.924  0.053  0.570  0.508 -0.717 -1.133 -0.723  0.645   \n",
       "4  0.420  0.666 -0.212 -1.016 -0.312  0.620  0.807  0.301 -0.342  1.556   \n",
       "\n",
       "      59     60     61     62     63     64     65     66     67     68  \\\n",
       "0  2.072  0.946 -1.105  0.008  0.933 -1.410 -0.770  1.740 -1.504 -0.391   \n",
       "1  0.472  1.315 -0.467 -0.064  1.808  0.633  1.221  1.112  1.133 -0.543   \n",
       "2  0.552  0.837  0.847 -0.807 -0.091  1.424  0.943  0.333  0.593 -0.544   \n",
       "3 -1.083  0.287 -0.396  0.178 -0.421  0.196 -0.706 -1.458  1.629 -1.112   \n",
       "4  1.138  2.066 -0.755 -1.172  0.679 -0.787  0.357  1.626 -0.142  1.717   \n",
       "\n",
       "      69     70     71     72     73     74     75     76     77     78  \\\n",
       "0 -1.551 -1.415 -0.974  0.796 -2.464 -1.424  1.230  0.219  0.130 -0.371   \n",
       "1 -2.144  0.151 -0.813  1.966 -1.190  0.190 -0.473  0.002  1.195 -0.799   \n",
       "2  0.154 -1.081  0.409 -0.964  1.910  0.837 -1.252  1.492 -0.971  0.355   \n",
       "3 -0.479 -0.264  0.205  1.092  0.606 -0.276  1.116  0.272  1.100 -0.811   \n",
       "4 -1.424  0.432  0.732 -0.433 -0.937 -0.473  1.246 -0.930  0.350  0.083   \n",
       "\n",
       "      79     80     81     82     83     84     85     86     87     88  \\\n",
       "0 -0.930  1.851  1.292 -0.380  1.318  1.146 -0.399  2.227  0.447  0.870   \n",
       "1  1.117 -0.759 -0.661  0.406 -0.846 -0.035 -1.634 -0.011  0.503  0.610   \n",
       "2  1.079  0.758 -0.031 -0.101  1.527 -0.942 -0.496 -0.572  0.533  1.020   \n",
       "3  0.037  0.030  0.312  1.848  0.455 -0.934  0.739  0.286 -0.860  0.290   \n",
       "4 -1.058 -0.187 -0.932 -0.054 -0.289  0.663 -1.218 -0.134  1.333 -0.115   \n",
       "\n",
       "      89     90     91     92     93     94     95     96     97  ...    200  \\\n",
       "0  1.420 -1.675  0.019  0.060  0.768  2.563  0.638  1.164  0.407  ... -2.017   \n",
       "1 -1.822 -0.030  1.188 -0.006 -0.279  1.914  0.620 -1.495  1.787  ... -0.551   \n",
       "2 -1.488  0.696  0.269 -1.476  0.545  0.636  0.857 -1.796  2.540  ...  0.968   \n",
       "3  1.188 -0.604  1.103 -1.823  0.863 -0.447 -1.108 -1.151 -0.919  ...  1.477   \n",
       "4  0.218 -1.906  0.892  0.475  0.313  0.518  0.114  0.527  1.438  ... -1.221   \n",
       "\n",
       "     201    202    203    204    205    206    207    208    209    210  \\\n",
       "0 -0.485  1.906 -0.119  0.609 -0.564  0.264 -0.604 -0.733 -2.352 -1.661   \n",
       "1  0.003 -0.344 -1.194 -0.106 -0.679  0.009  0.372  0.025  0.066  1.005   \n",
       "2 -0.738 -1.636 -0.533 -0.353  0.635  0.386 -1.081  0.161 -0.791  0.948   \n",
       "3  1.020  0.351  0.186 -0.037 -1.730  0.786  0.656  1.259  0.469 -1.561   \n",
       "4  0.554 -0.137 -0.174  0.567  0.648 -0.739 -0.143  0.742 -0.572 -0.369   \n",
       "\n",
       "     211    212    213    214    215    216    217    218    219    220  \\\n",
       "0  0.498 -0.841  0.907 -0.476  0.817  1.372  1.187  0.844  0.028  0.029   \n",
       "1 -0.822  0.468  0.413  0.004  0.329  1.213  0.216  0.584 -0.761 -0.151   \n",
       "2  1.670 -0.309  1.662 -0.053  0.307 -0.220  0.269  1.873 -0.395  0.186   \n",
       "3 -0.719 -1.040  0.142  0.505  1.410  1.042  0.066  0.340 -1.029 -1.382   \n",
       "4  0.910 -1.806 -0.686  0.093  1.960 -0.413  0.110 -0.657  0.530 -1.003   \n",
       "\n",
       "     221    222    223    224    225    226    227    228    229    230  \\\n",
       "0 -0.808  0.253  1.005  1.413 -0.133  0.655 -0.921  0.231 -1.902 -0.005   \n",
       "1 -0.175 -0.603  0.007  0.075 -0.354 -0.124  1.299  0.850 -0.318 -0.141   \n",
       "2  0.163 -0.118  0.129  0.301 -0.125 -1.181 -0.671 -0.303 -0.541 -0.285   \n",
       "3  1.350  0.294  0.036 -0.640  0.168  1.069 -0.235  0.327 -1.878  0.900   \n",
       "4  0.222  1.210  2.099  0.527  0.128  0.204  0.796  0.507 -0.126 -0.660   \n",
       "\n",
       "     231    232    233    234    235    236    237    238    239    240  \\\n",
       "0 -1.730  1.132 -0.194  0.039  1.489 -0.328  0.966 -0.057 -0.181  0.723   \n",
       "1  0.154 -0.441 -0.024  0.793 -1.470  0.386 -2.254 -0.463  0.366 -0.676   \n",
       "2 -0.226  0.751 -1.391 -0.906  0.933  0.773 -1.234 -0.967 -0.010 -0.815   \n",
       "3  1.059 -0.458  1.006  0.898  0.955  0.118  0.054  0.347  0.507  0.526   \n",
       "4 -0.628 -0.453  0.953 -0.993  0.518  0.055  0.159  0.625  0.024 -0.048   \n",
       "\n",
       "     241    242    243    244    245    246    247    248    249    250  \\\n",
       "0 -0.313 -0.165 -0.803  0.074 -2.851 -1.021 -0.894  0.967  0.218 -0.692   \n",
       "1  0.071  0.504  1.500 -1.160 -0.187 -0.430 -1.151  1.764  1.307 -0.731   \n",
       "2  1.000 -0.569 -0.486  2.342  0.779 -0.548 -2.330  2.158  2.165 -0.945   \n",
       "3  0.899  1.496 -0.447  1.176  1.852 -0.001 -0.414  1.350  0.027  0.795   \n",
       "4 -0.693 -0.492 -0.670 -0.233 -1.096 -0.728  0.842  1.914  1.490 -0.462   \n",
       "\n",
       "     251    252    253    254    255    256    257    258    259    260  \\\n",
       "0 -0.514  0.754 -1.892  0.203  2.174 -0.755 -1.053 -0.516 -1.109 -0.681   \n",
       "1 -1.234  0.960  1.470  0.652  0.483 -2.015 -1.258  0.630  1.158  0.971   \n",
       "2 -2.269  0.678  0.468 -0.405  1.059  0.483  2.470  1.459 -0.511 -0.540   \n",
       "3 -0.056 -0.497  0.814 -1.114 -0.800  1.495 -0.591  0.530 -0.528 -0.083   \n",
       "4 -0.767 -0.191  0.169  1.273 -0.160  0.393  0.231 -0.906  0.348 -1.050   \n",
       "\n",
       "     261    262    263    264    265    266    267    268    269    270  \\\n",
       "0  1.250 -0.565 -1.318 -0.923  0.075 -0.704  2.457  0.771 -0.460  0.569   \n",
       "1 -1.489  0.530  0.917 -0.094 -1.407  0.887 -0.104 -0.583  1.267 -1.667   \n",
       "2 -0.299  1.074 -0.748  1.086 -0.766 -0.931  0.432  1.345 -0.491 -1.602   \n",
       "3 -0.831  1.251 -0.206 -0.933 -1.215  0.281  0.512 -0.424  0.769  0.223   \n",
       "4 -0.347  0.904 -1.324 -0.849  3.432  0.222  0.416  0.174 -1.517 -0.337   \n",
       "\n",
       "     271    272    273    274    275    276    277    278    279    280  \\\n",
       "0 -1.320 -1.516 -2.145 -1.120  0.156  0.820 -1.049 -1.125  0.484  0.617   \n",
       "1 -2.771 -0.516  1.312  0.491  0.932  2.064  0.422  1.215  2.012  0.043   \n",
       "2 -0.727  0.346  0.780 -0.527 -1.122 -0.208 -0.730 -0.302  2.535 -1.045   \n",
       "3 -0.710  2.725  0.176  0.845 -1.226  1.527 -1.701  0.597  0.150  1.864   \n",
       "4  0.055 -0.464  0.014 -1.073  0.325 -0.523 -0.692  0.190 -0.883 -1.830   \n",
       "\n",
       "     281    282    283    284    285    286    287    288    289    290  \\\n",
       "0  1.253  1.248  0.504 -0.802 -0.896 -1.793 -0.284 -0.601  0.569  0.867   \n",
       "1 -0.307 -0.059  1.121  1.333  0.211  1.753  0.053  1.274 -0.612 -0.165   \n",
       "2  0.037  0.020  1.373  0.456 -0.277  1.381  1.843  0.749  0.202  0.013   \n",
       "3  0.322 -0.214  1.282  0.408 -0.910  1.020 -0.299 -1.574 -1.618 -0.404   \n",
       "4  1.408  2.319  1.704 -0.723  1.014  0.064  0.096 -0.775  1.845  0.898   \n",
       "\n",
       "     291    292    293    294    295    296    297    298    299  \n",
       "0  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
       "1 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
       "2  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
       "3  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
       "4  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "93aa6148650a23671d5f01a834f948c5e6721234"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>...</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-1.033</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>1.291</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>1.848</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.764</td>\n",
       "      <td>1.860</td>\n",
       "      <td>0.262</td>\n",
       "      <td>1.112</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-1.039</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-1.313</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.244</td>\n",
       "      <td>1.072</td>\n",
       "      <td>-1.003</td>\n",
       "      <td>0.832</td>\n",
       "      <td>-1.075</td>\n",
       "      <td>1.988</td>\n",
       "      <td>1.201</td>\n",
       "      <td>-2.065</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.732</td>\n",
       "      <td>1.235</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.788</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-1.625</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>1.556</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.645</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-2.174</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>-1.092</td>\n",
       "      <td>0.917</td>\n",
       "      <td>-1.010</td>\n",
       "      <td>-1.021</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>1.732</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-1.694</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-0.721</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-1.803</td>\n",
       "      <td>1.295</td>\n",
       "      <td>-1.031</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.198</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.740</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-1.011</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-1.585</td>\n",
       "      <td>0.532</td>\n",
       "      <td>-1.201</td>\n",
       "      <td>1.210</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>1.578</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.106</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-2.007</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-1.333</td>\n",
       "      <td>-1.102</td>\n",
       "      <td>2.145</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-1.382</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.890</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>0.340</td>\n",
       "      <td>2.345</td>\n",
       "      <td>2.748</td>\n",
       "      <td>0.774</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.437</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.532</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.308</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-1.460</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.667</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>1.266</td>\n",
       "      <td>0.962</td>\n",
       "      <td>1.285</td>\n",
       "      <td>1.176</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.372</td>\n",
       "      <td>1.505</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.641</td>\n",
       "      <td>-1.132</td>\n",
       "      <td>1.009</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-1.634</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>0.313</td>\n",
       "      <td>1.140</td>\n",
       "      <td>0.447</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>1.294</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.453</td>\n",
       "      <td>1.550</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>1.007</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-2.628</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>2.078</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>2.132</td>\n",
       "      <td>0.609</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>1.347</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.203</td>\n",
       "      <td>1.356</td>\n",
       "      <td>-1.086</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.876</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-1.394</td>\n",
       "      <td>0.385</td>\n",
       "      <td>1.891</td>\n",
       "      <td>-2.107</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-1.146</td>\n",
       "      <td>1.210</td>\n",
       "      <td>1.427</td>\n",
       "      <td>0.347</td>\n",
       "      <td>1.077</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>0.756</td>\n",
       "      <td>1.350</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.748</td>\n",
       "      <td>2.014</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1.343</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-1.097</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.799</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>1.483</td>\n",
       "      <td>1.670</td>\n",
       "      <td>1.403</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-1.564</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>-1.563</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-0.727</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>1.573</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1.691</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-1.665</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.089</td>\n",
       "      <td>2.032</td>\n",
       "      <td>-1.132</td>\n",
       "      <td>-1.827</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-1.748</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>2.004</td>\n",
       "      <td>1.216</td>\n",
       "      <td>1.547</td>\n",
       "      <td>1.322</td>\n",
       "      <td>0.481</td>\n",
       "      <td>1.819</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-1.236</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.860</td>\n",
       "      <td>-1.363</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-2.017</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>2.421</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1.059</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>1.502</td>\n",
       "      <td>-1.078</td>\n",
       "      <td>-1.196</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.869</td>\n",
       "      <td>-1.596</td>\n",
       "      <td>1.400</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.577</td>\n",
       "      <td>1.222</td>\n",
       "      <td>2.069</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.748</td>\n",
       "      <td>1.493</td>\n",
       "      <td>-2.634</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-1.824</td>\n",
       "      <td>-1.452</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>0.563</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.791</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>-1.558</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>2.583</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-2.746</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-1.145</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>0.271</td>\n",
       "      <td>2.639</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.648</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-1.582</td>\n",
       "      <td>-1.987</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-1.788</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-1.133</td>\n",
       "      <td>-3.138</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>1.428</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-2.009</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.459</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-1.039</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.552</td>\n",
       "      <td>-0.830</td>\n",
       "      <td>1.374</td>\n",
       "      <td>-0.914</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.327</td>\n",
       "      <td>1.117</td>\n",
       "      <td>0.871</td>\n",
       "      <td>-2.556</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.744</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>-1.784</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-1.063</td>\n",
       "      <td>-1.333</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>1.295</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>0.755</td>\n",
       "      <td>1.206</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-1.403</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.905</td>\n",
       "      <td>1.142</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.738</td>\n",
       "      <td>-1.881</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>-1.171</td>\n",
       "      <td>1.057</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>2.686</td>\n",
       "      <td>-2.471</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.063</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>2.202</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-1.104</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.906</td>\n",
       "      <td>-1.441</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.706</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.297</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-1.746</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.878</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>-1.128</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.065</td>\n",
       "      <td>1.707</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>1.248</td>\n",
       "      <td>-1.201</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>1.403</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-1.545</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>-0.751</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1.059</td>\n",
       "      <td>1.457</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>-1.058</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-1.529</td>\n",
       "      <td>1.167</td>\n",
       "      <td>-1.070</td>\n",
       "      <td>-2.563</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.589</td>\n",
       "      <td>0.844</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>-1.651</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.661</td>\n",
       "      <td>-1.134</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>-1.167</td>\n",
       "      <td>1.009</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>-1.383</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-1.558</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-2.090</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>1.672</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>-1.846</td>\n",
       "      <td>0.761</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.962</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>1.450</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-2.662</td>\n",
       "      <td>0.761</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-1.327</td>\n",
       "      <td>2.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-1.855</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>0.578</td>\n",
       "      <td>1.592</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-1.419</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.767</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>0.857</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-1.995</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>1.703</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>1.488</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>1.975</td>\n",
       "      <td>0.812</td>\n",
       "      <td>-1.838</td>\n",
       "      <td>1.449</td>\n",
       "      <td>2.116</td>\n",
       "      <td>1.988</td>\n",
       "      <td>-1.516</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-1.553</td>\n",
       "      <td>1.145</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>1.348</td>\n",
       "      <td>0.412</td>\n",
       "      <td>1.368</td>\n",
       "      <td>0.754</td>\n",
       "      <td>1.275</td>\n",
       "      <td>1.405</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.932</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>1.111</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>-1.841</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.106</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-1.244</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-1.649</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-2.060</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-1.032</td>\n",
       "      <td>0.728</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.602</td>\n",
       "      <td>-1.773</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>1.279</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.502</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>0.453</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-1.556</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-1.386</td>\n",
       "      <td>0.637</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-1.316</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-2.316</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>1.744</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-1.297</td>\n",
       "      <td>-1.636</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>0.243</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-1.274</td>\n",
       "      <td>0.872</td>\n",
       "      <td>1.181</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>0.827</td>\n",
       "      <td>-1.477</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-1.029</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.052</td>\n",
       "      <td>2.122</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-1.799</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.866</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>-1.560</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>2.167</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.583</td>\n",
       "      <td>1.480</td>\n",
       "      <td>-0.685</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-1.066</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>1.048</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-1.221</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>0.916</td>\n",
       "      <td>2.411</td>\n",
       "      <td>1.053</td>\n",
       "      <td>-1.601</td>\n",
       "      <td>-1.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>1.173</td>\n",
       "      <td>-1.623</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-1.763</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>1.284</td>\n",
       "      <td>-1.167</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>-2.184</td>\n",
       "      <td>2.146</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.421</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>1.938</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.635</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.346</td>\n",
       "      <td>1.175</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.611</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.389</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.168</td>\n",
       "      <td>-1.277</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>1.521</td>\n",
       "      <td>2.195</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.258</td>\n",
       "      <td>-1.360</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-1.210</td>\n",
       "      <td>1.643</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-1.321</td>\n",
       "      <td>1.749</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>1.269</td>\n",
       "      <td>-3.389</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-1.216</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>1.388</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.398</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>1.155</td>\n",
       "      <td>1.376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.847</td>\n",
       "      <td>1.718</td>\n",
       "      <td>0.562</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-1.888</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>0.624</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.214</td>\n",
       "      <td>1.192</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.811</td>\n",
       "      <td>1.154</td>\n",
       "      <td>1.663</td>\n",
       "      <td>-1.142</td>\n",
       "      <td>-1.592</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.140</td>\n",
       "      <td>1.414</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-1.567</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-1.316</td>\n",
       "      <td>1.161</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>-2.089</td>\n",
       "      <td>0.892</td>\n",
       "      <td>1.123</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>1.095</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>1.304</td>\n",
       "      <td>0.548</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-0.585</td>\n",
       "      <td>2.169</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-1.322</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.745</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.738</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.745</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-1.187</td>\n",
       "      <td>1.681</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-1.096</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>3.565</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>1.243</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>1.525</td>\n",
       "      <td>0.458</td>\n",
       "      <td>2.184</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.216</td>\n",
       "      <td>1.186</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      0      1      2      3      4      5      6      7      8      9  \\\n",
       "0  250  0.500 -1.033 -1.595  0.309 -0.714  0.502  0.535 -0.129 -0.687  1.291   \n",
       "1  251  0.776  0.914 -0.494  1.347 -0.867  0.480  0.578 -0.313  0.203  1.356   \n",
       "2  252  1.750  0.509 -0.057  0.835 -0.476  1.428 -0.701 -2.009 -1.378  0.167   \n",
       "3  253 -0.556 -1.855 -0.682  0.578  1.592  0.512 -1.419  0.722  0.511  0.567   \n",
       "4  254  0.754 -0.245  1.173 -1.623  0.009  0.370  0.781 -1.763 -1.432 -0.930   \n",
       "\n",
       "      10     11     12     13     14     15     16     17     18     19  \\\n",
       "0  0.507 -0.317  1.848 -0.232 -0.340 -0.051  0.804  0.764  1.860  0.262   \n",
       "1 -1.086  0.322  0.876 -0.563 -1.394  0.385  1.891 -2.107 -0.636 -0.055   \n",
       "2 -0.132  0.459 -0.341  0.014  0.184 -0.460 -0.991 -1.039  0.992  1.036   \n",
       "3  0.356 -0.060  0.767 -0.196  0.359  0.080 -0.956  0.857 -0.655 -0.090   \n",
       "4 -0.098  0.896  0.293 -0.259  0.030 -0.661  0.921  0.006 -0.631  1.284   \n",
       "\n",
       "      20     21     22     23     24     25     26     27     28     29  \\\n",
       "0  1.112 -0.491 -1.039 -0.492  0.183 -0.671 -1.313  0.149  0.244  1.072   \n",
       "1 -0.843  0.041  0.253  0.557  0.475 -0.839 -1.146  1.210  1.427  0.347   \n",
       "2  1.552 -0.830  1.374 -0.914  0.427  0.027  0.327  1.117  0.871 -2.556   \n",
       "3 -0.008 -0.596 -0.413 -1.030  0.173 -0.969  0.998  0.079  0.790 -0.776   \n",
       "4 -1.167 -0.744 -2.184  2.146  1.130  0.017  1.421 -0.590  1.938 -0.194   \n",
       "\n",
       "      30     31     32     33     34     35     36     37     38     39  \\\n",
       "0 -1.003  0.832 -1.075  1.988  1.201 -2.065 -0.826 -0.016  0.490  0.191   \n",
       "1  1.077 -0.194  0.323  0.543  0.894  1.190  0.342 -0.858  0.756  1.350   \n",
       "2 -0.036 -0.081  0.744 -1.191 -1.784  0.239  0.500  0.437  0.746  0.999   \n",
       "3 -0.374 -1.995  0.572  0.542  0.547  0.307 -0.074  1.703 -0.003  0.818   \n",
       "4  0.794  0.579  0.521  0.635 -0.023 -0.892 -0.363 -0.360  0.405  0.222   \n",
       "\n",
       "      40     41     42     43     44     45     46     47     48     49  \\\n",
       "0  0.732  1.235 -0.867 -0.616  0.340  0.788 -0.044  0.305 -0.819 -0.447   \n",
       "1 -0.414  0.748  2.014  0.858  0.025  1.343  0.784 -0.418 -0.515  0.694   \n",
       "2  0.489  0.467 -1.063 -1.333  1.062  0.482  0.984 -0.542  1.295 -1.191   \n",
       "3  0.182  0.082 -0.374 -0.475  1.488 -0.556  1.975  0.812 -1.838  1.449   \n",
       "4  0.346  1.175 -0.252  0.767  0.654  0.339  0.481  0.751  0.611 -0.052   \n",
       "\n",
       "      50     51     52     53     54     55     56     57     58     59  \\\n",
       "0 -1.625 -1.005 -0.653 -0.371  1.556  0.754 -0.688  0.061  0.644  0.645   \n",
       "1 -1.097  0.559 -0.799 -0.936  1.483  1.670  1.403  0.457 -1.564  0.049   \n",
       "2  0.755  1.206 -0.558 -1.403 -0.852  0.025  0.835  0.716  0.640 -1.007   \n",
       "3  2.116  1.988 -1.516  0.264 -0.232  0.974 -2.000  0.072 -1.553  1.145   \n",
       "4  0.389 -0.426  1.950  1.168 -1.277 -0.154 -1.829  1.521  2.195  0.012   \n",
       "\n",
       "      60     61     62     63     64     65     66     67     68     69  \\\n",
       "0 -0.222 -2.174 -0.610 -1.092  0.917 -1.010 -1.021 -0.179  1.732 -0.366   \n",
       "1  0.550 -0.085 -0.561 -0.529 -1.563 -0.781 -0.532  0.375 -0.727 -0.053   \n",
       "2  0.268 -1.148  1.019  0.905  1.142 -0.529  0.738 -1.881 -0.857 -1.171   \n",
       "3 -1.038 -1.004  1.348  0.412  1.368  0.754  1.275  1.405 -0.024  0.636   \n",
       "4  1.258 -1.360  0.770 -0.916 -0.198 -1.210  1.643  0.068  0.048 -0.781   \n",
       "\n",
       "      70     71     72     73     74     75     76     77     78     79  \\\n",
       "0 -1.694  1.038 -0.721  0.112 -0.783  0.940 -1.803  1.295 -1.031  0.452   \n",
       "1 -0.383 -0.123  1.573 -0.898 -0.070  0.811 -0.036  0.720  1.691 -0.673   \n",
       "2  1.057 -2.476  2.686 -2.471 -0.153  0.190  1.063  0.117 -1.038 -0.134   \n",
       "3 -1.180  0.506  0.932 -0.246  1.051 -0.220  1.111  0.401  0.502  0.315   \n",
       "4  0.356  0.335  0.211 -1.321  1.749  0.563  0.020 -0.433 -0.742  1.269   \n",
       "\n",
       "      80     81     82     83     84     85     86     87     88     89  \\\n",
       "0  1.198 -0.206  0.051 -1.055  1.740 -0.910 -0.509 -0.987 -1.011  0.718   \n",
       "1 -0.421 -1.665  0.099  0.089  2.032 -1.132 -1.827 -0.017 -1.748 -0.717   \n",
       "2 -1.030 -0.054 -0.608 -0.333  0.184  0.633  0.024 -0.056  2.202  0.434   \n",
       "3  0.560 -0.569 -1.841  0.830  0.543  0.090  0.062  0.106  1.030 -1.244   \n",
       "4 -3.389 -0.291 -1.216 -0.968  1.388  0.934  0.022  1.398 -0.571 -0.056   \n",
       "\n",
       "      90     91     92     93     94     95     96     97     98  ...    200  \\\n",
       "0  0.375  0.101  0.137 -1.585  0.532 -1.201  1.210 -0.374  0.300  ...  1.578   \n",
       "1  2.004  1.216  1.547  1.322  0.481  1.819 -0.809  0.617 -0.763  ... -1.270   \n",
       "2  0.065 -1.104 -0.455  0.290  0.906 -1.441  0.557  0.243  0.706  ... -1.297   \n",
       "3 -0.237 -1.649  0.405 -2.060 -0.870  1.206  1.490 -0.981 -0.828  ...  0.349   \n",
       "4 -0.033 -0.294  1.030 -0.972 -0.655  0.304 -0.028  1.155  1.376  ...  1.847   \n",
       "\n",
       "     201    202    203    204    205    206    207    208    209    210  \\\n",
       "0 -0.488  1.424  1.106  0.363 -2.007 -0.091  0.551  0.388  0.422  0.099   \n",
       "1 -0.426 -1.236 -0.036  0.187  0.860 -1.363 -0.279 -0.556 -2.017 -0.651   \n",
       "2 -0.847 -0.511 -0.181 -1.060 -0.205 -1.746 -0.371  0.878 -0.885 -1.128   \n",
       "3 -1.032  0.728 -0.691  0.936  1.075  0.602 -1.773 -0.550  1.279 -0.793   \n",
       "4  1.718  0.562 -0.162 -0.521 -0.425 -1.888 -0.333  0.210 -0.110  0.827   \n",
       "\n",
       "     211    212    213    214    215    216    217    218    219    220  \\\n",
       "0  0.378 -1.333 -1.102  2.145  0.745  0.345 -0.904  0.425 -0.273  0.547   \n",
       "1 -1.192 -0.339  0.363  0.416 -0.039  2.421  0.953  1.059  0.512 -0.616   \n",
       "2 -0.691  1.200  0.065  1.707 -0.846  1.248 -1.201 -0.480 -0.953  1.403   \n",
       "3  0.680  0.263 -0.394  0.121 -0.544  0.910  1.502 -0.817  0.453 -0.019   \n",
       "4  0.102 -0.832 -0.724  0.624 -0.496 -0.196  0.460  0.214  1.192 -0.090   \n",
       "\n",
       "     221    222    223    224    225    226    227    228    229    230  \\\n",
       "0 -0.184  0.458  0.182  0.592  0.966  0.540 -1.382  0.069  0.131 -0.068   \n",
       "1 -0.172  1.502 -1.078 -1.196  0.042  0.476 -0.271  0.869 -1.596  1.400   \n",
       "2 -0.228 -1.545 -0.085  0.554 -0.626 -0.751 -0.696  0.248  0.059  1.059   \n",
       "3 -1.556 -0.447 -0.076 -0.309  0.307 -1.386  0.637 -1.150  0.540  0.455   \n",
       "4 -0.089  0.811  1.154  1.663 -1.142 -1.592 -0.082  0.140  1.414  0.047   \n",
       "\n",
       "     231    232    233    234    235    236    237    238    239    240  \\\n",
       "0 -0.400  0.413 -0.030  0.890  1.000 -0.774  0.340  2.345  2.748  0.774   \n",
       "1  0.148  0.577  1.222  2.069 -0.820  0.443  0.025  0.089 -0.939 -0.643   \n",
       "2  1.457 -0.452 -1.058 -0.393 -1.529  1.167 -1.070 -2.563  0.427  0.369   \n",
       "3 -0.948 -1.316 -0.274 -2.316 -0.652 -0.652 -0.611  1.744  0.260  0.051   \n",
       "4  0.343  0.062  0.999 -0.270  0.234 -0.047 -1.567 -0.153 -0.273 -1.316   \n",
       "\n",
       "     241    242    243    244    245    246    247    248    249    250  \\\n",
       "0 -0.355  0.574  0.027  1.437 -0.877  0.532 -0.348  0.926  1.308 -0.120   \n",
       "1 -0.376  0.297  0.352  0.748  1.493 -2.634  0.368 -0.177 -0.143  0.835   \n",
       "2  0.011  1.589  0.844 -0.425 -0.572  0.558 -0.490 -0.424 -1.651  0.460   \n",
       "3 -0.256 -0.296 -1.297 -1.636  0.023 -0.872  0.243  1.110 -0.104 -0.483   \n",
       "4  1.161 -1.568 -2.089  0.892  1.123 -0.862 -0.993  1.095  0.266 -0.455   \n",
       "\n",
       "     251    252    253    254    255    256    257    258    259    260  \\\n",
       "0 -1.460  0.755  0.426  1.667 -0.264  1.266  0.962  1.285  1.176  0.824   \n",
       "1 -1.824 -1.452 -0.408 -0.417  0.563 -0.161 -0.494  0.170 -0.257 -1.791   \n",
       "2 -0.581  0.259  0.982  0.123 -0.723  0.034  1.661 -1.134 -0.643 -1.167   \n",
       "3 -0.189 -1.274  0.872  1.181 -0.627  0.827 -1.477  0.322 -0.620 -1.029   \n",
       "4  1.304  0.548 -0.654  0.276 -0.073 -0.860 -0.585  2.169  0.141 -0.486   \n",
       "\n",
       "     261    262    263    264    265    266    267    268    269    270  \\\n",
       "0  0.928  1.372  1.505  0.645  0.641 -1.132  1.009  0.998  0.210 -1.634   \n",
       "1  0.122 -0.669 -1.558 -0.244  2.583 -0.829  0.133 -2.746  0.341 -1.145   \n",
       "2  1.009 -0.180 -0.683 -1.383  1.020  0.268 -1.558  0.620 -0.489 -2.090   \n",
       "3 -0.340  0.052  2.122 -0.136 -1.799  1.450  1.866 -0.273 -0.237 -0.207   \n",
       "4 -0.068 -0.534 -1.322  0.500  0.263 -0.745  0.578 -0.064  0.738 -0.280   \n",
       "\n",
       "     271    272    273    274    275    276    277    278    279    280  \\\n",
       "0  1.046  0.114 -0.806  0.301  0.145 -0.684  0.794 -0.290 -1.688  0.313   \n",
       "1  0.492  0.437 -0.628  0.271  2.639  0.481 -0.687  1.017  1.648 -1.272   \n",
       "2 -0.977  1.672 -0.655 -0.801 -1.846  0.761 -0.846  0.181  0.962 -0.611   \n",
       "3 -0.196 -1.106 -1.560 -0.934  2.167  0.323  0.583  1.480 -0.685 -0.473   \n",
       "4  0.745 -0.588 -0.429 -0.588  0.154 -1.187  1.681 -0.832 -0.437 -0.038   \n",
       "\n",
       "     281    282    283    284    285    286    287    288    289    290  \\\n",
       "0  1.140  0.447 -0.616  1.294  0.785  0.453  1.550 -0.866  1.007 -0.088   \n",
       "1 -0.797 -0.870 -1.582 -1.987 -0.052 -0.194  0.539 -1.788 -0.433 -0.683   \n",
       "2  1.450  0.021  0.320 -0.951 -2.662  0.761 -0.665 -0.619 -0.645 -0.094   \n",
       "3 -1.066 -0.271  0.506 -0.753  1.048 -0.450 -0.300 -1.221  0.235 -0.336   \n",
       "4 -1.096 -0.156  3.565 -0.428 -0.384  1.243 -0.966  1.525  0.458  2.184   \n",
       "\n",
       "     291    292    293    294    295    296    297    298    299  \n",
       "0 -2.628 -0.845  2.078 -0.277  2.132  0.609 -0.104  0.312  0.979  \n",
       "1 -0.066  0.025  0.606 -0.353 -1.133 -3.138  0.281 -0.625 -0.761  \n",
       "2  0.351 -0.607 -0.737 -0.031  0.701  0.976  0.135 -1.327  2.463  \n",
       "3 -0.787  0.255 -0.031 -0.836  0.916  2.411  1.053 -1.601 -1.529  \n",
       "4 -1.090  0.216  1.186 -0.143  0.322 -0.068 -0.156 -1.153  0.825  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "6f7cd5bdd625e69c75e10f586a826da80c814cdf"
   },
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "predictors = train_df.columns.values.tolist()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "cc8cef4f66aa77c4183cc4f99acd6082b6f036bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    160\n",
       "0.0     90\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad51d7daad193e0dab467f82dfad4c7ce7876d56"
   },
   "source": [
    "The problem is unbalanced! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "496907f42901e10bb883858252b2783e30ff2e43"
   },
   "source": [
    "In this kernel I will be using **50% Stratified rows** as holdout rows for the validation-set to get optimal parameters. Later I will use 5 fold cross validation in the final model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "7ec87a3460c6358b9a134afea5bac561f7a84226"
   },
   "outputs": [],
   "source": [
    "bayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(train_df, train_df.target.values))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cbcebd0aacaeb637a1e119971303c9fcd60f9ea5"
   },
   "source": [
    "These `bayesian_tr_index` and `bayesian_val_index` indexes will be used for the bayesian optimization as training and validation index of training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f18184730f7261c3ddc2253ee025f9910ffedb6"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## 2. Black box function to be optimized (LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a5be20e9809494709c5da85861775c8720816ba"
   },
   "source": [
    "As data is loaded, let's create the black box function for LightGBM to find parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "196288ebb7caf614e230a0449b40354266efbc45"
   },
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    num_leaves,  # int\n",
    "    min_data_in_leaf,  # int\n",
    "    learning_rate,\n",
    "    min_sum_hessian_in_leaf,    # int  \n",
    "    feature_fraction,\n",
    "    lambda_l1,\n",
    "    lambda_l2,\n",
    "    min_gain_to_split,\n",
    "    max_depth):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. So we make them integer\n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "\n",
    "    param = {\n",
    "        'num_leaves': num_leaves,\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': min_data_in_leaf,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n",
    "        'bagging_fraction': 1.0,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'min_gain_to_split': min_gain_to_split,\n",
    "        'max_depth': max_depth,\n",
    "        'save_binary': True, \n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,   \n",
    "\n",
    "    }    \n",
    "    \n",
    "    \n",
    "    xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n",
    "                           label=train_df.iloc[bayesian_tr_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )\n",
    "    xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n",
    "                           label=train_df.iloc[bayesian_val_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )   \n",
    "\n",
    "    num_round = 5000\n",
    "    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n",
    "    \n",
    "    predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n",
    "    \n",
    "    score = metrics.roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40b3959e5f0672b6e030408cd197da137e1b66ee"
   },
   "source": [
    "The above `LGB_bayesian` function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the `LGB_bayesian` function. \n",
    "\n",
    "The `LGB_bayesian` function takes values for `num_leaves`, `min_data_in_leaf`, `learning_rate`, `min_sum_hessian_in_leaf`, `feature_fraction`, `lambda_l1`, `lambda_l2`, `min_gain_to_split`, `max_depth` from Bayesian optimization framework. Keep in mind that `num_leaves`, `min_data_in_leaf`, and `max_depth` should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fb52d3d130c2477a6ab71b2d6797c787dce9f21"
   },
   "source": [
    "Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "044ed09d293f7a712af6fe6c00e935df19ca1cf4"
   },
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (5, 20), \n",
    "    'min_data_in_leaf': (5, 20),  \n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'min_sum_hessian_in_leaf': (0.00001, 0.01),    \n",
    "    'feature_fraction': (0.05, 0.5),\n",
    "    'lambda_l1': (1, 5.0), \n",
    "    'lambda_l2': (1, 5.0), \n",
    "    'min_gain_to_split': (0, 1.0),\n",
    "    'max_depth':(2,5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a2a217d1bcc8b27e67b9cc7f4b7bbb237b8ee72"
   },
   "source": [
    "Let's put all of them in BayesianOptimization object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "c0fc26566d5b5bf64a9f507cd06bf0ab85b584e3"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "af6565a9d43d2b5a153a3f2009afca7618a699c1"
   },
   "outputs": [],
   "source": [
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95f597ee9781e5621c98fe89d69a30601902c716"
   },
   "source": [
    "Now, let's the the key space (parameters) we are going to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "281f53a7d9bca23329e965986939443bc63a927c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_fraction', 'lambda_l1', 'lambda_l2', 'learning_rate', 'max_depth', 'min_data_in_leaf', 'min_gain_to_split', 'min_sum_hessian_in_leaf', 'num_leaves']\n"
     ]
    }
   ],
   "source": [
    "print(LGB_BO.space.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2fbbcbe963f659df1884511af1a0009f28ef6d5"
   },
   "source": [
    "I have created the BayesianOptimization object (`LGB_BO`), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (`LGB_BO`) which we can pass to maximize:\n",
    "- `init_points`: How many initial random runs of **random** exploration we want to perform. In our case `LGB_bayesian` will be called `n_iter` times.\n",
    "- `n_iter`: How many runs of bayesian optimization we want to perform after number of `init_points` runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e06903e320665c4f77e9b029681cf6f651dfc9b4"
   },
   "source": [
    "Now, it's time to call the function from Bayesian optimization framework to maximize. I allow `LGB_BO` object to run for 5 `init_points` (exploration) and 5 `n_iter` (exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_uuid": "6c12e5f092c2d0f690fd720e40aec69268c9e70f"
   },
   "outputs": [],
   "source": [
    "init_points = 10\n",
    "n_iter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "f5f6d3f1a7752d818330b92980344956bf934e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.631944\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6319  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 1.95    \u001b[0m | \u001b[0m 4.297   \u001b[0m | \u001b[0m 0.2901  \u001b[0m | \u001b[0m 4.918   \u001b[0m | \u001b[0m 11.8    \u001b[0m | \u001b[0m 0.609   \u001b[0m | \u001b[0m 0.007758\u001b[0m | \u001b[0m 14.62   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's auc: 0.6875\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.6875  \u001b[0m | \u001b[95m 0.3749  \u001b[0m | \u001b[95m 1.14    \u001b[0m | \u001b[95m 2.194   \u001b[0m | \u001b[95m 0.02697 \u001b[0m | \u001b[95m 4.571   \u001b[0m | \u001b[95m 10.59   \u001b[0m | \u001b[95m 0.6798  \u001b[0m | \u001b[95m 0.00257 \u001b[0m | \u001b[95m 10.21   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's auc: 0.722222\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.7222  \u001b[0m | \u001b[95m 0.05424 \u001b[0m | \u001b[95m 2.433   \u001b[0m | \u001b[95m 4.796   \u001b[0m | \u001b[95m 0.07319 \u001b[0m | \u001b[95m 2.958   \u001b[0m | \u001b[95m 18.77   \u001b[0m | \u001b[95m 0.0319  \u001b[0m | \u001b[95m 0.000660\u001b[0m | \u001b[95m 14.45   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's auc: 0.659722\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6597  \u001b[0m | \u001b[0m 0.4432  \u001b[0m | \u001b[0m 1.035   \u001b[0m | \u001b[0m 3.986   \u001b[0m | \u001b[0m 0.2457  \u001b[0m | \u001b[0m 2.227   \u001b[0m | \u001b[0m 14.85   \u001b[0m | \u001b[0m 0.5093  \u001b[0m | \u001b[0m 0.004804\u001b[0m | \u001b[0m 19.33   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid_0's auc: 0.631944\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6319  \u001b[0m | \u001b[0m 0.05001 \u001b[0m | \u001b[0m 1.988   \u001b[0m | \u001b[0m 3.849   \u001b[0m | \u001b[0m 0.1041  \u001b[0m | \u001b[0m 2.831   \u001b[0m | \u001b[0m 15.43   \u001b[0m | \u001b[0m 0.9186  \u001b[0m | \u001b[0m 0.002452\u001b[0m | \u001b[0m 11.87   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.576389\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5764  \u001b[0m | \u001b[0m 0.1638  \u001b[0m | \u001b[0m 2.517   \u001b[0m | \u001b[0m 3.418   \u001b[0m | \u001b[0m 0.234   \u001b[0m | \u001b[0m 2.204   \u001b[0m | \u001b[0m 15.29   \u001b[0m | \u001b[0m 0.5483  \u001b[0m | \u001b[0m 0.001388\u001b[0m | \u001b[0m 6.481   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's auc: 0.590278\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5903  \u001b[0m | \u001b[0m 0.1605  \u001b[0m | \u001b[0m 1.607   \u001b[0m | \u001b[0m 4.704   \u001b[0m | \u001b[0m 0.2072  \u001b[0m | \u001b[0m 2.713   \u001b[0m | \u001b[0m 13.53   \u001b[0m | \u001b[0m 0.5566  \u001b[0m | \u001b[0m 0.000736\u001b[0m | \u001b[0m 17.6    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's auc: 0.75\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.75    \u001b[0m | \u001b[95m 0.2324  \u001b[0m | \u001b[95m 1.579   \u001b[0m | \u001b[95m 1.764   \u001b[0m | \u001b[95m 0.1523  \u001b[0m | \u001b[95m 4.136   \u001b[0m | \u001b[95m 19.77   \u001b[0m | \u001b[95m 0.8748  \u001b[0m | \u001b[95m 0.004995\u001b[0m | \u001b[95m 6.602   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.75\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.75    \u001b[0m | \u001b[0m 0.4609  \u001b[0m | \u001b[0m 2.46    \u001b[0m | \u001b[0m 1.906   \u001b[0m | \u001b[0m 0.263   \u001b[0m | \u001b[0m 2.409   \u001b[0m | \u001b[0m 8.546   \u001b[0m | \u001b[0m 0.5954  \u001b[0m | \u001b[0m 0.005644\u001b[0m | \u001b[0m 19.38   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's auc: 0.715278\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7153  \u001b[0m | \u001b[0m 0.254   \u001b[0m | \u001b[0m 1.516   \u001b[0m | \u001b[0m 4.042   \u001b[0m | \u001b[0m 0.06847 \u001b[0m | \u001b[0m 2.527   \u001b[0m | \u001b[0m 11.56   \u001b[0m | \u001b[0m 0.3403  \u001b[0m | \u001b[0m 0.009673\u001b[0m | \u001b[0m 7.145   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's auc: 0.736111\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7361  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's auc: 0.791667\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.7917  \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 20.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 20.0    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's auc: 0.680556\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6806  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.784722\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7847  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.621528\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6215  \u001b[0m | \u001b[0m 0.05    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's auc: 0.784722\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.7847  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.663194\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.6632  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 10.74   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's auc: 0.743056\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7431  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.6875\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.6875  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.649306\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.6493  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's auc: 0.826389\n",
      "| \u001b[95m 21      \u001b[0m | \u001b[95m 0.8264  \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 20.0    \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 11.06   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's auc: 0.597222\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.5972  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's auc: 0.722222\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.7222  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 14.41   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.711806\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7118  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's auc: 0.701389\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.7014  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's auc: 0.840278\n",
      "| \u001b[95m 26      \u001b[0m | \u001b[95m 0.8403  \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 20.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 11.48   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's auc: 0.715278\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.7153  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 8.082   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.763889\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.7639  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 10.58   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's auc: 0.770833\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7708  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.597222\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.5972  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 10.84   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdea40277b15b11b5b92238e519cf5fc2576f64f"
   },
   "source": [
    "As the optimization is done, let's see what is the maximum value we have got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "d0d5a3fe41a7d9f1625cdfb5eceb113f6cabcf93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8402777777777778"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad0a200a70e00955a924ca5f3fc622b492eaf3e3"
   },
   "source": [
    "The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "ced952bfd9397a68e3a2d394a4f7e075b40a9aff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.5,\n",
       " 'lambda_l1': 1.0,\n",
       " 'lambda_l2': 5.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 5.0,\n",
       " 'min_data_in_leaf': 20.0,\n",
       " 'min_gain_to_split': 1.0,\n",
       " 'min_sum_hessian_in_leaf': 0.01,\n",
       " 'num_leaves': 11.475737771544752}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f6bc1d0859b643bdbbeba1e0e3c8b336860c023"
   },
   "source": [
    "Now we can use these parameters to our final model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06096bd955639e891320f5b31408fad1785f5949"
   },
   "source": [
    "Wait, I want to show one more cool option from BayesianOptimization library. You can probe the `LGB_bayesian` function, if you have an idea of the optimal parameters or it you get **parameters from other kernel** like mine [mine](https://www.kaggle.com/fayzur/customer-transaction-prediction). I will copy and paste parameters from my other kernel here. You can probe as folowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "a0275beb85200e0c8cce95b477dd756e8d1ef2ef"
   },
   "outputs": [],
   "source": [
    "# parameters from version 2 of\n",
    "#https://www.kaggle.com/fayzur/customer-transaction-prediction?scriptVersionId=10522231\n",
    "\n",
    "LGB_BO.probe(\n",
    "    params={'feature_fraction': 0.1403, \n",
    "            'lambda_l1': 4.218, \n",
    "            'lambda_l2': 1.734, \n",
    "            'learning_rate': 0.07, \n",
    "            'max_depth': 14, \n",
    "            'min_data_in_leaf': 17, \n",
    "            'min_gain_to_split': 0.1501, \n",
    "            'min_sum_hessian_in_leaf': 0.000446, \n",
    "            'num_leaves': 6},\n",
    "    lazy=True, # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2893bc7de2bf69fd02ef26e90fc354f71b83e10"
   },
   "source": [
    "OK, by default these will be explored lazily (lazy=True), meaning these points will be evaluated only the next time you call maximize. Let's do a maximize call of `LGB_BO` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "8e3feb820520bb03000225416e338610e2d90b1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.86243\n",
      "[500]\tvalid_0's auc: 0.880738\n",
      "[750]\tvalid_0's auc: 0.887841\n",
      "[1000]\tvalid_0's auc: 0.890905\n",
      "[1250]\tvalid_0's auc: 0.892214\n",
      "[1500]\tvalid_0's auc: 0.892655\n",
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's auc: 0.892672\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.8927  \u001b[0m | \u001b[95m 0.1403  \u001b[0m | \u001b[95m 4.218   \u001b[0m | \u001b[95m 1.734   \u001b[0m | \u001b[95m 0.07    \u001b[0m | \u001b[95m 14.0    \u001b[0m | \u001b[95m 17.0    \u001b[0m | \u001b[95m 0.1501  \u001b[0m | \u001b[95m 0.000446\u001b[0m | \u001b[95m 6.0     \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "LGB_BO.maximize(init_points=0, n_iter=0) # remember no init_points or n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9bd70047d5313694681d73ed049339d6b00261c0"
   },
   "source": [
    "Finally, the list of all parameters probed and their corresponding target values is available via the property LGB_BO.res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "416bfb9650849e75106ee63b40dfd7519aa2ee29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\t{'target': 0.8785456504868869, 'params': {'feature_fraction': 0.3999660847582191, 'lambda_l1': 1.1877061001745615, 'lambda_l2': 4.1213926633068425, 'learning_rate': 0.2900672674324699, 'max_depth': 14.67121336685872, 'min_data_in_leaf': 11.801738711259683, 'min_gain_to_split': 0.6090424627612779, 'min_sum_hessian_in_leaf': 0.007757509880902418, 'num_leaves': 14.624200171386038}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.8915776403640969, 'params': {'feature_fraction': 0.3749082032826262, 'lambda_l1': 0.17518262050718658, 'lambda_l2': 1.492247354445897, 'learning_rate': 0.026968622645801674, 'max_depth': 13.284731311046386, 'min_data_in_leaf': 10.592810418122113, 'min_gain_to_split': 0.679847951578097, 'min_sum_hessian_in_leaf': 0.002570236693773035, 'num_leaves': 10.21371822728738}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.8924701144136037, 'params': {'feature_fraction': 0.05423574653643624, 'lambda_l1': 1.7916689135248487, 'lambda_l2': 4.745470908391052, 'learning_rate': 0.07319071264818977, 'max_depth': 6.8326963965643746, 'min_data_in_leaf': 18.76658579000881, 'min_gain_to_split': 0.03190366643989473, 'min_sum_hessian_in_leaf': 0.0006601945250547198, 'num_leaves': 14.447434986617345}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.8843155956741141, 'params': {'feature_fraction': 0.4432160494757924, 'lambda_l1': 0.04357866151892431, 'lambda_l2': 3.7328861849696877, 'learning_rate': 0.24572393959076078, 'max_depth': 3.9086093540672007, 'min_data_in_leaf': 14.846830018954368, 'min_gain_to_split': 0.5092622000835182, 'min_sum_hessian_in_leaf': 0.004804035081048867, 'num_leaves': 19.333612173965996}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.8918922376241952, 'params': {'feature_fraction': 0.0500054151062683, 'lambda_l1': 1.2348935049595817, 'lambda_l2': 3.5611633895579176, 'learning_rate': 0.1041287944381468, 'max_depth': 6.323956276605713, 'min_data_in_leaf': 15.431681788302118, 'min_gain_to_split': 0.9185517481459488, 'min_sum_hessian_in_leaf': 0.0024523122649579236, 'num_leaves': 11.871287259151455}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.8877741089318031, 'params': {'feature_fraction': 0.28880235499863005, 'lambda_l1': 0.0676352739721825, 'lambda_l2': 0.06367783660314708, 'learning_rate': 0.23658833958998987, 'max_depth': 8.82154329429286, 'min_data_in_leaf': 19.911222977094404, 'min_gain_to_split': 0.7962518122529608, 'min_sum_hessian_in_leaf': 0.00982563254531773, 'num_leaves': 5.699237677594455}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.8838845198764629, 'params': {'feature_fraction': 0.5, 'lambda_l1': 5.0, 'lambda_l2': 1.2648294958898453e-09, 'learning_rate': 0.010000000280869852, 'max_depth': 3.0, 'min_data_in_leaf': 5.000000002619426, 'min_gain_to_split': 0.0, 'min_sum_hessian_in_leaf': 1.0000022537751995e-05, 'num_leaves': 20.0}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.8914029939138292, 'params': {'feature_fraction': 0.24571681728644235, 'lambda_l1': 0.46805485709572603, 'lambda_l2': 4.634252859543099, 'learning_rate': 0.10893673688261334, 'max_depth': 5.74869199423548, 'min_data_in_leaf': 5.199199561225826, 'min_gain_to_split': 0.03917651107600029, 'min_sum_hessian_in_leaf': 0.007056042775055096, 'num_leaves': 5.49237053333988}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.885671663972105, 'params': {'feature_fraction': 0.3120275776133379, 'lambda_l1': 5.0, 'lambda_l2': 9.084821479118918e-09, 'learning_rate': 0.01, 'max_depth': 14.999999991290764, 'min_data_in_leaf': 5.0, 'min_gain_to_split': 1.0, 'min_sum_hessian_in_leaf': 0.009999999929977302, 'num_leaves': 5.0}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.8896394504207458, 'params': {'feature_fraction': 0.46632352517727926, 'lambda_l1': 4.554786486122389, 'lambda_l2': 0.07813817250912569, 'learning_rate': 0.1009416028187477, 'max_depth': 3.3923822872270604, 'min_data_in_leaf': 10.028922050131339, 'min_gain_to_split': 0.01464358752550321, 'min_sum_hessian_in_leaf': 0.008685058946939706, 'num_leaves': 7.056018802118709}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.8926723153666576, 'params': {'feature_fraction': 0.1403, 'lambda_l1': 4.218, 'lambda_l2': 1.734, 'learning_rate': 0.07, 'max_depth': 14.0, 'min_data_in_leaf': 17.0, 'min_gain_to_split': 0.1501, 'min_sum_hessian_in_leaf': 0.000446, 'num_leaves': 6.0}}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(LGB_BO.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32a1bd2e328c121915b77177e72f281e6b64ca4c"
   },
   "source": [
    "We have got a better validation score in the probe! As previously I ran `LGB_BO` only for 10 runs. In practice I increase it to arround 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_uuid": "5e881cbd9dc198f4cc91bb65823542ec473c6b39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8402777777777778"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_uuid": "1a132698b07c7652d62c329f5d6508211540834f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.5,\n",
       " 'lambda_l1': 1.0,\n",
       " 'lambda_l2': 5.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 5.0,\n",
       " 'min_data_in_leaf': 20.0,\n",
       " 'min_gain_to_split': 1.0,\n",
       " 'min_sum_hessian_in_leaf': 0.01,\n",
       " 'num_leaves': 11.475737771544752}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e17fc50406f38145fffae3c4aa585f5520433cab"
   },
   "source": [
    "Let's build a model together use therse parameters ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b91db8b5c98da4f014f1863ba7de18e241f517c6"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 3. Training LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "_uuid": "56d006d366fc5c2686249887b5d1f302d4a708f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 11,\n",
       " 'min_data_in_leaf': 20,\n",
       " 'learning_rate': 0.01,\n",
       " 'min_sum_hessian_in_leaf': 0.01,\n",
       " 'feature_fraction': 0.5,\n",
       " 'lambda_l1': 1.0,\n",
       " 'lambda_l2': 5.0,\n",
       " 'min_gain_to_split': 1.0,\n",
       " 'max_depth': 5,\n",
       " 'save_binary': True,\n",
       " 'objective': 'binary',\n",
       " 'boosting_type': 'gbdt',\n",
       " 'verbose': 1,\n",
       " 'metric': 'auc',\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_lgb = {\n",
    "        'num_leaves': int(LGB_BO.max['params']['num_leaves']), # remember to int here\n",
    "        #'max_bin': 63,\n",
    "        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), # remember to int here\n",
    "        'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "        'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n",
    "        #'bagging_fraction': 1.0, \n",
    "        #'bagging_freq': 5, \n",
    "        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n",
    "        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n",
    "        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n",
    "        'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n",
    "        'max_depth': int(LGB_BO.max['params']['max_depth']), # remember to int here\n",
    "        'save_binary': True,\n",
    "     #   'seed': 1337,\n",
    "        #'feature_fraction_seed': 1337,\n",
    "        #'bagging_seed': 1337,\n",
    "        #'drop_seed': 1337,\n",
    "        #'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        #'is_unbalance': True,\n",
    "        'boost_from_average': False,\n",
    "    }\n",
    "\n",
    "param_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5a87047368fe7504c895aab4e9deb1b43746d7d"
   },
   "source": [
    "As you see, I assined `LGB_BO`'s optimal parameters to the `param_lgb` dictionary and they will be used to train a model with 5 fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ab8ef627687c7602b81a9ac83f90ff3a2094d0c"
   },
   "source": [
    "Number of Kfolds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "_uuid": "9f306e20ae748715da17a2f15702ea1aa4d81497"
   },
   "outputs": [],
   "source": [
    "nfold = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_uuid": "f2f01a6006dcb34ffb53ca33a055e592ee9e75e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 1\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's auc: 0.925\n",
      "\n",
      "fold 2\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's auc: 0.7\n",
      "\n",
      "fold 3\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's auc: 0.6\n",
      "\n",
      "fold 4\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's auc: 0.7\n",
      "\n",
      "fold 5\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's auc: 0.7625\n",
      "\n",
      "fold 6\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's auc: 1\n",
      "\n",
      "fold 7\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's auc: 0.8\n",
      "\n",
      "fold 8\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's auc: 0.875\n",
      "\n",
      "fold 9\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's auc: 0.65\n",
      "\n",
      "fold 10\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's auc: 0.675\n",
      "\n",
      "fold 11\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's auc: 0.78125\n",
      "\n",
      "fold 12\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's auc: 0.875\n",
      "\n",
      "fold 13\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's auc: 0.671875\n",
      "\n",
      "fold 14\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's auc: 1\n",
      "\n",
      "fold 15\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.34375\n",
      "\n",
      "fold 16\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's auc: 0.5625\n",
      "\n",
      "fold 17\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.90625\n",
      "\n",
      "fold 18\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's auc: 0.90625\n",
      "\n",
      "fold 19\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's auc: 0.8125\n",
      "\n",
      "fold 20\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's auc: 0.34375\n",
      "\n",
      "\n",
      "CV AUC: 0.67\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros((len(test_df),nfold))\n",
    "\n",
    "i = 1\n",
    "for train_index, valid_index in skf.split(train_df, train_df.target.values):\n",
    "    print(\"\\nfold {}\".format(i))\n",
    "    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n",
    "                           label=train_df.iloc[train_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )\n",
    "    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n",
    "                           label=train_df.iloc[valid_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )   \n",
    "\n",
    "    \n",
    "    clf = lgb.train(param_lgb, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n",
    "    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n",
    "    \n",
    "    predictions[:,i-1] += clf.predict(test_df[predictors], num_iteration=clf.best_iteration)\n",
    "    i = i + 1\n",
    "\n",
    "print(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6c8fde3d71858028688bd9a62bf1a4430b6bbb1"
   },
   "source": [
    "So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "_uuid": "a889d2c1f63dbe989b3f0b0373ce9e1c80e3ee10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52191212, 0.52273183, 0.53406855, ..., 0.52909054, 0.5099173 ,\n",
       "        0.52058333],\n",
       "       [0.5301896 , 0.55323109, 0.54883977, ..., 0.52899379, 0.50856721,\n",
       "        0.57193042],\n",
       "       [0.50454907, 0.50679613, 0.4855532 , ..., 0.5233784 , 0.50314271,\n",
       "        0.51140575],\n",
       "       ...,\n",
       "       [0.50023519, 0.50301422, 0.48504433, ..., 0.5044876 , 0.50003892,\n",
       "        0.49861962],\n",
       "       [0.53167874, 0.55167153, 0.5934861 , ..., 0.56028022, 0.5153673 ,\n",
       "        0.5798077 ],\n",
       "       [0.51816932, 0.5118961 , 0.5223954 , ..., 0.52908129, 0.5028021 ,\n",
       "        0.50452161]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01339e95305a72802fd23a2c740cb7cc988d1c27"
   },
   "source": [
    "If you are still reading, bare with me. I will not take much of your time. :D We are almost done. Let's do a rank averaging on 5 fold predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "116f2c0c3c43095d456a88d50425de0a3e7fdd11"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## 4. Rank averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "6ec40259b4636edd2c336583c5eb5c62feaaaa31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank averaging on 20 fold predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank averaging on\", nfold, \"fold predictions\")\n",
    "rank_predictions = np.zeros((predictions.shape[0],1))\n",
    "for i in range(nfold):\n",
    "    rank_predictions[:, 0] = np.add(rank_predictions[:, 0], rankdata(predictions[:, i].reshape(-1,1))/rank_predictions.shape[0]) \n",
    "\n",
    "rank_predictions /= nfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74eea39312bae04f6542a2f557c0454eda19d2f3"
   },
   "source": [
    "Let's submit prediction to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08a4c881ae24d784e9ee3c197d1188e26fdb40c7"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## 5. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "1fe8a68970387c8e57d373b4d0944731a32ccd51"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.277034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251</td>\n",
       "      <td>0.358053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>0.453792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>0.738590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>0.443934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>255</td>\n",
       "      <td>0.281243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>256</td>\n",
       "      <td>0.444399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>257</td>\n",
       "      <td>0.212629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>258</td>\n",
       "      <td>0.792330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>259</td>\n",
       "      <td>0.195742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id    target\n",
       "0  250  0.277034\n",
       "1  251  0.358053\n",
       "2  252  0.453792\n",
       "3  253  0.738590\n",
       "4  254  0.443934\n",
       "5  255  0.281243\n",
       "6  256  0.444399\n",
       "7  257  0.212629\n",
       "8  258  0.792330\n",
       "9  259  0.195742"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.DataFrame({\"id\": test_df.id.values})\n",
    "sub_df[\"target\"] = rank_predictions\n",
    "sub_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "65cd74398d3f80186d65a2ff3431403a6846e229"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"C:\\\\PythonScripts\\\\Kaggle\\\\dont_overfitt\\\\submission_baysian_strat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Do not forget to upvote :) Also fork and modify for your own use. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "497ff211885b3ab9864adf2d2938d2d06050974d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
