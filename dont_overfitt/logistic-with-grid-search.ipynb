{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.feature_selection import RFE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "cd7f82d2d3d01f58bd4669c995309262b8ff66f7"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ab414c58027f2f54aa6ac9d395af33d34c571ba3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>124.500000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.023292</td>\n",
       "      <td>-0.026872</td>\n",
       "      <td>0.167404</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>-0.007304</td>\n",
       "      <td>0.032052</td>\n",
       "      <td>0.078412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044652</td>\n",
       "      <td>0.126344</td>\n",
       "      <td>0.018436</td>\n",
       "      <td>-0.012092</td>\n",
       "      <td>-0.065720</td>\n",
       "      <td>-0.106112</td>\n",
       "      <td>0.046472</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>-0.128952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72.312977</td>\n",
       "      <td>0.480963</td>\n",
       "      <td>0.998354</td>\n",
       "      <td>1.009314</td>\n",
       "      <td>1.021709</td>\n",
       "      <td>1.011751</td>\n",
       "      <td>1.035411</td>\n",
       "      <td>0.955700</td>\n",
       "      <td>1.006657</td>\n",
       "      <td>0.939731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011416</td>\n",
       "      <td>0.972567</td>\n",
       "      <td>0.954229</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>1.057414</td>\n",
       "      <td>1.038389</td>\n",
       "      <td>0.967661</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>1.008099</td>\n",
       "      <td>0.971219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.319000</td>\n",
       "      <td>-2.931000</td>\n",
       "      <td>-2.477000</td>\n",
       "      <td>-2.359000</td>\n",
       "      <td>-2.566000</td>\n",
       "      <td>-2.845000</td>\n",
       "      <td>-2.976000</td>\n",
       "      <td>-3.444000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.804000</td>\n",
       "      <td>-2.443000</td>\n",
       "      <td>-2.757000</td>\n",
       "      <td>-2.466000</td>\n",
       "      <td>-3.287000</td>\n",
       "      <td>-3.072000</td>\n",
       "      <td>-2.634000</td>\n",
       "      <td>-2.776000</td>\n",
       "      <td>-3.211000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>62.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.644750</td>\n",
       "      <td>-0.739750</td>\n",
       "      <td>-0.425250</td>\n",
       "      <td>-0.686500</td>\n",
       "      <td>-0.659000</td>\n",
       "      <td>-0.643750</td>\n",
       "      <td>-0.675000</td>\n",
       "      <td>-0.550750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617000</td>\n",
       "      <td>-0.510500</td>\n",
       "      <td>-0.535750</td>\n",
       "      <td>-0.657000</td>\n",
       "      <td>-0.818500</td>\n",
       "      <td>-0.821000</td>\n",
       "      <td>-0.605500</td>\n",
       "      <td>-0.751250</td>\n",
       "      <td>-0.550000</td>\n",
       "      <td>-0.754250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>124.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015500</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>-0.016500</td>\n",
       "      <td>-0.023000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>-0.021000</td>\n",
       "      <td>-0.009000</td>\n",
       "      <td>-0.079500</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>-0.009000</td>\n",
       "      <td>-0.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>186.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.620750</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.783250</td>\n",
       "      <td>0.766250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797250</td>\n",
       "      <td>0.804250</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>0.650250</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.794250</td>\n",
       "      <td>0.654250</td>\n",
       "      <td>0.503250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.567000</td>\n",
       "      <td>2.419000</td>\n",
       "      <td>3.392000</td>\n",
       "      <td>2.771000</td>\n",
       "      <td>2.901000</td>\n",
       "      <td>2.793000</td>\n",
       "      <td>2.546000</td>\n",
       "      <td>2.846000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.865000</td>\n",
       "      <td>2.801000</td>\n",
       "      <td>2.736000</td>\n",
       "      <td>2.596000</td>\n",
       "      <td>2.226000</td>\n",
       "      <td>3.131000</td>\n",
       "      <td>3.236000</td>\n",
       "      <td>2.626000</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>2.771000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id      target           0           1           2           3  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
       "mean   124.500000    0.640000    0.023292   -0.026872    0.167404    0.001904   \n",
       "std     72.312977    0.480963    0.998354    1.009314    1.021709    1.011751   \n",
       "min      0.000000    0.000000   -2.319000   -2.931000   -2.477000   -2.359000   \n",
       "25%     62.250000    0.000000   -0.644750   -0.739750   -0.425250   -0.686500   \n",
       "50%    124.500000    1.000000   -0.015500    0.057000    0.184000   -0.016500   \n",
       "75%    186.750000    1.000000    0.677000    0.620750    0.805000    0.720000   \n",
       "max    249.000000    1.000000    2.567000    2.419000    3.392000    2.771000   \n",
       "\n",
       "                4           5           6           7  ...         290  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000  ...  250.000000   \n",
       "mean     0.001588   -0.007304    0.032052    0.078412  ...    0.044652   \n",
       "std      1.035411    0.955700    1.006657    0.939731  ...    1.011416   \n",
       "min     -2.566000   -2.845000   -2.976000   -3.444000  ...   -2.804000   \n",
       "25%     -0.659000   -0.643750   -0.675000   -0.550750  ...   -0.617000   \n",
       "50%     -0.023000    0.037500    0.060500    0.183500  ...    0.067500   \n",
       "75%      0.735000    0.660500    0.783250    0.766250  ...    0.797250   \n",
       "max      2.901000    2.793000    2.546000    2.846000  ...    2.865000   \n",
       "\n",
       "              291         292         293         294         295         296  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
       "mean     0.126344    0.018436   -0.012092   -0.065720   -0.106112    0.046472   \n",
       "std      0.972567    0.954229    0.960630    1.057414    1.038389    0.967661   \n",
       "min     -2.443000   -2.757000   -2.466000   -3.287000   -3.072000   -2.634000   \n",
       "25%     -0.510500   -0.535750   -0.657000   -0.818500   -0.821000   -0.605500   \n",
       "50%      0.091000    0.057500   -0.021000   -0.009000   -0.079500    0.009500   \n",
       "75%      0.804250    0.631500    0.650250    0.739500    0.493000    0.683000   \n",
       "max      2.801000    2.736000    2.596000    2.226000    3.131000    3.236000   \n",
       "\n",
       "              297         298         299  \n",
       "count  250.000000  250.000000  250.000000  \n",
       "mean     0.006452    0.009372   -0.128952  \n",
       "std      0.998984    1.008099    0.971219  \n",
       "min     -2.776000   -3.211000   -3.500000  \n",
       "25%     -0.751250   -0.550000   -0.754250  \n",
       "50%      0.005500   -0.009000   -0.132500  \n",
       "75%      0.794250    0.654250    0.503250  \n",
       "max      2.626000    3.530000    2.771000  \n",
       "\n",
       "[8 rows x 302 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "efce76dd996e514da2d972601eea32bb3ea9f306"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.098022</td>\n",
       "      <td>2.164062</td>\n",
       "      <td>0.681152</td>\n",
       "      <td>-0.613770</td>\n",
       "      <td>1.308594</td>\n",
       "      <td>-0.455078</td>\n",
       "      <td>-0.235962</td>\n",
       "      <td>0.275879</td>\n",
       "      <td>-2.246094</td>\n",
       "      <td>1.825195</td>\n",
       "      <td>-0.912109</td>\n",
       "      <td>-0.106995</td>\n",
       "      <td>0.304932</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>0.826172</td>\n",
       "      <td>0.416992</td>\n",
       "      <td>0.177002</td>\n",
       "      <td>-0.672852</td>\n",
       "      <td>-0.502930</td>\n",
       "      <td>1.864258</td>\n",
       "      <td>0.409912</td>\n",
       "      <td>-1.926758</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>-0.931152</td>\n",
       "      <td>1.762695</td>\n",
       "      <td>1.449219</td>\n",
       "      <td>-1.096680</td>\n",
       "      <td>-0.686035</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-1.859375</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.008789</td>\n",
       "      <td>-2.296875</td>\n",
       "      <td>0.385010</td>\n",
       "      <td>-0.875977</td>\n",
       "      <td>1.528320</td>\n",
       "      <td>-0.144043</td>\n",
       "      <td>-1.078125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.681152</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>-0.564941</td>\n",
       "      <td>-1.318359</td>\n",
       "      <td>-0.922852</td>\n",
       "      <td>0.075012</td>\n",
       "      <td>-0.704102</td>\n",
       "      <td>2.457031</td>\n",
       "      <td>0.770996</td>\n",
       "      <td>-0.459961</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>-1.320312</td>\n",
       "      <td>-1.515625</td>\n",
       "      <td>-2.144531</td>\n",
       "      <td>-1.120117</td>\n",
       "      <td>0.156006</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>-1.048828</td>\n",
       "      <td>-1.125000</td>\n",
       "      <td>0.483887</td>\n",
       "      <td>0.617188</td>\n",
       "      <td>1.252930</td>\n",
       "      <td>1.248047</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>-0.801758</td>\n",
       "      <td>-0.895996</td>\n",
       "      <td>-1.792969</td>\n",
       "      <td>-0.283936</td>\n",
       "      <td>-0.601074</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>1.346680</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>-0.648926</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>-2.097656</td>\n",
       "      <td>1.050781</td>\n",
       "      <td>-0.414062</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>-1.065430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.081055</td>\n",
       "      <td>-0.973145</td>\n",
       "      <td>-0.383057</td>\n",
       "      <td>0.325928</td>\n",
       "      <td>-0.427979</td>\n",
       "      <td>0.316895</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>0.352051</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>-0.291016</td>\n",
       "      <td>2.906250</td>\n",
       "      <td>1.084961</td>\n",
       "      <td>2.144531</td>\n",
       "      <td>1.540039</td>\n",
       "      <td>0.583984</td>\n",
       "      <td>1.132812</td>\n",
       "      <td>1.097656</td>\n",
       "      <td>-0.237061</td>\n",
       "      <td>-0.498047</td>\n",
       "      <td>0.282959</td>\n",
       "      <td>-1.099609</td>\n",
       "      <td>-0.416992</td>\n",
       "      <td>1.381836</td>\n",
       "      <td>-0.515137</td>\n",
       "      <td>-1.518555</td>\n",
       "      <td>0.619141</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>0.866211</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>1.238281</td>\n",
       "      <td>-0.227051</td>\n",
       "      <td>0.269043</td>\n",
       "      <td>-0.389893</td>\n",
       "      <td>-2.720703</td>\n",
       "      <td>1.659180</td>\n",
       "      <td>0.106018</td>\n",
       "      <td>-0.120972</td>\n",
       "      <td>1.718750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971191</td>\n",
       "      <td>-1.489258</td>\n",
       "      <td>0.529785</td>\n",
       "      <td>0.916992</td>\n",
       "      <td>-0.093994</td>\n",
       "      <td>-1.407227</td>\n",
       "      <td>0.887207</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>-0.583008</td>\n",
       "      <td>1.266602</td>\n",
       "      <td>-1.666992</td>\n",
       "      <td>-2.771484</td>\n",
       "      <td>-0.516113</td>\n",
       "      <td>1.311523</td>\n",
       "      <td>0.490967</td>\n",
       "      <td>0.932129</td>\n",
       "      <td>2.064453</td>\n",
       "      <td>0.422119</td>\n",
       "      <td>1.214844</td>\n",
       "      <td>2.011719</td>\n",
       "      <td>0.042999</td>\n",
       "      <td>-0.306885</td>\n",
       "      <td>-0.058990</td>\n",
       "      <td>1.121094</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.211060</td>\n",
       "      <td>1.752930</td>\n",
       "      <td>0.053009</td>\n",
       "      <td>1.274414</td>\n",
       "      <td>-0.611816</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>-1.695312</td>\n",
       "      <td>-1.256836</td>\n",
       "      <td>1.359375</td>\n",
       "      <td>-0.808105</td>\n",
       "      <td>-1.624023</td>\n",
       "      <td>-0.458008</td>\n",
       "      <td>-1.098633</td>\n",
       "      <td>-0.936035</td>\n",
       "      <td>0.973145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.522949</td>\n",
       "      <td>-0.088989</td>\n",
       "      <td>-0.347900</td>\n",
       "      <td>0.147949</td>\n",
       "      <td>-0.022003</td>\n",
       "      <td>0.404053</td>\n",
       "      <td>-0.022995</td>\n",
       "      <td>-0.171997</td>\n",
       "      <td>0.136963</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.458984</td>\n",
       "      <td>0.478027</td>\n",
       "      <td>-0.425049</td>\n",
       "      <td>0.352051</td>\n",
       "      <td>1.094727</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>-1.043945</td>\n",
       "      <td>0.270020</td>\n",
       "      <td>-1.038086</td>\n",
       "      <td>0.144043</td>\n",
       "      <td>-1.658203</td>\n",
       "      <td>-0.945801</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>-0.771973</td>\n",
       "      <td>1.786133</td>\n",
       "      <td>0.135986</td>\n",
       "      <td>-0.103027</td>\n",
       "      <td>-1.222656</td>\n",
       "      <td>2.273438</td>\n",
       "      <td>0.054993</td>\n",
       "      <td>-2.031250</td>\n",
       "      <td>-0.451904</td>\n",
       "      <td>0.064026</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.691895</td>\n",
       "      <td>-0.067017</td>\n",
       "      <td>-0.916992</td>\n",
       "      <td>1.896484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>-0.299072</td>\n",
       "      <td>1.074219</td>\n",
       "      <td>-0.748047</td>\n",
       "      <td>1.085938</td>\n",
       "      <td>-0.766113</td>\n",
       "      <td>-0.931152</td>\n",
       "      <td>0.431885</td>\n",
       "      <td>1.344727</td>\n",
       "      <td>-0.490967</td>\n",
       "      <td>-1.601562</td>\n",
       "      <td>-0.727051</td>\n",
       "      <td>0.345947</td>\n",
       "      <td>0.779785</td>\n",
       "      <td>-0.526855</td>\n",
       "      <td>-1.122070</td>\n",
       "      <td>-0.208008</td>\n",
       "      <td>-0.729980</td>\n",
       "      <td>-0.302002</td>\n",
       "      <td>2.535156</td>\n",
       "      <td>-1.044922</td>\n",
       "      <td>0.036987</td>\n",
       "      <td>0.020004</td>\n",
       "      <td>1.373047</td>\n",
       "      <td>0.456055</td>\n",
       "      <td>-0.277100</td>\n",
       "      <td>1.380859</td>\n",
       "      <td>1.842773</td>\n",
       "      <td>0.749023</td>\n",
       "      <td>0.202026</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>-1.221680</td>\n",
       "      <td>0.726074</td>\n",
       "      <td>1.444336</td>\n",
       "      <td>-1.165039</td>\n",
       "      <td>-1.543945</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.799805</td>\n",
       "      <td>-1.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067017</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>0.392090</td>\n",
       "      <td>-1.636719</td>\n",
       "      <td>-0.446045</td>\n",
       "      <td>-0.725098</td>\n",
       "      <td>-1.035156</td>\n",
       "      <td>0.833984</td>\n",
       "      <td>0.502930</td>\n",
       "      <td>0.273926</td>\n",
       "      <td>0.334961</td>\n",
       "      <td>-1.148438</td>\n",
       "      <td>0.067017</td>\n",
       "      <td>-1.009766</td>\n",
       "      <td>1.047852</td>\n",
       "      <td>-1.442383</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>-0.325928</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>-0.764160</td>\n",
       "      <td>0.248047</td>\n",
       "      <td>-1.307617</td>\n",
       "      <td>2.126953</td>\n",
       "      <td>0.364990</td>\n",
       "      <td>0.295898</td>\n",
       "      <td>-0.808105</td>\n",
       "      <td>1.853516</td>\n",
       "      <td>0.117981</td>\n",
       "      <td>0.379883</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>-1.170898</td>\n",
       "      <td>2.798828</td>\n",
       "      <td>0.394043</td>\n",
       "      <td>-1.047852</td>\n",
       "      <td>1.078125</td>\n",
       "      <td>0.400879</td>\n",
       "      <td>-0.486084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083008</td>\n",
       "      <td>-0.831055</td>\n",
       "      <td>1.250977</td>\n",
       "      <td>-0.206055</td>\n",
       "      <td>-0.933105</td>\n",
       "      <td>-1.214844</td>\n",
       "      <td>0.281006</td>\n",
       "      <td>0.512207</td>\n",
       "      <td>-0.424072</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>-0.709961</td>\n",
       "      <td>2.724609</td>\n",
       "      <td>0.176025</td>\n",
       "      <td>0.845215</td>\n",
       "      <td>-1.225586</td>\n",
       "      <td>1.527344</td>\n",
       "      <td>-1.701172</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>0.150024</td>\n",
       "      <td>1.864258</td>\n",
       "      <td>0.322021</td>\n",
       "      <td>-0.213989</td>\n",
       "      <td>1.282227</td>\n",
       "      <td>0.407959</td>\n",
       "      <td>-0.910156</td>\n",
       "      <td>1.019531</td>\n",
       "      <td>-0.299072</td>\n",
       "      <td>-1.574219</td>\n",
       "      <td>-1.618164</td>\n",
       "      <td>-0.404053</td>\n",
       "      <td>0.640137</td>\n",
       "      <td>-0.595215</td>\n",
       "      <td>-0.965820</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>0.467041</td>\n",
       "      <td>-0.562012</td>\n",
       "      <td>-0.253906</td>\n",
       "      <td>-0.533203</td>\n",
       "      <td>0.238037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.347656</td>\n",
       "      <td>-0.831055</td>\n",
       "      <td>0.511230</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>1.224609</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>0.584961</td>\n",
       "      <td>1.508789</td>\n",
       "      <td>-0.012001</td>\n",
       "      <td>2.197266</td>\n",
       "      <td>0.189941</td>\n",
       "      <td>0.452881</td>\n",
       "      <td>0.493896</td>\n",
       "      <td>1.477539</td>\n",
       "      <td>-1.412109</td>\n",
       "      <td>0.270020</td>\n",
       "      <td>-1.311523</td>\n",
       "      <td>-0.322021</td>\n",
       "      <td>-0.687988</td>\n",
       "      <td>-0.197998</td>\n",
       "      <td>-0.284912</td>\n",
       "      <td>1.041992</td>\n",
       "      <td>-0.314941</td>\n",
       "      <td>-0.478027</td>\n",
       "      <td>0.024002</td>\n",
       "      <td>-0.189941</td>\n",
       "      <td>1.656250</td>\n",
       "      <td>-0.468994</td>\n",
       "      <td>-1.436523</td>\n",
       "      <td>-0.581055</td>\n",
       "      <td>-0.308105</td>\n",
       "      <td>-0.836914</td>\n",
       "      <td>-1.739258</td>\n",
       "      <td>0.036987</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>-1.101562</td>\n",
       "      <td>2.371094</td>\n",
       "      <td>0.554199</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.049805</td>\n",
       "      <td>-0.346924</td>\n",
       "      <td>0.903809</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>-0.849121</td>\n",
       "      <td>3.431641</td>\n",
       "      <td>0.222046</td>\n",
       "      <td>0.416016</td>\n",
       "      <td>0.173950</td>\n",
       "      <td>-1.516602</td>\n",
       "      <td>-0.336914</td>\n",
       "      <td>0.054993</td>\n",
       "      <td>-0.464111</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>-1.073242</td>\n",
       "      <td>0.324951</td>\n",
       "      <td>-0.522949</td>\n",
       "      <td>-0.691895</td>\n",
       "      <td>0.189941</td>\n",
       "      <td>-0.882812</td>\n",
       "      <td>-1.830078</td>\n",
       "      <td>1.408203</td>\n",
       "      <td>2.318359</td>\n",
       "      <td>1.704102</td>\n",
       "      <td>-0.723145</td>\n",
       "      <td>1.013672</td>\n",
       "      <td>0.064026</td>\n",
       "      <td>0.096008</td>\n",
       "      <td>-0.774902</td>\n",
       "      <td>1.844727</td>\n",
       "      <td>0.897949</td>\n",
       "      <td>0.134033</td>\n",
       "      <td>2.414062</td>\n",
       "      <td>-0.996094</td>\n",
       "      <td>-1.005859</td>\n",
       "      <td>1.377930</td>\n",
       "      <td>1.246094</td>\n",
       "      <td>1.477539</td>\n",
       "      <td>0.427979</td>\n",
       "      <td>0.252930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target         0    ...          297       298       299\n",
       "0   0     1.0 -0.098022    ...    -0.414062  1.038086 -1.065430\n",
       "1   1     0.0  1.081055    ...    -1.098633 -0.936035  0.973145\n",
       "2   2     1.0 -0.522949    ...     0.004002  0.799805 -1.210938\n",
       "3   3     1.0  0.067017    ...    -0.253906 -0.533203  0.238037\n",
       "4   4     1.0  2.347656    ...     1.477539  0.427979  0.252930\n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4ca6aeecad24f4c0e49e75aebb9e0d80e3a81acc"
   },
   "outputs": [],
   "source": [
    "# drop the columns which are not required for modelling like ID. \n",
    "target = train_df[\"target\"]\n",
    "train_df = train_df.drop([\"target\",\"id\"],axis=1)\n",
    "test_id = test_df[\"id\"]\n",
    "test_df = test_df.drop([\"id\"],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "2824e5077e161795df7a43606f348daed9230130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Target')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(target,kde=False,color='b')\n",
    "plt.title(\"Target Distribution\",fontsize=16)\n",
    "plt.xlabel(\"Target Freq\",fontsize=12)\n",
    "plt.ylabel(\"Target\",fontsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e092c0fe7d7c9d6743602eaf882682e458a1bfb5"
   },
   "source": [
    "We have 160 1's and 90 0's. Next lets check if any of the features are co-rellated. Corr gives the correlation, we will use heat map for quick visual inspaction. There are no signs of correlation. Creator of competition did quite a work to ensure that there is no correlation among features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns, index=train_df.index)\n",
    "X_test = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "4680455d76e96ebe9a0ba637535c3b657b7fe316"
   },
   "outputs": [],
   "source": [
    "# split the training into 0.75 train and 0.25 test for cross validation\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "shuffle_split = StratifiedShuffleSplit(test_size=0.2,train_size=0.8,n_splits=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "4c6c53f014a18775206e4f6e28e5a2166a5e95d2"
   },
   "outputs": [],
   "source": [
    "param_grid = {'C'     : [1,0.01,0.1,0.9,0.8,2,0.5,0.4,0.7],\n",
    "              'penalty' : [\"l1\", \"l2\"],\n",
    "              'class_weight' : [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3},{1:0.8, 0:0.2}],\n",
    "             # 'warm_start ' : [True,False],\n",
    "              'intercept_scaling':[1,0.9,0.8,0.7,0.6,0.5],\n",
    "               'solver' : ['liblinear', 'saga']\n",
    "                     \n",
    "          }\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = LogisticRegression(random_state=42,solver=\"liblinear\"),\n",
    "    param_grid = param_grid, \n",
    "     scoring='roc_auc',\n",
    "    cv = shuffle_split,\n",
    "    n_jobs=-1\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "ded36fe35764fd296f649560d07309a1d0f92c5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=50, random_state=None, test_size=0.2,\n",
       "            train_size=0.8),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'C': [1, 0.01, 0.1, 0.9, 0.8, 2, 0.5, 0.4, 0.7], 'penalty': ['l1', 'l2'], 'class_weight': [{1: 0.5, 0: 0.5}, {1: 0.4, 0: 0.6}, {1: 0.6, 0: 0.4}, {1: 0.7, 0: 0.3}, {1: 0.8, 0: 0.2}], 'intercept_scaling': [1, 0.9, 0.8, 0.7, 0.6, 0.5], 'solver': ['liblinear', 'saga']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(train_df,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "ba7bebb27ccee081e68fedf8c9e470d79174698b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : {'C': 0.4, 'class_weight': {1: 0.4, 0: 0.6}, 'intercept_scaling': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best cross validation score: 0.79\n",
      "Best estimator: LogisticRegression(C=0.4, class_weight={1: 0.4, 0: 0.6}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=42,\n",
      "          solver='saga', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters : {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Best estimator: {}\".format(grid_search.best_estimator_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_table(model, subtitle):\n",
    "    scores = ['roc_auc']\n",
    "    res = []\n",
    "    for sc in scores:\n",
    "        scores = cross_val_score(model, train_df,target, cv = 10, scoring = sc)\n",
    "        res.append(scores)\n",
    "    df = pd.DataFrame(res).T\n",
    "    df.loc['mean'] = df.mean()\n",
    "    df.loc['std'] = df.std()\n",
    "    df= df.rename(columns={0: 'accuracy', 1:'roc_auc'})\n",
    "\n",
    "#     trace = go.Table(\n",
    "#         header=dict(values=['<b>Fold', '<b>Accuracy','<b>Roc auc'],\n",
    "#                     line = dict(color='#7D7F80'),\n",
    "#                     fill = dict(color='#a1c3d1'),\n",
    "#                     align = ['center'],\n",
    "#                     font = dict(size = 15)),\n",
    "#         cells=dict(values=[('1','2','3','4','5','mean', 'std'),\n",
    "#                            np.round(df['accuracy'],3),\n",
    "#                            np.round(df['roc_auc'],3)],\n",
    "#                    line = dict(color='#7D7F80'),\n",
    "#                    fill = dict(color='#EDFAFF'),\n",
    "#                    align = ['center'], font = dict(size = 15)))\n",
    "\n",
    "#     layout = dict(width=800, height=400, title = '<b>Cross Validation - 5 folds</b><br>'+subtitle, font = dict(size = 15))\n",
    "#     fig = dict(data=[trace], layout=layout)\n",
    "\n",
    "#     py.iplot(fig, filename = 'styled_table')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.902778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.965278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.840278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.868056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.847222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.089532</td>\n",
       "      <td>0.112665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy   roc_auc\n",
       "0     0.800000  0.902778\n",
       "1     0.560000  0.569444\n",
       "2     0.680000  0.687500\n",
       "3     0.800000  0.965278\n",
       "4     0.880000  0.840278\n",
       "5     0.720000  0.770833\n",
       "6     0.800000  0.868056\n",
       "7     0.680000  0.805556\n",
       "8     0.640000  0.680556\n",
       "9     0.760000  0.847222\n",
       "mean  0.732000  0.793750\n",
       "std   0.089532  0.112665"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(**grid_search.best_params_)\n",
    "log_clf.fit(train_df,target)\n",
    "\n",
    "selector = RFE(log_clf, 25, step=1)\n",
    "selector.fit(train_df,target)\n",
    "scores_table(selector, 'selector_clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=50, random_state=None, test_size=0.2,\n",
       "            train_size=0.8),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=10000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'alpha': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'l1_ratio': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'tol': [0.0001, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_en = {\n",
    "                'alpha'     : np.arange(0.1,1.00,0.10),\n",
    "                'l1_ratio'  :  np.arange(0.1,1.00,0.10),\n",
    "                'tol'       : [0.0001,0.001]\n",
    "            }\n",
    "eNet = ElasticNet(max_iter=10000)\n",
    "grid_search_en = GridSearchCV(eNet, \n",
    "                           param_grid_en, \n",
    "                           scoring='roc_auc', \n",
    "                           cv = shuffle_split,\n",
    "                           return_train_score=True,\n",
    "                           n_jobs = -1)\n",
    "grid_search_en.fit(train_df,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : {'alpha': 0.1, 'l1_ratio': 0.5, 'tol': 0.001}\n",
      "Best cross validation score: 0.78\n",
      "Best estimator: ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=10000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters : {}\".format(grid_search_en.best_params_))\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_search_en.best_score_))\n",
    "print(\"Best estimator: {}\".format(grid_search_en.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.850877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.896104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.756944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.878205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.814103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.799980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.063746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy\n",
       "0     0.850877\n",
       "1     0.809524\n",
       "2     0.666667\n",
       "3     0.785714\n",
       "4     0.896104\n",
       "5     0.756944\n",
       "6     0.878205\n",
       "7     0.750000\n",
       "8     0.791667\n",
       "9     0.814103\n",
       "mean  0.799980\n",
       "std   0.063746"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_clf = ElasticNet(**grid_search_en.best_params_)\n",
    "elastic_clf.fit(train_df,target)\n",
    "\n",
    "selector_elastic = RFE(elastic_clf, 25, step=1)\n",
    "selector_elastic.fit(train_df,target)\n",
    "scores_table(selector_elastic, 'selector_clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "836afa20c3298fe59d093ec9e66936609dbb773a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_intercept_scaling</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>split42_train_score</th>\n",
       "      <th>split43_train_score</th>\n",
       "      <th>split44_train_score</th>\n",
       "      <th>split45_train_score</th>\n",
       "      <th>split46_train_score</th>\n",
       "      <th>split47_train_score</th>\n",
       "      <th>split48_train_score</th>\n",
       "      <th>split49_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>1</td>\n",
       "      <td>{1: 0.5, 0: 0.5}</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 1, 'class_weight': {1: 0.5, 0: 0.5}, 'in...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.012912      0.005532         0.004197        0.002637       1   \n",
       "\n",
       "  param_class_weight param_intercept_scaling param_penalty param_solver  \\\n",
       "0   {1: 0.5, 0: 0.5}                       1            l1    liblinear   \n",
       "\n",
       "                                              params  ...  \\\n",
       "0  {'C': 1, 'class_weight': {1: 0.5, 0: 0.5}, 'in...  ...   \n",
       "\n",
       "   split42_train_score  split43_train_score  split44_train_score  \\\n",
       "0                  1.0                  1.0                  1.0   \n",
       "\n",
       "   split45_train_score  split46_train_score  split47_train_score  \\\n",
       "0                  1.0                  1.0                  1.0   \n",
       "\n",
       "   split48_train_score  split49_train_score  mean_train_score  std_train_score  \n",
       "0                  1.0                  1.0               1.0              0.0  \n",
       "\n",
       "[1 rows x 115 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "634edecd627ed34bf6f9f4957adbbc5160d58846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean CV scores for each fold [0.77319444 0.74569444 0.77263889 0.74506944 0.77138889 0.74475694\n",
      " 0.76944444 0.74440972 0.7678125  0.74399306 0.76565972 0.74263889\n",
      " 0.77350694 0.74694444 0.77309028 0.74611111 0.771875   0.74565972\n",
      " 0.76986111 0.74520833 0.76822917 0.74458333 0.76548611 0.74447917\n",
      " 0.77364583 0.74440972 0.7725     0.744375   0.77069444 0.74371528\n",
      " 0.7696875  0.74253472 0.76788194 0.74173611 0.76496528 0.7396875\n",
      " 0.77336806 0.74395833 0.77208333 0.74305556 0.77076389 0.74194444\n",
      " 0.76927083 0.739375   0.7675     0.73774306 0.76444444 0.73583333\n",
      " 0.77298611 0.74184028 0.7721875  0.73930556 0.77131944 0.73760417\n",
      " 0.76930556 0.73576389 0.76746528 0.73395833 0.7640625  0.73201389\n",
      " 0.5        0.74253472 0.5        0.74256944 0.5        0.74225694\n",
      " 0.5        0.74163194 0.5        0.74135417 0.5        0.74079861\n",
      " 0.5        0.74888889 0.5        0.74875    0.5        0.74857639\n",
      " 0.5        0.74868056 0.5        0.7484375  0.5        0.74819444\n",
      " 0.5        0.73527778 0.5        0.73465278 0.5        0.73378472\n",
      " 0.5        0.73291667 0.5        0.731875   0.5        0.73111111\n",
      " 0.5        0.72232639 0.5        0.72145833 0.5        0.72038194\n",
      " 0.5        0.71920139 0.5        0.71819444 0.5        0.71729167\n",
      " 0.5        0.70579861 0.5        0.70409722 0.5        0.70236111\n",
      " 0.5        0.70166667 0.5        0.70086806 0.5        0.7\n",
      " 0.76090278 0.74701389 0.76045139 0.74694444 0.75878472 0.74625\n",
      " 0.75652778 0.74524306 0.75694444 0.74475694 0.75694444 0.74454861\n",
      " 0.76246528 0.74819444 0.76246528 0.74795139 0.76246528 0.74763889\n",
      " 0.76246528 0.74743056 0.76246528 0.74767361 0.76246528 0.74763889\n",
      " 0.75753472 0.74479167 0.75715278 0.74413194 0.75586806 0.74326389\n",
      " 0.75385417 0.74190972 0.75288194 0.740625   0.74684028 0.73975694\n",
      " 0.74930556 0.74111111 0.74729167 0.73920139 0.74670139 0.73822917\n",
      " 0.74357639 0.73625    0.73857639 0.73444444 0.73309028 0.73222222\n",
      " 0.62902778 0.73274306 0.64539931 0.73083333 0.65522569 0.72850694\n",
      " 0.66079861 0.72625    0.67017361 0.72381944 0.67598958 0.72197917\n",
      " 0.77427083 0.74548611 0.77427083 0.74513889 0.77170139 0.74479167\n",
      " 0.77041667 0.74416667 0.76892361 0.74381944 0.7653125  0.74274306\n",
      " 0.77493056 0.7471875  0.7746875  0.74614583 0.77305556 0.74548611\n",
      " 0.7715625  0.74520833 0.76913194 0.74489583 0.76604167 0.74440972\n",
      " 0.77413194 0.74482639 0.77340278 0.74413194 0.77177083 0.74381944\n",
      " 0.77034722 0.74284722 0.76854167 0.74142361 0.76489583 0.73947917\n",
      " 0.77357639 0.74378472 0.77322917 0.74288194 0.7721875  0.74159722\n",
      " 0.77055556 0.73940972 0.76868056 0.73795139 0.76461806 0.73586806\n",
      " 0.77364583 0.74159722 0.77274306 0.73923611 0.77201389 0.73763889\n",
      " 0.77083333 0.73614583 0.76847222 0.73375    0.76371528 0.73163194\n",
      " 0.77513889 0.74559028 0.77479167 0.745      0.77347222 0.74479167\n",
      " 0.77190972 0.74392361 0.76895833 0.74395833 0.76618056 0.74263889\n",
      " 0.77590278 0.74746528 0.77513889 0.7465625  0.77371528 0.74576389\n",
      " 0.77222222 0.74513889 0.76951389 0.74493056 0.76690972 0.744375\n",
      " 0.77472222 0.74489583 0.774375   0.74399306 0.77309028 0.74371528\n",
      " 0.77135417 0.74239583 0.76892361 0.74111111 0.76579861 0.73951389\n",
      " 0.77482639 0.74375    0.77364583 0.74277778 0.77260417 0.74135417\n",
      " 0.77069444 0.73940972 0.76840278 0.73788194 0.76503472 0.73604167\n",
      " 0.77527778 0.74125    0.77416667 0.73923611 0.77274306 0.73777778\n",
      " 0.77104167 0.73559028 0.76875    0.73329861 0.76385417 0.73163194\n",
      " 0.76888889 0.74527778 0.76809028 0.74489583 0.76770833 0.74440972\n",
      " 0.76583333 0.74388889 0.76347222 0.74315972 0.76125    0.741875\n",
      " 0.76885417 0.74618056 0.76861111 0.74559028 0.76795139 0.74513889\n",
      " 0.76618056 0.74465278 0.76371528 0.744375   0.76138889 0.74399306\n",
      " 0.76861111 0.74454861 0.76836806 0.74430556 0.76743056 0.74361111\n",
      " 0.76583333 0.74225694 0.76381944 0.74135417 0.76125    0.74034722\n",
      " 0.76902778 0.74385417 0.76847222 0.74336806 0.76774306 0.74190972\n",
      " 0.76586806 0.74076389 0.76371528 0.73902778 0.76149306 0.73666667\n",
      " 0.76979167 0.74215278 0.76895833 0.74104167 0.76798611 0.73923611\n",
      " 0.76659722 0.73697917 0.765      0.73447917 0.76267361 0.73267361\n",
      " 0.78059028 0.74635417 0.78010417 0.74583333 0.77861111 0.74472222\n",
      " 0.77704861 0.74461806 0.77420139 0.74371528 0.7709375  0.74302083\n",
      " 0.78180556 0.74822917 0.78055556 0.74701389 0.77930556 0.74684028\n",
      " 0.77729167 0.74631944 0.7746875  0.74541667 0.77246528 0.74475694\n",
      " 0.78125    0.74479167 0.78083333 0.74427083 0.77902778 0.7434375\n",
      " 0.77677083 0.7425     0.77388889 0.74114583 0.76965278 0.73975694\n",
      " 0.78017361 0.74336806 0.77878472 0.74211806 0.77798611 0.74086806\n",
      " 0.77631944 0.73934028 0.77340278 0.73791667 0.76878472 0.73604167\n",
      " 0.78125    0.74079861 0.78024306 0.73916667 0.77927083 0.73736111\n",
      " 0.77743056 0.73447917 0.77354167 0.73243056 0.76833333 0.73052083\n",
      " 0.78413194 0.74680556 0.78326389 0.74611111 0.78163194 0.74510417\n",
      " 0.77965278 0.74444444 0.77729167 0.74440972 0.77350694 0.74361111\n",
      " 0.78604167 0.74784722 0.78527778 0.74725694 0.78368056 0.74722222\n",
      " 0.78173611 0.7465625  0.77815972 0.74600694 0.77517361 0.74493056\n",
      " 0.78368056 0.74496528 0.78243056 0.74440972 0.78173611 0.74388889\n",
      " 0.77899306 0.74277778 0.77670139 0.74114583 0.77267361 0.74017361\n",
      " 0.78368056 0.74378472 0.78232639 0.74194444 0.78083333 0.74065972\n",
      " 0.77944444 0.73913194 0.77527778 0.73819444 0.77104167 0.73611111\n",
      " 0.78402778 0.74010417 0.78295139 0.73895833 0.78135417 0.73680556\n",
      " 0.77940972 0.73416667 0.77628472 0.73190972 0.77284722 0.72965278\n",
      " 0.77677083 0.74559028 0.77579861 0.7453125  0.7740625  0.74458333\n",
      " 0.77239583 0.74430556 0.77020833 0.74402778 0.7665625  0.74256944\n",
      " 0.77777778 0.74767361 0.77652778 0.74652778 0.77503472 0.74572917\n",
      " 0.7728125  0.74555556 0.77069444 0.74475694 0.76767361 0.74472222\n",
      " 0.77579861 0.74479167 0.77513889 0.74409722 0.77364583 0.74378472\n",
      " 0.77201389 0.74236111 0.76993056 0.74114583 0.76625    0.73975694\n",
      " 0.77600694 0.74368056 0.77527778 0.74260417 0.77454861 0.74111111\n",
      " 0.77256944 0.73923611 0.77065972 0.7378125  0.76597222 0.73625\n",
      " 0.77697917 0.74118056 0.77618056 0.73902778 0.77440972 0.73739583\n",
      " 0.7721875  0.73534722 0.76930556 0.73326389 0.76427083 0.73152778] \n",
      "std CV scores for each fold [0.05845902 0.06531308 0.05846439 0.06486029 0.0589035  0.0654912\n",
      " 0.05862506 0.06518101 0.05895196 0.06559671 0.05947896 0.06532445\n",
      " 0.05862836 0.06565123 0.05854299 0.06546088 0.05881293 0.06512824\n",
      " 0.05849758 0.06526814 0.05901271 0.06533652 0.05894688 0.06515733\n",
      " 0.05864653 0.06581972 0.0585602  0.06509149 0.05872557 0.06499263\n",
      " 0.05848361 0.06535553 0.0586212  0.06509334 0.05905523 0.06497548\n",
      " 0.05758679 0.0645294  0.05772567 0.06535438 0.05809097 0.06521196\n",
      " 0.05826535 0.06492836 0.05861255 0.06491887 0.05839163 0.06511723\n",
      " 0.0575903  0.06481928 0.05752089 0.06517183 0.05722012 0.06496491\n",
      " 0.0574468  0.06558089 0.05784081 0.0659252  0.05796662 0.06641188\n",
      " 0.         0.0660446  0.         0.06593296 0.         0.06596419\n",
      " 0.         0.06597833 0.         0.06557123 0.         0.0655609\n",
      " 0.         0.06485881 0.         0.06473161 0.         0.06471545\n",
      " 0.         0.0646473  0.         0.06461436 0.         0.06464301\n",
      " 0.         0.06591668 0.         0.06597255 0.         0.06558697\n",
      " 0.         0.0657107  0.         0.0655553  0.         0.06546372\n",
      " 0.         0.06694406 0.         0.06720177 0.         0.06727014\n",
      " 0.         0.06737884 0.         0.0675456  0.         0.06757587\n",
      " 0.         0.06859569 0.         0.06895326 0.         0.0695218\n",
      " 0.         0.06949682 0.         0.06975473 0.         0.07016639\n",
      " 0.06644843 0.06549723 0.06615608 0.06526167 0.06535476 0.06536025\n",
      " 0.06460999 0.06556141 0.0650122  0.06545161 0.0650122  0.06510394\n",
      " 0.06693801 0.06649642 0.06693801 0.0668515  0.06693801 0.06668453\n",
      " 0.06693801 0.06612851 0.06693801 0.06572561 0.06693801 0.06584579\n",
      " 0.06622813 0.06555147 0.06558056 0.06541052 0.06442249 0.0654149\n",
      " 0.06327003 0.06527003 0.0638621  0.06522604 0.06264421 0.06510417\n",
      " 0.06633671 0.06550146 0.06551298 0.06526327 0.06569684 0.06576841\n",
      " 0.06281868 0.06586417 0.06153782 0.06617199 0.06352061 0.0664743\n",
      " 0.09962128 0.06638069 0.09509504 0.06705072 0.09181134 0.06757545\n",
      " 0.08389829 0.06814446 0.0753954  0.06823322 0.07807846 0.06815735\n",
      " 0.05834211 0.06538666 0.05838755 0.06532947 0.0586654  0.06541523\n",
      " 0.05849078 0.06475556 0.05890638 0.06562698 0.05876647 0.06526826\n",
      " 0.05763341 0.0654704  0.05768415 0.06555634 0.05803915 0.06528886\n",
      " 0.05789985 0.06496634 0.05835037 0.06533354 0.05852066 0.06494475\n",
      " 0.05744639 0.06528931 0.05805701 0.06493974 0.05810783 0.0655461\n",
      " 0.05828789 0.06497199 0.05916695 0.06512746 0.05867346 0.0647124\n",
      " 0.05730439 0.06473073 0.05719994 0.06544034 0.05736032 0.06517738\n",
      " 0.0583709  0.06496977 0.05828429 0.0648622  0.0580527  0.06531933\n",
      " 0.05549573 0.06492161 0.05595155 0.06501591 0.05622796 0.06477685\n",
      " 0.05645218 0.06575759 0.05668743 0.06576517 0.05775512 0.06664504\n",
      " 0.05699549 0.06520498 0.05734955 0.06513322 0.05807466 0.06505206\n",
      " 0.05830428 0.06501429 0.05824009 0.06568765 0.05837511 0.06527922\n",
      " 0.05674278 0.06588738 0.05680585 0.06538895 0.05694509 0.06522619\n",
      " 0.05772145 0.06521125 0.05788264 0.06523514 0.05858474 0.0649125\n",
      " 0.0575725  0.06506263 0.05748385 0.06496416 0.05743275 0.06551182\n",
      " 0.05797701 0.06506864 0.05803106 0.06496735 0.05798637 0.0650372\n",
      " 0.05756799 0.06505762 0.05678001 0.06525132 0.056804   0.06478602\n",
      " 0.05705638 0.06504859 0.05746606 0.06514142 0.05780453 0.06578094\n",
      " 0.05503462 0.0645973  0.05496851 0.06448806 0.05386675 0.06501124\n",
      " 0.05505705 0.06570601 0.05551757 0.06604153 0.05641853 0.06640037\n",
      " 0.06001386 0.06556438 0.06028991 0.06535384 0.06027282 0.06552231\n",
      " 0.05983883 0.06435634 0.06007603 0.06494765 0.06067577 0.0651941\n",
      " 0.06032181 0.06629581 0.06037778 0.06597333 0.05995856 0.06541984\n",
      " 0.06005383 0.06533394 0.060375   0.06541851 0.06084931 0.06462645\n",
      " 0.0599248  0.06539588 0.06020486 0.06521222 0.06013861 0.06446135\n",
      " 0.05988918 0.06532782 0.05989353 0.0654912  0.06079487 0.06454987\n",
      " 0.05967371 0.06487316 0.05958151 0.06487175 0.05961543 0.06503965\n",
      " 0.05978276 0.06490961 0.05977595 0.06450529 0.0606476  0.064423\n",
      " 0.05988954 0.06514211 0.05973316 0.06528827 0.05934937 0.06490548\n",
      " 0.05931625 0.06437789 0.06016246 0.06485012 0.05986966 0.06507383\n",
      " 0.05364655 0.06503654 0.05353826 0.06509375 0.05303555 0.06495836\n",
      " 0.05303766 0.06465913 0.05387364 0.06560469 0.05385126 0.06519692\n",
      " 0.05340165 0.06575667 0.05311819 0.06534335 0.0530701  0.0649776\n",
      " 0.05289306 0.06520815 0.05340374 0.06496003 0.05368645 0.06498762\n",
      " 0.05367251 0.06457867 0.05374273 0.06454996 0.05362874 0.06570605\n",
      " 0.05375678 0.06496531 0.05370795 0.06450044 0.05372465 0.06425886\n",
      " 0.05337958 0.06499059 0.05318747 0.06466934 0.05289602 0.06429972\n",
      " 0.05382433 0.06467661 0.05303328 0.06513537 0.05375634 0.0655082\n",
      " 0.05439655 0.06443312 0.05424572 0.06487906 0.05331756 0.06532951\n",
      " 0.05242711 0.06587933 0.05252608 0.06615348 0.05336746 0.06726734\n",
      " 0.05206089 0.06512649 0.05191449 0.06498397 0.05181562 0.06484108\n",
      " 0.05188485 0.06486925 0.05195243 0.06468247 0.0519329  0.06522183\n",
      " 0.05190311 0.06574299 0.05150892 0.06587538 0.0510995  0.06550916\n",
      " 0.05104408 0.065482   0.05204199 0.06506897 0.05222233 0.06491091\n",
      " 0.05258096 0.06476998 0.05293908 0.06471601 0.052403   0.06500964\n",
      " 0.05211255 0.06464524 0.05174297 0.06454435 0.05217285 0.06439602\n",
      " 0.05365679 0.06450029 0.05322011 0.06444762 0.05313925 0.06452347\n",
      " 0.05292829 0.06522772 0.05186663 0.06566081 0.05244968 0.06595303\n",
      " 0.05232237 0.06517357 0.05234251 0.06570556 0.05268222 0.065511\n",
      " 0.05314362 0.0660924  0.05334921 0.06697258 0.05348503 0.06723664\n",
      " 0.05624474 0.06541174 0.05649276 0.06501522 0.05619104 0.06504061\n",
      " 0.05683557 0.06498349 0.05710487 0.06588093 0.05766943 0.06511879\n",
      " 0.05564771 0.0657091  0.05570406 0.06534423 0.05524009 0.06514393\n",
      " 0.05584021 0.0650587  0.05704475 0.06535299 0.05812285 0.0649528\n",
      " 0.05655888 0.06501499 0.05647486 0.06494912 0.05620914 0.06560211\n",
      " 0.05634577 0.0650567  0.05686737 0.06496235 0.05724615 0.06503098\n",
      " 0.05540288 0.06513519 0.05540796 0.06482259 0.05511546 0.06460726\n",
      " 0.05592003 0.06484137 0.05580591 0.06525422 0.05727799 0.06546759\n",
      " 0.0535288  0.06462517 0.05393674 0.06426092 0.0539843  0.06520276\n",
      " 0.05408234 0.0658974  0.05461623 0.06592904 0.05582864 0.06663932] \n"
     ]
    }
   ],
   "source": [
    "scores_mean = np.array(results.mean_test_score).reshape(-1)\n",
    "scores_std = np.array(results.std_test_score).reshape(-1)\n",
    "\n",
    "print(\"mean CV scores for each fold {} \".format(scores_mean))\n",
    "print(\"std CV scores for each fold {} \".format(scores_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "06b4a9f5ee615c625d9e36c760cd87ad05da3e5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2391"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "efd0b352855e253c68363eb1f176046e7be133ee"
   },
   "outputs": [],
   "source": [
    "#print( \"Predictions on test set {}\".format(grid_search.predict(test_df)))\n",
    "pred_lr_all_features= grid_search.predict_proba(test_df)[:,1]\n",
    "pred_en_all_features=grid_search_en.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = selector.predict_proba(test_df)[:,1]\n",
    "pred_en=grid_search_en.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "_uuid": "34a0c1eabbb11267bf8af0b27c4decf4f22921f7"
   },
   "outputs": [],
   "source": [
    "pred_lr_en=np.mean([pred_lr,pred_en], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76928252, 0.52082825, 0.68436677, ..., 0.37771706, 0.88207461,\n",
       "       0.31929382])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lr_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "_uuid": "388e2d869bf13339c33c261f5e832e25510edc4a"
   },
   "outputs": [],
   "source": [
    "#print test file \n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"id\"] = test_id \n",
    "sub_df[\"target\"] = pred_lr_en\n",
    "sub_df.to_csv(\"Log_regression_Elasticnet_Grid_Search_top_25_Features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "_uuid": "71aaa3095ae29b1180a3061b7227344aab85d0a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.769283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251</td>\n",
       "      <td>0.520828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>0.684367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>0.848034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>0.639345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id    target\n",
       "0  250  0.769283\n",
       "1  251  0.520828\n",
       "2  252  0.684367\n",
       "3  253  0.848034\n",
       "4  254  0.639345"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "d6eff1462dc9b040a76146d9646ee1bed36b7959"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
