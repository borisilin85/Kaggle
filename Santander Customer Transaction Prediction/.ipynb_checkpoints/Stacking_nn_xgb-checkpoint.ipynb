{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve,auc,log_loss, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,BatchNormalization,Dropout\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,Callback\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.optimizers import SGD\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roc_auc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict_proba(self.x, verbose=0)\n",
    "        roc = roc_auc_score(self.y, y_pred)\n",
    "        logs['roc_auc'] = roc_auc_score(self.y, y_pred)\n",
    "        logs['norm_gini'] = ( roc_auc_score(self.y, y_pred) * 2 ) - 1\n",
    "\n",
    "        y_pred_val = self.model.predict_proba(self.x_val, verbose=0)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        logs['roc_auc_val'] = roc_auc_score(self.y_val, y_pred_val)\n",
    "        logs['norm_gini_val'] = ( roc_auc_score(self.y_val, y_pred_val) * 2 ) - 1\n",
    "\n",
    "        print('\\rroc_auc: %s - roc_auc_val: %s - norm_gini: %s - norm_gini_val: %s' % (str(round(roc,5)),str(round(roc_val,5)),str(round((roc*2-1),5)),str(round((roc_val*2-1),5))), end=10*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train.drop(['target','ID_code'],axis=1,inplace=False)\n",
    "X_test=test.drop('ID_code',axis=1,inplace=False)\n",
    "y=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartA_X, PartB_X, PartA_Y, PartB_Y = train_test_split(X, y, test_size=0.66, random_state=42)\n",
    "#split the test again to get 33% dev and 33% test\n",
    "PartB_X, PartC_X, PartB_Y, PartC_Y = train_test_split(PartB_X, PartB_Y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_PartA_X = xgb.DMatrix(PartA_X, PartA_Y)\n",
    "XGB_PartB_X = xgb.DMatrix(PartB_X, PartB_Y)\n",
    "XGB_PartC_X= xgb.DMatrix(PartC_X, PartC_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": 0.2,\n",
    "        \"tree_method\": 'exact',\n",
    "        \"max_depth\": 17,\n",
    "        \"subsample\": 1,\n",
    "        \"colsample_bytree\": 1,\n",
    "        \"silent\": 1,\n",
    "        \"min_chil_weight\":1,\n",
    "        \"seed\": 42,\n",
    "        #\"num_class\" : 22,\n",
    "    }\n",
    "num_boost_round = 2000\n",
    "early_stopping_rounds = 50\n",
    "watchlist = [(XGB_PartA_X, 'train'), (XGB_PartB_X, 'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.815357\teval-auc:0.626386\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "[1]\ttrain-auc:0.892997\teval-auc:0.675602\n",
      "[2]\ttrain-auc:0.931484\teval-auc:0.695948\n",
      "[3]\ttrain-auc:0.955434\teval-auc:0.706716\n",
      "[4]\ttrain-auc:0.968432\teval-auc:0.715722\n",
      "[5]\ttrain-auc:0.981702\teval-auc:0.723284\n",
      "[6]\ttrain-auc:0.988312\teval-auc:0.729455\n",
      "[7]\ttrain-auc:0.992777\teval-auc:0.734907\n",
      "[8]\ttrain-auc:0.995687\teval-auc:0.73978\n",
      "[9]\ttrain-auc:0.997694\teval-auc:0.743848\n",
      "[10]\ttrain-auc:0.99853\teval-auc:0.749144\n",
      "[11]\ttrain-auc:0.999015\teval-auc:0.755822\n",
      "[12]\ttrain-auc:0.999582\teval-auc:0.759767\n",
      "[13]\ttrain-auc:0.999735\teval-auc:0.764258\n",
      "[14]\ttrain-auc:0.99992\teval-auc:0.767642\n",
      "[15]\ttrain-auc:0.999976\teval-auc:0.771436\n",
      "[16]\ttrain-auc:0.999995\teval-auc:0.775211\n",
      "[17]\ttrain-auc:0.999996\teval-auc:0.777753\n",
      "[18]\ttrain-auc:0.999998\teval-auc:0.780586\n",
      "[19]\ttrain-auc:1\teval-auc:0.783119\n",
      "[20]\ttrain-auc:1\teval-auc:0.786821\n",
      "[21]\ttrain-auc:1\teval-auc:0.789604\n",
      "[22]\ttrain-auc:1\teval-auc:0.791696\n",
      "[23]\ttrain-auc:1\teval-auc:0.793816\n",
      "[24]\ttrain-auc:1\teval-auc:0.796551\n",
      "[25]\ttrain-auc:1\teval-auc:0.798736\n",
      "[26]\ttrain-auc:1\teval-auc:0.800723\n",
      "[27]\ttrain-auc:1\teval-auc:0.80231\n",
      "[28]\ttrain-auc:1\teval-auc:0.80404\n",
      "[29]\ttrain-auc:1\teval-auc:0.805541\n",
      "[30]\ttrain-auc:1\teval-auc:0.80712\n",
      "[31]\ttrain-auc:1\teval-auc:0.808352\n",
      "[32]\ttrain-auc:1\teval-auc:0.810221\n",
      "[33]\ttrain-auc:1\teval-auc:0.811887\n",
      "[34]\ttrain-auc:1\teval-auc:0.812758\n",
      "[35]\ttrain-auc:1\teval-auc:0.814021\n",
      "[36]\ttrain-auc:1\teval-auc:0.815072\n",
      "[37]\ttrain-auc:1\teval-auc:0.816622\n",
      "[38]\ttrain-auc:1\teval-auc:0.817236\n",
      "[39]\ttrain-auc:1\teval-auc:0.818325\n",
      "[40]\ttrain-auc:1\teval-auc:0.8196\n",
      "[41]\ttrain-auc:1\teval-auc:0.820374\n",
      "[42]\ttrain-auc:1\teval-auc:0.821342\n",
      "[43]\ttrain-auc:1\teval-auc:0.821939\n",
      "[44]\ttrain-auc:1\teval-auc:0.823147\n",
      "[45]\ttrain-auc:1\teval-auc:0.824105\n",
      "[46]\ttrain-auc:1\teval-auc:0.825117\n",
      "[47]\ttrain-auc:1\teval-auc:0.826167\n",
      "[48]\ttrain-auc:1\teval-auc:0.827193\n",
      "[49]\ttrain-auc:1\teval-auc:0.828436\n",
      "[50]\ttrain-auc:1\teval-auc:0.829059\n",
      "[51]\ttrain-auc:1\teval-auc:0.82996\n",
      "[52]\ttrain-auc:1\teval-auc:0.83089\n",
      "[53]\ttrain-auc:1\teval-auc:0.831568\n",
      "[54]\ttrain-auc:1\teval-auc:0.832163\n",
      "[55]\ttrain-auc:1\teval-auc:0.83262\n",
      "[56]\ttrain-auc:1\teval-auc:0.833177\n",
      "[57]\ttrain-auc:1\teval-auc:0.833656\n",
      "[58]\ttrain-auc:1\teval-auc:0.834364\n",
      "[59]\ttrain-auc:1\teval-auc:0.834981\n",
      "[60]\ttrain-auc:1\teval-auc:0.835266\n",
      "[61]\ttrain-auc:1\teval-auc:0.83609\n",
      "[62]\ttrain-auc:1\teval-auc:0.836703\n",
      "[63]\ttrain-auc:1\teval-auc:0.837418\n",
      "[64]\ttrain-auc:1\teval-auc:0.837712\n",
      "[65]\ttrain-auc:1\teval-auc:0.838525\n",
      "[66]\ttrain-auc:1\teval-auc:0.839196\n",
      "[67]\ttrain-auc:1\teval-auc:0.839746\n",
      "[68]\ttrain-auc:1\teval-auc:0.840366\n",
      "[69]\ttrain-auc:1\teval-auc:0.840842\n",
      "[70]\ttrain-auc:1\teval-auc:0.841026\n",
      "[71]\ttrain-auc:1\teval-auc:0.841573\n",
      "[72]\ttrain-auc:1\teval-auc:0.842118\n",
      "[73]\ttrain-auc:1\teval-auc:0.842651\n",
      "[74]\ttrain-auc:1\teval-auc:0.843137\n",
      "[75]\ttrain-auc:1\teval-auc:0.843447\n",
      "[76]\ttrain-auc:1\teval-auc:0.843738\n",
      "[77]\ttrain-auc:1\teval-auc:0.844181\n",
      "[78]\ttrain-auc:1\teval-auc:0.844433\n",
      "[79]\ttrain-auc:1\teval-auc:0.844634\n",
      "[80]\ttrain-auc:1\teval-auc:0.845124\n",
      "[81]\ttrain-auc:1\teval-auc:0.845271\n",
      "[82]\ttrain-auc:1\teval-auc:0.845688\n",
      "[83]\ttrain-auc:1\teval-auc:0.84616\n",
      "[84]\ttrain-auc:1\teval-auc:0.84648\n",
      "[85]\ttrain-auc:1\teval-auc:0.84687\n",
      "[86]\ttrain-auc:1\teval-auc:0.847237\n",
      "[87]\ttrain-auc:1\teval-auc:0.847552\n",
      "[88]\ttrain-auc:1\teval-auc:0.847912\n",
      "[89]\ttrain-auc:1\teval-auc:0.848248\n",
      "[90]\ttrain-auc:1\teval-auc:0.848605\n",
      "[91]\ttrain-auc:1\teval-auc:0.848857\n",
      "[92]\ttrain-auc:1\teval-auc:0.849377\n",
      "[93]\ttrain-auc:1\teval-auc:0.849605\n",
      "[94]\ttrain-auc:1\teval-auc:0.849842\n",
      "[95]\ttrain-auc:1\teval-auc:0.850174\n",
      "[96]\ttrain-auc:1\teval-auc:0.850505\n",
      "[97]\ttrain-auc:1\teval-auc:0.850704\n",
      "[98]\ttrain-auc:1\teval-auc:0.851129\n",
      "[99]\ttrain-auc:1\teval-auc:0.851531\n",
      "[100]\ttrain-auc:1\teval-auc:0.851707\n",
      "[101]\ttrain-auc:1\teval-auc:0.852045\n",
      "[102]\ttrain-auc:1\teval-auc:0.852403\n",
      "[103]\ttrain-auc:1\teval-auc:0.852605\n",
      "[104]\ttrain-auc:1\teval-auc:0.852855\n",
      "[105]\ttrain-auc:1\teval-auc:0.853078\n",
      "[106]\ttrain-auc:1\teval-auc:0.853278\n",
      "[107]\ttrain-auc:1\teval-auc:0.853473\n",
      "[108]\ttrain-auc:1\teval-auc:0.853629\n",
      "[109]\ttrain-auc:1\teval-auc:0.853763\n",
      "[110]\ttrain-auc:1\teval-auc:0.85409\n",
      "[111]\ttrain-auc:1\teval-auc:0.85436\n",
      "[112]\ttrain-auc:1\teval-auc:0.854688\n",
      "[113]\ttrain-auc:1\teval-auc:0.854898\n",
      "[114]\ttrain-auc:1\teval-auc:0.855233\n",
      "[115]\ttrain-auc:1\teval-auc:0.85551\n",
      "[116]\ttrain-auc:1\teval-auc:0.855741\n",
      "[117]\ttrain-auc:1\teval-auc:0.855948\n",
      "[118]\ttrain-auc:1\teval-auc:0.856098\n",
      "[119]\ttrain-auc:1\teval-auc:0.856286\n",
      "[120]\ttrain-auc:1\teval-auc:0.856533\n",
      "[121]\ttrain-auc:1\teval-auc:0.856664\n",
      "[122]\ttrain-auc:1\teval-auc:0.856864\n",
      "[123]\ttrain-auc:1\teval-auc:0.856976\n",
      "[124]\ttrain-auc:1\teval-auc:0.857139\n",
      "[125]\ttrain-auc:1\teval-auc:0.85724\n",
      "[126]\ttrain-auc:1\teval-auc:0.857423\n",
      "[127]\ttrain-auc:1\teval-auc:0.857466\n",
      "[128]\ttrain-auc:1\teval-auc:0.857641\n",
      "[129]\ttrain-auc:1\teval-auc:0.85783\n",
      "[130]\ttrain-auc:1\teval-auc:0.858084\n",
      "[131]\ttrain-auc:1\teval-auc:0.858256\n",
      "[132]\ttrain-auc:1\teval-auc:0.858444\n",
      "[133]\ttrain-auc:1\teval-auc:0.858566\n",
      "[134]\ttrain-auc:1\teval-auc:0.858736\n",
      "[135]\ttrain-auc:1\teval-auc:0.858926\n",
      "[136]\ttrain-auc:1\teval-auc:0.859112\n",
      "[137]\ttrain-auc:1\teval-auc:0.859191\n",
      "[138]\ttrain-auc:1\teval-auc:0.85942\n",
      "[139]\ttrain-auc:1\teval-auc:0.859488\n",
      "[140]\ttrain-auc:1\teval-auc:0.85954\n",
      "[141]\ttrain-auc:1\teval-auc:0.859668\n",
      "[142]\ttrain-auc:1\teval-auc:0.859812\n",
      "[143]\ttrain-auc:1\teval-auc:0.859844\n",
      "[144]\ttrain-auc:1\teval-auc:0.859974\n",
      "[145]\ttrain-auc:1\teval-auc:0.860073\n",
      "[146]\ttrain-auc:1\teval-auc:0.860147\n",
      "[147]\ttrain-auc:1\teval-auc:0.860331\n",
      "[148]\ttrain-auc:1\teval-auc:0.860598\n",
      "[149]\ttrain-auc:1\teval-auc:0.860703\n",
      "[150]\ttrain-auc:1\teval-auc:0.86103\n",
      "[151]\ttrain-auc:1\teval-auc:0.861065\n",
      "[152]\ttrain-auc:1\teval-auc:0.861232\n",
      "[153]\ttrain-auc:1\teval-auc:0.861333\n",
      "[154]\ttrain-auc:1\teval-auc:0.861413\n",
      "[155]\ttrain-auc:1\teval-auc:0.861481\n",
      "[156]\ttrain-auc:1\teval-auc:0.861598\n",
      "[157]\ttrain-auc:1\teval-auc:0.861725\n",
      "[158]\ttrain-auc:1\teval-auc:0.861869\n",
      "[159]\ttrain-auc:1\teval-auc:0.861942\n",
      "[160]\ttrain-auc:1\teval-auc:0.862111\n",
      "[161]\ttrain-auc:1\teval-auc:0.862229\n",
      "[162]\ttrain-auc:1\teval-auc:0.86232\n",
      "[163]\ttrain-auc:1\teval-auc:0.86253\n",
      "[164]\ttrain-auc:1\teval-auc:0.862686\n",
      "[165]\ttrain-auc:1\teval-auc:0.862787\n",
      "[166]\ttrain-auc:1\teval-auc:0.862899\n",
      "[167]\ttrain-auc:1\teval-auc:0.862966\n",
      "[168]\ttrain-auc:1\teval-auc:0.863051\n",
      "[169]\ttrain-auc:1\teval-auc:0.863171\n",
      "[170]\ttrain-auc:1\teval-auc:0.86331\n",
      "[171]\ttrain-auc:1\teval-auc:0.863465\n",
      "[172]\ttrain-auc:1\teval-auc:0.863599\n",
      "[173]\ttrain-auc:1\teval-auc:0.863748\n",
      "[174]\ttrain-auc:1\teval-auc:0.863891\n",
      "[175]\ttrain-auc:1\teval-auc:0.863971\n",
      "[176]\ttrain-auc:1\teval-auc:0.864041\n",
      "[177]\ttrain-auc:1\teval-auc:0.864194\n",
      "[178]\ttrain-auc:1\teval-auc:0.864329\n",
      "[179]\ttrain-auc:1\teval-auc:0.864398\n",
      "[180]\ttrain-auc:1\teval-auc:0.864492\n",
      "[181]\ttrain-auc:1\teval-auc:0.864485\n",
      "[182]\ttrain-auc:1\teval-auc:0.864629\n",
      "[183]\ttrain-auc:1\teval-auc:0.864701\n",
      "[184]\ttrain-auc:1\teval-auc:0.864778\n",
      "[185]\ttrain-auc:1\teval-auc:0.864899\n",
      "[186]\ttrain-auc:1\teval-auc:0.864971\n",
      "[187]\ttrain-auc:1\teval-auc:0.864957\n",
      "[188]\ttrain-auc:1\teval-auc:0.864982\n",
      "[189]\ttrain-auc:1\teval-auc:0.865025\n",
      "[190]\ttrain-auc:1\teval-auc:0.865121\n",
      "[191]\ttrain-auc:1\teval-auc:0.865279\n",
      "[192]\ttrain-auc:1\teval-auc:0.865355\n",
      "[193]\ttrain-auc:1\teval-auc:0.865485\n",
      "[194]\ttrain-auc:1\teval-auc:0.86555\n",
      "[195]\ttrain-auc:1\teval-auc:0.865579\n",
      "[196]\ttrain-auc:1\teval-auc:0.865637\n",
      "[197]\ttrain-auc:1\teval-auc:0.865728\n",
      "[198]\ttrain-auc:1\teval-auc:0.865767\n",
      "[199]\ttrain-auc:1\teval-auc:0.86577\n",
      "[200]\ttrain-auc:1\teval-auc:0.865834\n",
      "[201]\ttrain-auc:1\teval-auc:0.865897\n",
      "[202]\ttrain-auc:1\teval-auc:0.865955\n",
      "[203]\ttrain-auc:1\teval-auc:0.866103\n",
      "[204]\ttrain-auc:1\teval-auc:0.866107\n",
      "[205]\ttrain-auc:1\teval-auc:0.866162\n",
      "[206]\ttrain-auc:1\teval-auc:0.866223\n",
      "[207]\ttrain-auc:1\teval-auc:0.866346\n",
      "[208]\ttrain-auc:1\teval-auc:0.866437\n",
      "[209]\ttrain-auc:1\teval-auc:0.866442\n",
      "[210]\ttrain-auc:1\teval-auc:0.866569\n",
      "[211]\ttrain-auc:1\teval-auc:0.866661\n",
      "[212]\ttrain-auc:1\teval-auc:0.866719\n",
      "[213]\ttrain-auc:1\teval-auc:0.866851\n",
      "[214]\ttrain-auc:1\teval-auc:0.86693\n",
      "[215]\ttrain-auc:1\teval-auc:0.866982\n",
      "[216]\ttrain-auc:1\teval-auc:0.867046\n",
      "[217]\ttrain-auc:1\teval-auc:0.867091\n",
      "[218]\ttrain-auc:1\teval-auc:0.867181\n",
      "[219]\ttrain-auc:1\teval-auc:0.867299\n",
      "[220]\ttrain-auc:1\teval-auc:0.867315\n",
      "[221]\ttrain-auc:1\teval-auc:0.86739\n",
      "[222]\ttrain-auc:1\teval-auc:0.86749\n",
      "[223]\ttrain-auc:1\teval-auc:0.867559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224]\ttrain-auc:1\teval-auc:0.867679\n",
      "[225]\ttrain-auc:1\teval-auc:0.867711\n",
      "[226]\ttrain-auc:1\teval-auc:0.867777\n",
      "[227]\ttrain-auc:1\teval-auc:0.867882\n",
      "[228]\ttrain-auc:1\teval-auc:0.86792\n",
      "[229]\ttrain-auc:1\teval-auc:0.86793\n",
      "[230]\ttrain-auc:1\teval-auc:0.867953\n",
      "[231]\ttrain-auc:1\teval-auc:0.868041\n",
      "[232]\ttrain-auc:1\teval-auc:0.868075\n",
      "[233]\ttrain-auc:1\teval-auc:0.868114\n",
      "[234]\ttrain-auc:1\teval-auc:0.868126\n",
      "[235]\ttrain-auc:1\teval-auc:0.868126\n",
      "[236]\ttrain-auc:1\teval-auc:0.868131\n",
      "[237]\ttrain-auc:1\teval-auc:0.868186\n",
      "[238]\ttrain-auc:1\teval-auc:0.868207\n",
      "[239]\ttrain-auc:1\teval-auc:0.868296\n",
      "[240]\ttrain-auc:1\teval-auc:0.868297\n",
      "[241]\ttrain-auc:1\teval-auc:0.868429\n",
      "[242]\ttrain-auc:1\teval-auc:0.868512\n",
      "[243]\ttrain-auc:1\teval-auc:0.868518\n",
      "[244]\ttrain-auc:1\teval-auc:0.8686\n",
      "[245]\ttrain-auc:1\teval-auc:0.868654\n",
      "[246]\ttrain-auc:1\teval-auc:0.868704\n",
      "[247]\ttrain-auc:1\teval-auc:0.868764\n",
      "[248]\ttrain-auc:1\teval-auc:0.868878\n",
      "[249]\ttrain-auc:1\teval-auc:0.868881\n",
      "[250]\ttrain-auc:1\teval-auc:0.868978\n",
      "[251]\ttrain-auc:1\teval-auc:0.869124\n",
      "[252]\ttrain-auc:1\teval-auc:0.869214\n",
      "[253]\ttrain-auc:1\teval-auc:0.869293\n",
      "[254]\ttrain-auc:1\teval-auc:0.869309\n",
      "[255]\ttrain-auc:1\teval-auc:0.869412\n",
      "[256]\ttrain-auc:1\teval-auc:0.869424\n",
      "[257]\ttrain-auc:1\teval-auc:0.869478\n",
      "[258]\ttrain-auc:1\teval-auc:0.869499\n",
      "[259]\ttrain-auc:1\teval-auc:0.869552\n",
      "[260]\ttrain-auc:1\teval-auc:0.86961\n",
      "[261]\ttrain-auc:1\teval-auc:0.869649\n",
      "[262]\ttrain-auc:1\teval-auc:0.869721\n",
      "[263]\ttrain-auc:1\teval-auc:0.86974\n",
      "[264]\ttrain-auc:1\teval-auc:0.869828\n",
      "[265]\ttrain-auc:1\teval-auc:0.869842\n",
      "[266]\ttrain-auc:1\teval-auc:0.869824\n",
      "[267]\ttrain-auc:1\teval-auc:0.86986\n",
      "[268]\ttrain-auc:1\teval-auc:0.869809\n",
      "[269]\ttrain-auc:1\teval-auc:0.869843\n",
      "[270]\ttrain-auc:1\teval-auc:0.869891\n",
      "[271]\ttrain-auc:1\teval-auc:0.869931\n",
      "[272]\ttrain-auc:1\teval-auc:0.869961\n",
      "[273]\ttrain-auc:1\teval-auc:0.869973\n",
      "[274]\ttrain-auc:1\teval-auc:0.870061\n",
      "[275]\ttrain-auc:1\teval-auc:0.870091\n",
      "[276]\ttrain-auc:1\teval-auc:0.870142\n",
      "[277]\ttrain-auc:1\teval-auc:0.870162\n",
      "[278]\ttrain-auc:1\teval-auc:0.870143\n",
      "[279]\ttrain-auc:1\teval-auc:0.87017\n",
      "[280]\ttrain-auc:1\teval-auc:0.870205\n",
      "[281]\ttrain-auc:1\teval-auc:0.870281\n",
      "[282]\ttrain-auc:1\teval-auc:0.870271\n",
      "[283]\ttrain-auc:1\teval-auc:0.870398\n",
      "[284]\ttrain-auc:1\teval-auc:0.870455\n",
      "[285]\ttrain-auc:1\teval-auc:0.87053\n",
      "[286]\ttrain-auc:1\teval-auc:0.870545\n",
      "[287]\ttrain-auc:1\teval-auc:0.870539\n",
      "[288]\ttrain-auc:1\teval-auc:0.870586\n",
      "[289]\ttrain-auc:1\teval-auc:0.870686\n",
      "[290]\ttrain-auc:1\teval-auc:0.870744\n",
      "[291]\ttrain-auc:1\teval-auc:0.870817\n",
      "[292]\ttrain-auc:1\teval-auc:0.870899\n",
      "[293]\ttrain-auc:1\teval-auc:0.870917\n",
      "[294]\ttrain-auc:1\teval-auc:0.870933\n",
      "[295]\ttrain-auc:1\teval-auc:0.870959\n",
      "[296]\ttrain-auc:1\teval-auc:0.871038\n",
      "[297]\ttrain-auc:1\teval-auc:0.87111\n",
      "[298]\ttrain-auc:1\teval-auc:0.871147\n",
      "[299]\ttrain-auc:1\teval-auc:0.87117\n",
      "[300]\ttrain-auc:1\teval-auc:0.871178\n",
      "[301]\ttrain-auc:1\teval-auc:0.871282\n",
      "[302]\ttrain-auc:1\teval-auc:0.871395\n",
      "[303]\ttrain-auc:1\teval-auc:0.871408\n",
      "[304]\ttrain-auc:1\teval-auc:0.871484\n",
      "[305]\ttrain-auc:1\teval-auc:0.8715\n",
      "[306]\ttrain-auc:1\teval-auc:0.871525\n",
      "[307]\ttrain-auc:1\teval-auc:0.871565\n",
      "[308]\ttrain-auc:1\teval-auc:0.871594\n",
      "[309]\ttrain-auc:1\teval-auc:0.871619\n",
      "[310]\ttrain-auc:1\teval-auc:0.871615\n",
      "[311]\ttrain-auc:1\teval-auc:0.871618\n",
      "[312]\ttrain-auc:1\teval-auc:0.871628\n",
      "[313]\ttrain-auc:1\teval-auc:0.871666\n",
      "[314]\ttrain-auc:1\teval-auc:0.871671\n",
      "[315]\ttrain-auc:1\teval-auc:0.871694\n",
      "[316]\ttrain-auc:1\teval-auc:0.871687\n",
      "[317]\ttrain-auc:1\teval-auc:0.87173\n",
      "[318]\ttrain-auc:1\teval-auc:0.871825\n",
      "[319]\ttrain-auc:1\teval-auc:0.871861\n",
      "[320]\ttrain-auc:1\teval-auc:0.87186\n",
      "[321]\ttrain-auc:1\teval-auc:0.871909\n",
      "[322]\ttrain-auc:1\teval-auc:0.871944\n",
      "[323]\ttrain-auc:1\teval-auc:0.871947\n",
      "[324]\ttrain-auc:1\teval-auc:0.872002\n",
      "[325]\ttrain-auc:1\teval-auc:0.871997\n",
      "[326]\ttrain-auc:1\teval-auc:0.872051\n",
      "[327]\ttrain-auc:1\teval-auc:0.872126\n",
      "[328]\ttrain-auc:1\teval-auc:0.87218\n",
      "[329]\ttrain-auc:1\teval-auc:0.872203\n",
      "[330]\ttrain-auc:1\teval-auc:0.872203\n",
      "[331]\ttrain-auc:1\teval-auc:0.872257\n",
      "[332]\ttrain-auc:1\teval-auc:0.872241\n",
      "[333]\ttrain-auc:1\teval-auc:0.872249\n",
      "[334]\ttrain-auc:1\teval-auc:0.872257\n",
      "[335]\ttrain-auc:1\teval-auc:0.872294\n",
      "[336]\ttrain-auc:1\teval-auc:0.872329\n",
      "[337]\ttrain-auc:1\teval-auc:0.872346\n",
      "[338]\ttrain-auc:1\teval-auc:0.87239\n",
      "[339]\ttrain-auc:1\teval-auc:0.872394\n",
      "[340]\ttrain-auc:1\teval-auc:0.872449\n",
      "[341]\ttrain-auc:1\teval-auc:0.87248\n",
      "[342]\ttrain-auc:1\teval-auc:0.872484\n",
      "[343]\ttrain-auc:1\teval-auc:0.872496\n",
      "[344]\ttrain-auc:1\teval-auc:0.872505\n",
      "[345]\ttrain-auc:1\teval-auc:0.872532\n",
      "[346]\ttrain-auc:1\teval-auc:0.872533\n",
      "[347]\ttrain-auc:1\teval-auc:0.872564\n",
      "[348]\ttrain-auc:1\teval-auc:0.872594\n",
      "[349]\ttrain-auc:1\teval-auc:0.872663\n",
      "[350]\ttrain-auc:1\teval-auc:0.872708\n",
      "[351]\ttrain-auc:1\teval-auc:0.872749\n",
      "[352]\ttrain-auc:1\teval-auc:0.87275\n",
      "[353]\ttrain-auc:1\teval-auc:0.872777\n",
      "[354]\ttrain-auc:1\teval-auc:0.872779\n",
      "[355]\ttrain-auc:1\teval-auc:0.8728\n",
      "[356]\ttrain-auc:1\teval-auc:0.872865\n",
      "[357]\ttrain-auc:1\teval-auc:0.872894\n",
      "[358]\ttrain-auc:1\teval-auc:0.872893\n",
      "[359]\ttrain-auc:1\teval-auc:0.872923\n",
      "[360]\ttrain-auc:1\teval-auc:0.872917\n",
      "[361]\ttrain-auc:1\teval-auc:0.872952\n",
      "[362]\ttrain-auc:1\teval-auc:0.872973\n",
      "[363]\ttrain-auc:1\teval-auc:0.872982\n",
      "[364]\ttrain-auc:1\teval-auc:0.873008\n",
      "[365]\ttrain-auc:1\teval-auc:0.873062\n",
      "[366]\ttrain-auc:1\teval-auc:0.873092\n",
      "[367]\ttrain-auc:1\teval-auc:0.873101\n",
      "[368]\ttrain-auc:1\teval-auc:0.873115\n",
      "[369]\ttrain-auc:1\teval-auc:0.873126\n",
      "[370]\ttrain-auc:1\teval-auc:0.87313\n",
      "[371]\ttrain-auc:1\teval-auc:0.873139\n",
      "[372]\ttrain-auc:1\teval-auc:0.873126\n",
      "[373]\ttrain-auc:1\teval-auc:0.873122\n",
      "[374]\ttrain-auc:1\teval-auc:0.873153\n",
      "[375]\ttrain-auc:1\teval-auc:0.873167\n",
      "[376]\ttrain-auc:1\teval-auc:0.873256\n",
      "[377]\ttrain-auc:1\teval-auc:0.873273\n",
      "[378]\ttrain-auc:1\teval-auc:0.873294\n",
      "[379]\ttrain-auc:1\teval-auc:0.873275\n",
      "[380]\ttrain-auc:1\teval-auc:0.873352\n",
      "[381]\ttrain-auc:1\teval-auc:0.873424\n",
      "[382]\ttrain-auc:1\teval-auc:0.873438\n",
      "[383]\ttrain-auc:1\teval-auc:0.873444\n",
      "[384]\ttrain-auc:1\teval-auc:0.873436\n",
      "[385]\ttrain-auc:1\teval-auc:0.87347\n",
      "[386]\ttrain-auc:1\teval-auc:0.873495\n",
      "[387]\ttrain-auc:1\teval-auc:0.8735\n",
      "[388]\ttrain-auc:1\teval-auc:0.873533\n",
      "[389]\ttrain-auc:1\teval-auc:0.87352\n",
      "[390]\ttrain-auc:1\teval-auc:0.873522\n",
      "[391]\ttrain-auc:1\teval-auc:0.873533\n",
      "[392]\ttrain-auc:1\teval-auc:0.873561\n",
      "[393]\ttrain-auc:1\teval-auc:0.873571\n",
      "[394]\ttrain-auc:1\teval-auc:0.873566\n",
      "[395]\ttrain-auc:1\teval-auc:0.873581\n",
      "[396]\ttrain-auc:1\teval-auc:0.873535\n",
      "[397]\ttrain-auc:1\teval-auc:0.87359\n",
      "[398]\ttrain-auc:1\teval-auc:0.873629\n",
      "[399]\ttrain-auc:1\teval-auc:0.873672\n",
      "[400]\ttrain-auc:1\teval-auc:0.873719\n",
      "[401]\ttrain-auc:1\teval-auc:0.873725\n",
      "[402]\ttrain-auc:1\teval-auc:0.873731\n",
      "[403]\ttrain-auc:1\teval-auc:0.873745\n",
      "[404]\ttrain-auc:1\teval-auc:0.87374\n",
      "[405]\ttrain-auc:1\teval-auc:0.873781\n",
      "[406]\ttrain-auc:1\teval-auc:0.873795\n",
      "[407]\ttrain-auc:1\teval-auc:0.873878\n",
      "[408]\ttrain-auc:1\teval-auc:0.873892\n",
      "[409]\ttrain-auc:1\teval-auc:0.873906\n",
      "[410]\ttrain-auc:1\teval-auc:0.873936\n",
      "[411]\ttrain-auc:1\teval-auc:0.873916\n",
      "[412]\ttrain-auc:1\teval-auc:0.873897\n",
      "[413]\ttrain-auc:1\teval-auc:0.873922\n",
      "[414]\ttrain-auc:1\teval-auc:0.873942\n",
      "[415]\ttrain-auc:1\teval-auc:0.8739\n",
      "[416]\ttrain-auc:1\teval-auc:0.873927\n",
      "[417]\ttrain-auc:1\teval-auc:0.873939\n",
      "[418]\ttrain-auc:1\teval-auc:0.873945\n",
      "[419]\ttrain-auc:1\teval-auc:0.873948\n",
      "[420]\ttrain-auc:1\teval-auc:0.874004\n",
      "[421]\ttrain-auc:1\teval-auc:0.874037\n",
      "[422]\ttrain-auc:1\teval-auc:0.874101\n",
      "[423]\ttrain-auc:1\teval-auc:0.874113\n",
      "[424]\ttrain-auc:1\teval-auc:0.874148\n",
      "[425]\ttrain-auc:1\teval-auc:0.874158\n",
      "[426]\ttrain-auc:1\teval-auc:0.874193\n",
      "[427]\ttrain-auc:1\teval-auc:0.874203\n",
      "[428]\ttrain-auc:1\teval-auc:0.874198\n",
      "[429]\ttrain-auc:1\teval-auc:0.874217\n",
      "[430]\ttrain-auc:1\teval-auc:0.874214\n",
      "[431]\ttrain-auc:1\teval-auc:0.874215\n",
      "[432]\ttrain-auc:1\teval-auc:0.874216\n",
      "[433]\ttrain-auc:1\teval-auc:0.87422\n",
      "[434]\ttrain-auc:1\teval-auc:0.87426\n",
      "[435]\ttrain-auc:1\teval-auc:0.874233\n",
      "[436]\ttrain-auc:1\teval-auc:0.874254\n",
      "[437]\ttrain-auc:1\teval-auc:0.87427\n",
      "[438]\ttrain-auc:1\teval-auc:0.874312\n",
      "[439]\ttrain-auc:1\teval-auc:0.874359\n",
      "[440]\ttrain-auc:1\teval-auc:0.874355\n",
      "[441]\ttrain-auc:1\teval-auc:0.874382\n",
      "[442]\ttrain-auc:1\teval-auc:0.874416\n",
      "[443]\ttrain-auc:1\teval-auc:0.874455\n",
      "[444]\ttrain-auc:1\teval-auc:0.874462\n",
      "[445]\ttrain-auc:1\teval-auc:0.874465\n",
      "[446]\ttrain-auc:1\teval-auc:0.874472\n",
      "[447]\ttrain-auc:1\teval-auc:0.874437\n",
      "[448]\ttrain-auc:1\teval-auc:0.874436\n",
      "[449]\ttrain-auc:1\teval-auc:0.874433\n",
      "[450]\ttrain-auc:1\teval-auc:0.874454\n",
      "[451]\ttrain-auc:1\teval-auc:0.874479\n",
      "[452]\ttrain-auc:1\teval-auc:0.874463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[453]\ttrain-auc:1\teval-auc:0.874524\n",
      "[454]\ttrain-auc:1\teval-auc:0.874564\n",
      "[455]\ttrain-auc:1\teval-auc:0.874576\n",
      "[456]\ttrain-auc:1\teval-auc:0.874604\n",
      "[457]\ttrain-auc:1\teval-auc:0.874628\n",
      "[458]\ttrain-auc:1\teval-auc:0.874669\n",
      "[459]\ttrain-auc:1\teval-auc:0.874661\n",
      "[460]\ttrain-auc:1\teval-auc:0.874676\n",
      "[461]\ttrain-auc:1\teval-auc:0.874681\n",
      "[462]\ttrain-auc:1\teval-auc:0.874695\n",
      "[463]\ttrain-auc:1\teval-auc:0.874686\n",
      "[464]\ttrain-auc:1\teval-auc:0.874697\n",
      "[465]\ttrain-auc:1\teval-auc:0.874709\n",
      "[466]\ttrain-auc:1\teval-auc:0.874717\n",
      "[467]\ttrain-auc:1\teval-auc:0.874741\n",
      "[468]\ttrain-auc:1\teval-auc:0.874765\n",
      "[469]\ttrain-auc:1\teval-auc:0.87478\n",
      "[470]\ttrain-auc:1\teval-auc:0.874818\n",
      "[471]\ttrain-auc:1\teval-auc:0.874822\n",
      "[472]\ttrain-auc:1\teval-auc:0.87482\n",
      "[473]\ttrain-auc:1\teval-auc:0.874823\n",
      "[474]\ttrain-auc:1\teval-auc:0.874854\n",
      "[475]\ttrain-auc:1\teval-auc:0.874848\n",
      "[476]\ttrain-auc:1\teval-auc:0.874852\n",
      "[477]\ttrain-auc:1\teval-auc:0.874882\n",
      "[478]\ttrain-auc:1\teval-auc:0.87491\n",
      "[479]\ttrain-auc:1\teval-auc:0.874945\n",
      "[480]\ttrain-auc:1\teval-auc:0.874953\n",
      "[481]\ttrain-auc:1\teval-auc:0.874961\n",
      "[482]\ttrain-auc:1\teval-auc:0.87496\n",
      "[483]\ttrain-auc:1\teval-auc:0.874977\n",
      "[484]\ttrain-auc:1\teval-auc:0.874983\n",
      "[485]\ttrain-auc:1\teval-auc:0.874989\n",
      "[486]\ttrain-auc:1\teval-auc:0.875012\n",
      "[487]\ttrain-auc:1\teval-auc:0.875025\n",
      "[488]\ttrain-auc:1\teval-auc:0.875025\n",
      "[489]\ttrain-auc:1\teval-auc:0.875017\n",
      "[490]\ttrain-auc:1\teval-auc:0.875018\n",
      "[491]\ttrain-auc:1\teval-auc:0.875033\n",
      "[492]\ttrain-auc:1\teval-auc:0.875061\n",
      "[493]\ttrain-auc:1\teval-auc:0.87507\n",
      "[494]\ttrain-auc:1\teval-auc:0.875061\n",
      "[495]\ttrain-auc:1\teval-auc:0.875071\n",
      "[496]\ttrain-auc:1\teval-auc:0.87508\n",
      "[497]\ttrain-auc:1\teval-auc:0.875102\n",
      "[498]\ttrain-auc:1\teval-auc:0.875105\n",
      "[499]\ttrain-auc:1\teval-auc:0.875127\n",
      "[500]\ttrain-auc:1\teval-auc:0.875105\n",
      "[501]\ttrain-auc:1\teval-auc:0.875116\n",
      "[502]\ttrain-auc:1\teval-auc:0.875156\n",
      "[503]\ttrain-auc:1\teval-auc:0.875164\n",
      "[504]\ttrain-auc:1\teval-auc:0.875173\n",
      "[505]\ttrain-auc:1\teval-auc:0.875157\n",
      "[506]\ttrain-auc:1\teval-auc:0.875173\n",
      "[507]\ttrain-auc:1\teval-auc:0.87517\n",
      "[508]\ttrain-auc:1\teval-auc:0.875193\n",
      "[509]\ttrain-auc:1\teval-auc:0.875225\n",
      "[510]\ttrain-auc:1\teval-auc:0.875242\n",
      "[511]\ttrain-auc:1\teval-auc:0.875242\n",
      "[512]\ttrain-auc:1\teval-auc:0.875241\n",
      "[513]\ttrain-auc:1\teval-auc:0.875257\n",
      "[514]\ttrain-auc:1\teval-auc:0.87526\n",
      "[515]\ttrain-auc:1\teval-auc:0.875271\n",
      "[516]\ttrain-auc:1\teval-auc:0.875318\n",
      "[517]\ttrain-auc:1\teval-auc:0.875337\n",
      "[518]\ttrain-auc:1\teval-auc:0.875372\n",
      "[519]\ttrain-auc:1\teval-auc:0.875373\n",
      "[520]\ttrain-auc:1\teval-auc:0.875385\n",
      "[521]\ttrain-auc:1\teval-auc:0.8754\n",
      "[522]\ttrain-auc:1\teval-auc:0.875373\n",
      "[523]\ttrain-auc:1\teval-auc:0.875396\n",
      "[524]\ttrain-auc:1\teval-auc:0.875412\n",
      "[525]\ttrain-auc:1\teval-auc:0.875419\n",
      "[526]\ttrain-auc:1\teval-auc:0.875425\n",
      "[527]\ttrain-auc:1\teval-auc:0.875437\n",
      "[528]\ttrain-auc:1\teval-auc:0.87545\n",
      "[529]\ttrain-auc:1\teval-auc:0.875469\n",
      "[530]\ttrain-auc:1\teval-auc:0.875467\n",
      "[531]\ttrain-auc:1\teval-auc:0.87548\n",
      "[532]\ttrain-auc:1\teval-auc:0.875449\n",
      "[533]\ttrain-auc:1\teval-auc:0.875474\n",
      "[534]\ttrain-auc:1\teval-auc:0.875458\n",
      "[535]\ttrain-auc:1\teval-auc:0.875456\n",
      "[536]\ttrain-auc:1\teval-auc:0.87545\n",
      "[537]\ttrain-auc:1\teval-auc:0.875461\n",
      "[538]\ttrain-auc:1\teval-auc:0.875486\n",
      "[539]\ttrain-auc:1\teval-auc:0.875539\n",
      "[540]\ttrain-auc:1\teval-auc:0.875553\n",
      "[541]\ttrain-auc:1\teval-auc:0.875559\n",
      "[542]\ttrain-auc:1\teval-auc:0.875546\n",
      "[543]\ttrain-auc:1\teval-auc:0.875559\n",
      "[544]\ttrain-auc:1\teval-auc:0.87558\n",
      "[545]\ttrain-auc:1\teval-auc:0.875591\n",
      "[546]\ttrain-auc:1\teval-auc:0.875604\n",
      "[547]\ttrain-auc:1\teval-auc:0.875603\n",
      "[548]\ttrain-auc:1\teval-auc:0.875604\n",
      "[549]\ttrain-auc:1\teval-auc:0.875628\n",
      "[550]\ttrain-auc:1\teval-auc:0.875637\n",
      "[551]\ttrain-auc:1\teval-auc:0.875638\n",
      "[552]\ttrain-auc:1\teval-auc:0.875626\n",
      "[553]\ttrain-auc:1\teval-auc:0.875654\n",
      "[554]\ttrain-auc:1\teval-auc:0.875701\n",
      "[555]\ttrain-auc:1\teval-auc:0.875722\n",
      "[556]\ttrain-auc:1\teval-auc:0.875726\n",
      "[557]\ttrain-auc:1\teval-auc:0.875725\n",
      "[558]\ttrain-auc:1\teval-auc:0.875772\n",
      "[559]\ttrain-auc:1\teval-auc:0.875765\n",
      "[560]\ttrain-auc:1\teval-auc:0.875747\n",
      "[561]\ttrain-auc:1\teval-auc:0.875749\n",
      "[562]\ttrain-auc:1\teval-auc:0.875748\n",
      "[563]\ttrain-auc:1\teval-auc:0.875757\n",
      "[564]\ttrain-auc:1\teval-auc:0.875767\n",
      "[565]\ttrain-auc:1\teval-auc:0.875784\n",
      "[566]\ttrain-auc:1\teval-auc:0.875803\n",
      "[567]\ttrain-auc:1\teval-auc:0.875836\n",
      "[568]\ttrain-auc:1\teval-auc:0.875852\n",
      "[569]\ttrain-auc:1\teval-auc:0.875861\n",
      "[570]\ttrain-auc:1\teval-auc:0.875849\n",
      "[571]\ttrain-auc:1\teval-auc:0.875839\n",
      "[572]\ttrain-auc:1\teval-auc:0.875835\n",
      "[573]\ttrain-auc:1\teval-auc:0.875855\n",
      "[574]\ttrain-auc:1\teval-auc:0.875848\n",
      "[575]\ttrain-auc:1\teval-auc:0.875867\n",
      "[576]\ttrain-auc:1\teval-auc:0.875837\n",
      "[577]\ttrain-auc:1\teval-auc:0.87581\n",
      "[578]\ttrain-auc:1\teval-auc:0.875792\n",
      "[579]\ttrain-auc:1\teval-auc:0.875797\n",
      "[580]\ttrain-auc:1\teval-auc:0.875786\n",
      "[581]\ttrain-auc:1\teval-auc:0.875769\n",
      "[582]\ttrain-auc:1\teval-auc:0.875775\n",
      "[583]\ttrain-auc:1\teval-auc:0.875772\n",
      "[584]\ttrain-auc:1\teval-auc:0.875779\n",
      "[585]\ttrain-auc:1\teval-auc:0.875793\n",
      "[586]\ttrain-auc:1\teval-auc:0.875833\n",
      "[587]\ttrain-auc:1\teval-auc:0.875831\n",
      "[588]\ttrain-auc:1\teval-auc:0.875835\n",
      "[589]\ttrain-auc:1\teval-auc:0.875849\n",
      "[590]\ttrain-auc:1\teval-auc:0.875852\n",
      "[591]\ttrain-auc:1\teval-auc:0.875872\n",
      "[592]\ttrain-auc:1\teval-auc:0.875903\n",
      "[593]\ttrain-auc:1\teval-auc:0.875897\n",
      "[594]\ttrain-auc:1\teval-auc:0.875902\n",
      "[595]\ttrain-auc:1\teval-auc:0.875906\n",
      "[596]\ttrain-auc:1\teval-auc:0.875921\n",
      "[597]\ttrain-auc:1\teval-auc:0.87594\n",
      "[598]\ttrain-auc:1\teval-auc:0.875948\n",
      "[599]\ttrain-auc:1\teval-auc:0.875956\n",
      "[600]\ttrain-auc:1\teval-auc:0.875977\n",
      "[601]\ttrain-auc:1\teval-auc:0.876007\n",
      "[602]\ttrain-auc:1\teval-auc:0.876039\n",
      "[603]\ttrain-auc:1\teval-auc:0.876033\n",
      "[604]\ttrain-auc:1\teval-auc:0.876053\n",
      "[605]\ttrain-auc:1\teval-auc:0.876076\n",
      "[606]\ttrain-auc:1\teval-auc:0.876084\n",
      "[607]\ttrain-auc:1\teval-auc:0.876064\n",
      "[608]\ttrain-auc:1\teval-auc:0.876057\n",
      "[609]\ttrain-auc:1\teval-auc:0.876062\n",
      "[610]\ttrain-auc:1\teval-auc:0.876069\n",
      "[611]\ttrain-auc:1\teval-auc:0.876064\n",
      "[612]\ttrain-auc:1\teval-auc:0.876063\n",
      "[613]\ttrain-auc:1\teval-auc:0.87607\n",
      "[614]\ttrain-auc:1\teval-auc:0.876049\n",
      "[615]\ttrain-auc:1\teval-auc:0.876049\n",
      "[616]\ttrain-auc:1\teval-auc:0.876056\n",
      "[617]\ttrain-auc:1\teval-auc:0.876054\n",
      "[618]\ttrain-auc:1\teval-auc:0.87607\n",
      "[619]\ttrain-auc:1\teval-auc:0.876065\n",
      "[620]\ttrain-auc:1\teval-auc:0.876078\n",
      "[621]\ttrain-auc:1\teval-auc:0.876063\n",
      "[622]\ttrain-auc:1\teval-auc:0.876049\n",
      "[623]\ttrain-auc:1\teval-auc:0.876064\n",
      "[624]\ttrain-auc:1\teval-auc:0.876074\n",
      "[625]\ttrain-auc:1\teval-auc:0.876066\n",
      "[626]\ttrain-auc:1\teval-auc:0.876091\n",
      "[627]\ttrain-auc:1\teval-auc:0.876089\n",
      "[628]\ttrain-auc:1\teval-auc:0.876121\n",
      "[629]\ttrain-auc:1\teval-auc:0.876124\n",
      "[630]\ttrain-auc:1\teval-auc:0.876138\n",
      "[631]\ttrain-auc:1\teval-auc:0.876153\n",
      "[632]\ttrain-auc:1\teval-auc:0.876162\n",
      "[633]\ttrain-auc:1\teval-auc:0.876183\n",
      "[634]\ttrain-auc:1\teval-auc:0.87618\n",
      "[635]\ttrain-auc:1\teval-auc:0.876195\n",
      "[636]\ttrain-auc:1\teval-auc:0.876198\n",
      "[637]\ttrain-auc:1\teval-auc:0.876208\n",
      "[638]\ttrain-auc:1\teval-auc:0.876199\n",
      "[639]\ttrain-auc:1\teval-auc:0.876193\n",
      "[640]\ttrain-auc:1\teval-auc:0.876207\n",
      "[641]\ttrain-auc:1\teval-auc:0.876229\n",
      "[642]\ttrain-auc:1\teval-auc:0.876245\n",
      "[643]\ttrain-auc:1\teval-auc:0.876243\n",
      "[644]\ttrain-auc:1\teval-auc:0.87623\n",
      "[645]\ttrain-auc:1\teval-auc:0.876246\n",
      "[646]\ttrain-auc:1\teval-auc:0.87625\n",
      "[647]\ttrain-auc:1\teval-auc:0.876242\n",
      "[648]\ttrain-auc:1\teval-auc:0.876238\n",
      "[649]\ttrain-auc:1\teval-auc:0.876261\n",
      "[650]\ttrain-auc:1\teval-auc:0.876262\n",
      "[651]\ttrain-auc:1\teval-auc:0.87627\n",
      "[652]\ttrain-auc:1\teval-auc:0.876277\n",
      "[653]\ttrain-auc:1\teval-auc:0.876281\n",
      "[654]\ttrain-auc:1\teval-auc:0.87627\n",
      "[655]\ttrain-auc:1\teval-auc:0.876274\n",
      "[656]\ttrain-auc:1\teval-auc:0.876292\n",
      "[657]\ttrain-auc:1\teval-auc:0.876294\n",
      "[658]\ttrain-auc:1\teval-auc:0.876321\n",
      "[659]\ttrain-auc:1\teval-auc:0.876338\n",
      "[660]\ttrain-auc:1\teval-auc:0.87635\n",
      "[661]\ttrain-auc:1\teval-auc:0.876363\n",
      "[662]\ttrain-auc:1\teval-auc:0.876375\n",
      "[663]\ttrain-auc:1\teval-auc:0.876341\n",
      "[664]\ttrain-auc:1\teval-auc:0.876368\n",
      "[665]\ttrain-auc:1\teval-auc:0.876364\n",
      "[666]\ttrain-auc:1\teval-auc:0.876356\n",
      "[667]\ttrain-auc:1\teval-auc:0.876375\n",
      "[668]\ttrain-auc:1\teval-auc:0.876365\n",
      "[669]\ttrain-auc:1\teval-auc:0.876357\n",
      "[670]\ttrain-auc:1\teval-auc:0.876376\n",
      "[671]\ttrain-auc:1\teval-auc:0.876371\n",
      "[672]\ttrain-auc:1\teval-auc:0.876406\n",
      "[673]\ttrain-auc:1\teval-auc:0.876403\n",
      "[674]\ttrain-auc:1\teval-auc:0.876396\n",
      "[675]\ttrain-auc:1\teval-auc:0.876415\n",
      "[676]\ttrain-auc:1\teval-auc:0.876421\n",
      "[677]\ttrain-auc:1\teval-auc:0.876437\n",
      "[678]\ttrain-auc:1\teval-auc:0.876438\n",
      "[679]\ttrain-auc:1\teval-auc:0.876454\n",
      "[680]\ttrain-auc:1\teval-auc:0.876455\n",
      "[681]\ttrain-auc:1\teval-auc:0.876449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[682]\ttrain-auc:1\teval-auc:0.876444\n",
      "[683]\ttrain-auc:1\teval-auc:0.876443\n",
      "[684]\ttrain-auc:1\teval-auc:0.876474\n",
      "[685]\ttrain-auc:1\teval-auc:0.876488\n",
      "[686]\ttrain-auc:1\teval-auc:0.876468\n",
      "[687]\ttrain-auc:1\teval-auc:0.876463\n",
      "[688]\ttrain-auc:1\teval-auc:0.876469\n",
      "[689]\ttrain-auc:1\teval-auc:0.876475\n",
      "[690]\ttrain-auc:1\teval-auc:0.876484\n",
      "[691]\ttrain-auc:1\teval-auc:0.876496\n",
      "[692]\ttrain-auc:1\teval-auc:0.876511\n",
      "[693]\ttrain-auc:1\teval-auc:0.876524\n",
      "[694]\ttrain-auc:1\teval-auc:0.87652\n",
      "[695]\ttrain-auc:1\teval-auc:0.876552\n",
      "[696]\ttrain-auc:1\teval-auc:0.876569\n",
      "[697]\ttrain-auc:1\teval-auc:0.876561\n",
      "[698]\ttrain-auc:1\teval-auc:0.876591\n",
      "[699]\ttrain-auc:1\teval-auc:0.8766\n",
      "[700]\ttrain-auc:1\teval-auc:0.876612\n",
      "[701]\ttrain-auc:1\teval-auc:0.876608\n",
      "[702]\ttrain-auc:1\teval-auc:0.876633\n",
      "[703]\ttrain-auc:1\teval-auc:0.876618\n",
      "[704]\ttrain-auc:1\teval-auc:0.876616\n",
      "[705]\ttrain-auc:1\teval-auc:0.876607\n",
      "[706]\ttrain-auc:1\teval-auc:0.876622\n",
      "[707]\ttrain-auc:1\teval-auc:0.876628\n",
      "[708]\ttrain-auc:1\teval-auc:0.876638\n",
      "[709]\ttrain-auc:1\teval-auc:0.876636\n",
      "[710]\ttrain-auc:1\teval-auc:0.876624\n",
      "[711]\ttrain-auc:1\teval-auc:0.876646\n",
      "[712]\ttrain-auc:1\teval-auc:0.876651\n",
      "[713]\ttrain-auc:1\teval-auc:0.876639\n",
      "[714]\ttrain-auc:1\teval-auc:0.876625\n",
      "[715]\ttrain-auc:1\teval-auc:0.876623\n",
      "[716]\ttrain-auc:1\teval-auc:0.876661\n",
      "[717]\ttrain-auc:1\teval-auc:0.87665\n",
      "[718]\ttrain-auc:1\teval-auc:0.876661\n",
      "[719]\ttrain-auc:1\teval-auc:0.876665\n",
      "[720]\ttrain-auc:1\teval-auc:0.876669\n",
      "[721]\ttrain-auc:1\teval-auc:0.876677\n",
      "[722]\ttrain-auc:1\teval-auc:0.876706\n",
      "[723]\ttrain-auc:1\teval-auc:0.876698\n",
      "[724]\ttrain-auc:1\teval-auc:0.876708\n",
      "[725]\ttrain-auc:1\teval-auc:0.876711\n",
      "[726]\ttrain-auc:1\teval-auc:0.876699\n",
      "[727]\ttrain-auc:1\teval-auc:0.876678\n",
      "[728]\ttrain-auc:1\teval-auc:0.876691\n",
      "[729]\ttrain-auc:1\teval-auc:0.876698\n",
      "[730]\ttrain-auc:1\teval-auc:0.876728\n",
      "[731]\ttrain-auc:1\teval-auc:0.876733\n",
      "[732]\ttrain-auc:1\teval-auc:0.876744\n",
      "[733]\ttrain-auc:1\teval-auc:0.876751\n",
      "[734]\ttrain-auc:1\teval-auc:0.876746\n",
      "[735]\ttrain-auc:1\teval-auc:0.876747\n",
      "[736]\ttrain-auc:1\teval-auc:0.876744\n",
      "[737]\ttrain-auc:1\teval-auc:0.876756\n",
      "[738]\ttrain-auc:1\teval-auc:0.876762\n",
      "[739]\ttrain-auc:1\teval-auc:0.876767\n",
      "[740]\ttrain-auc:1\teval-auc:0.876778\n",
      "[741]\ttrain-auc:1\teval-auc:0.87678\n",
      "[742]\ttrain-auc:1\teval-auc:0.876792\n",
      "[743]\ttrain-auc:1\teval-auc:0.8768\n",
      "[744]\ttrain-auc:1\teval-auc:0.876786\n",
      "[745]\ttrain-auc:1\teval-auc:0.876788\n",
      "[746]\ttrain-auc:1\teval-auc:0.876801\n",
      "[747]\ttrain-auc:1\teval-auc:0.876819\n",
      "[748]\ttrain-auc:1\teval-auc:0.876822\n",
      "[749]\ttrain-auc:1\teval-auc:0.876842\n",
      "[750]\ttrain-auc:1\teval-auc:0.876874\n",
      "[751]\ttrain-auc:1\teval-auc:0.876871\n",
      "[752]\ttrain-auc:1\teval-auc:0.876892\n",
      "[753]\ttrain-auc:1\teval-auc:0.876895\n",
      "[754]\ttrain-auc:1\teval-auc:0.87688\n",
      "[755]\ttrain-auc:1\teval-auc:0.876877\n",
      "[756]\ttrain-auc:1\teval-auc:0.876888\n",
      "[757]\ttrain-auc:1\teval-auc:0.876911\n",
      "[758]\ttrain-auc:1\teval-auc:0.876899\n",
      "[759]\ttrain-auc:1\teval-auc:0.876902\n",
      "[760]\ttrain-auc:1\teval-auc:0.876915\n",
      "[761]\ttrain-auc:1\teval-auc:0.876924\n",
      "[762]\ttrain-auc:1\teval-auc:0.876932\n",
      "[763]\ttrain-auc:1\teval-auc:0.876954\n",
      "[764]\ttrain-auc:1\teval-auc:0.876963\n",
      "[765]\ttrain-auc:1\teval-auc:0.876954\n",
      "[766]\ttrain-auc:1\teval-auc:0.87698\n",
      "[767]\ttrain-auc:1\teval-auc:0.876998\n",
      "[768]\ttrain-auc:1\teval-auc:0.876992\n",
      "[769]\ttrain-auc:1\teval-auc:0.877009\n",
      "[770]\ttrain-auc:1\teval-auc:0.87701\n",
      "[771]\ttrain-auc:1\teval-auc:0.876998\n",
      "[772]\ttrain-auc:1\teval-auc:0.877007\n",
      "[773]\ttrain-auc:1\teval-auc:0.877012\n",
      "[774]\ttrain-auc:1\teval-auc:0.877002\n",
      "[775]\ttrain-auc:1\teval-auc:0.876998\n",
      "[776]\ttrain-auc:1\teval-auc:0.877011\n",
      "[777]\ttrain-auc:1\teval-auc:0.876998\n",
      "[778]\ttrain-auc:1\teval-auc:0.877024\n",
      "[779]\ttrain-auc:1\teval-auc:0.877016\n",
      "[780]\ttrain-auc:1\teval-auc:0.877015\n",
      "[781]\ttrain-auc:1\teval-auc:0.877001\n",
      "[782]\ttrain-auc:1\teval-auc:0.876985\n",
      "[783]\ttrain-auc:1\teval-auc:0.876975\n",
      "[784]\ttrain-auc:1\teval-auc:0.876971\n",
      "[785]\ttrain-auc:1\teval-auc:0.876976\n",
      "[786]\ttrain-auc:1\teval-auc:0.876972\n",
      "[787]\ttrain-auc:1\teval-auc:0.876973\n",
      "[788]\ttrain-auc:1\teval-auc:0.876958\n",
      "[789]\ttrain-auc:1\teval-auc:0.876974\n",
      "[790]\ttrain-auc:1\teval-auc:0.876999\n",
      "[791]\ttrain-auc:1\teval-auc:0.877\n",
      "[792]\ttrain-auc:1\teval-auc:0.877012\n",
      "[793]\ttrain-auc:1\teval-auc:0.877018\n",
      "[794]\ttrain-auc:1\teval-auc:0.877013\n",
      "[795]\ttrain-auc:1\teval-auc:0.877023\n",
      "[796]\ttrain-auc:1\teval-auc:0.877031\n",
      "[797]\ttrain-auc:1\teval-auc:0.877028\n",
      "[798]\ttrain-auc:1\teval-auc:0.877056\n",
      "[799]\ttrain-auc:1\teval-auc:0.877048\n",
      "[800]\ttrain-auc:1\teval-auc:0.877061\n",
      "[801]\ttrain-auc:1\teval-auc:0.877065\n",
      "[802]\ttrain-auc:1\teval-auc:0.877063\n",
      "[803]\ttrain-auc:1\teval-auc:0.877067\n",
      "[804]\ttrain-auc:1\teval-auc:0.87706\n",
      "[805]\ttrain-auc:1\teval-auc:0.877068\n",
      "[806]\ttrain-auc:1\teval-auc:0.877082\n",
      "[807]\ttrain-auc:1\teval-auc:0.877063\n",
      "[808]\ttrain-auc:1\teval-auc:0.877062\n",
      "[809]\ttrain-auc:1\teval-auc:0.877062\n",
      "[810]\ttrain-auc:1\teval-auc:0.877064\n",
      "[811]\ttrain-auc:1\teval-auc:0.877066\n",
      "[812]\ttrain-auc:1\teval-auc:0.87707\n",
      "[813]\ttrain-auc:1\teval-auc:0.877068\n",
      "[814]\ttrain-auc:1\teval-auc:0.877071\n",
      "[815]\ttrain-auc:1\teval-auc:0.877084\n",
      "[816]\ttrain-auc:1\teval-auc:0.877087\n",
      "[817]\ttrain-auc:1\teval-auc:0.877101\n",
      "[818]\ttrain-auc:1\teval-auc:0.877102\n",
      "[819]\ttrain-auc:1\teval-auc:0.877114\n",
      "[820]\ttrain-auc:1\teval-auc:0.877109\n",
      "[821]\ttrain-auc:1\teval-auc:0.877111\n",
      "[822]\ttrain-auc:1\teval-auc:0.877125\n",
      "[823]\ttrain-auc:1\teval-auc:0.877124\n",
      "[824]\ttrain-auc:1\teval-auc:0.877154\n",
      "[825]\ttrain-auc:1\teval-auc:0.877155\n",
      "[826]\ttrain-auc:1\teval-auc:0.877156\n",
      "[827]\ttrain-auc:1\teval-auc:0.877146\n",
      "[828]\ttrain-auc:1\teval-auc:0.877166\n",
      "[829]\ttrain-auc:1\teval-auc:0.877175\n",
      "[830]\ttrain-auc:1\teval-auc:0.877172\n",
      "[831]\ttrain-auc:1\teval-auc:0.877162\n",
      "[832]\ttrain-auc:1\teval-auc:0.877186\n",
      "[833]\ttrain-auc:1\teval-auc:0.877195\n",
      "[834]\ttrain-auc:1\teval-auc:0.877188\n",
      "[835]\ttrain-auc:1\teval-auc:0.877193\n",
      "[836]\ttrain-auc:1\teval-auc:0.877211\n",
      "[837]\ttrain-auc:1\teval-auc:0.877216\n",
      "[838]\ttrain-auc:1\teval-auc:0.877212\n",
      "[839]\ttrain-auc:1\teval-auc:0.877229\n",
      "[840]\ttrain-auc:1\teval-auc:0.877237\n",
      "[841]\ttrain-auc:1\teval-auc:0.877237\n",
      "[842]\ttrain-auc:1\teval-auc:0.877239\n",
      "[843]\ttrain-auc:1\teval-auc:0.877254\n",
      "[844]\ttrain-auc:1\teval-auc:0.877252\n",
      "[845]\ttrain-auc:1\teval-auc:0.877259\n",
      "[846]\ttrain-auc:1\teval-auc:0.877251\n",
      "[847]\ttrain-auc:1\teval-auc:0.877242\n",
      "[848]\ttrain-auc:1\teval-auc:0.877239\n",
      "[849]\ttrain-auc:1\teval-auc:0.877207\n",
      "[850]\ttrain-auc:1\teval-auc:0.877219\n",
      "[851]\ttrain-auc:1\teval-auc:0.877245\n",
      "[852]\ttrain-auc:1\teval-auc:0.877235\n",
      "[853]\ttrain-auc:1\teval-auc:0.877245\n",
      "[854]\ttrain-auc:1\teval-auc:0.877245\n",
      "[855]\ttrain-auc:1\teval-auc:0.877247\n",
      "[856]\ttrain-auc:1\teval-auc:0.877264\n",
      "[857]\ttrain-auc:1\teval-auc:0.877275\n",
      "[858]\ttrain-auc:1\teval-auc:0.877273\n",
      "[859]\ttrain-auc:1\teval-auc:0.877277\n",
      "[860]\ttrain-auc:1\teval-auc:0.87728\n",
      "[861]\ttrain-auc:1\teval-auc:0.877287\n",
      "[862]\ttrain-auc:1\teval-auc:0.877315\n",
      "[863]\ttrain-auc:1\teval-auc:0.877312\n",
      "[864]\ttrain-auc:1\teval-auc:0.877315\n",
      "[865]\ttrain-auc:1\teval-auc:0.877311\n",
      "[866]\ttrain-auc:1\teval-auc:0.877337\n",
      "[867]\ttrain-auc:1\teval-auc:0.877333\n",
      "[868]\ttrain-auc:1\teval-auc:0.877353\n",
      "[869]\ttrain-auc:1\teval-auc:0.877346\n",
      "[870]\ttrain-auc:1\teval-auc:0.877349\n",
      "[871]\ttrain-auc:1\teval-auc:0.877349\n",
      "[872]\ttrain-auc:1\teval-auc:0.877343\n",
      "[873]\ttrain-auc:1\teval-auc:0.877345\n",
      "[874]\ttrain-auc:1\teval-auc:0.877357\n",
      "[875]\ttrain-auc:1\teval-auc:0.877349\n",
      "[876]\ttrain-auc:1\teval-auc:0.877344\n",
      "[877]\ttrain-auc:1\teval-auc:0.877347\n",
      "[878]\ttrain-auc:1\teval-auc:0.877356\n",
      "[879]\ttrain-auc:1\teval-auc:0.877365\n",
      "[880]\ttrain-auc:1\teval-auc:0.877347\n",
      "[881]\ttrain-auc:1\teval-auc:0.87735\n",
      "[882]\ttrain-auc:1\teval-auc:0.877343\n",
      "[883]\ttrain-auc:1\teval-auc:0.877355\n",
      "[884]\ttrain-auc:1\teval-auc:0.877359\n",
      "[885]\ttrain-auc:1\teval-auc:0.877349\n",
      "[886]\ttrain-auc:1\teval-auc:0.877357\n",
      "[887]\ttrain-auc:1\teval-auc:0.877348\n",
      "[888]\ttrain-auc:1\teval-auc:0.877361\n",
      "[889]\ttrain-auc:1\teval-auc:0.87736\n",
      "[890]\ttrain-auc:1\teval-auc:0.877371\n",
      "[891]\ttrain-auc:1\teval-auc:0.877399\n",
      "[892]\ttrain-auc:1\teval-auc:0.877397\n",
      "[893]\ttrain-auc:1\teval-auc:0.8774\n",
      "[894]\ttrain-auc:1\teval-auc:0.877419\n",
      "[895]\ttrain-auc:1\teval-auc:0.877414\n",
      "[896]\ttrain-auc:1\teval-auc:0.87742\n",
      "[897]\ttrain-auc:1\teval-auc:0.877434\n",
      "[898]\ttrain-auc:1\teval-auc:0.877437\n",
      "[899]\ttrain-auc:1\teval-auc:0.877447\n",
      "[900]\ttrain-auc:1\teval-auc:0.877457\n",
      "[901]\ttrain-auc:1\teval-auc:0.877454\n",
      "[902]\ttrain-auc:1\teval-auc:0.87745\n",
      "[903]\ttrain-auc:1\teval-auc:0.877455\n",
      "[904]\ttrain-auc:1\teval-auc:0.877445\n",
      "[905]\ttrain-auc:1\teval-auc:0.87745\n",
      "[906]\ttrain-auc:1\teval-auc:0.877478\n",
      "[907]\ttrain-auc:1\teval-auc:0.877457\n",
      "[908]\ttrain-auc:1\teval-auc:0.877464\n",
      "[909]\ttrain-auc:1\teval-auc:0.877458\n",
      "[910]\ttrain-auc:1\teval-auc:0.877466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[911]\ttrain-auc:1\teval-auc:0.877477\n",
      "[912]\ttrain-auc:1\teval-auc:0.877465\n",
      "[913]\ttrain-auc:1\teval-auc:0.877482\n",
      "[914]\ttrain-auc:1\teval-auc:0.877488\n",
      "[915]\ttrain-auc:1\teval-auc:0.877478\n",
      "[916]\ttrain-auc:1\teval-auc:0.877498\n",
      "[917]\ttrain-auc:1\teval-auc:0.877496\n",
      "[918]\ttrain-auc:1\teval-auc:0.877506\n",
      "[919]\ttrain-auc:1\teval-auc:0.877511\n",
      "[920]\ttrain-auc:1\teval-auc:0.8775\n",
      "[921]\ttrain-auc:1\teval-auc:0.877496\n",
      "[922]\ttrain-auc:1\teval-auc:0.877501\n",
      "[923]\ttrain-auc:1\teval-auc:0.877497\n",
      "[924]\ttrain-auc:1\teval-auc:0.877502\n",
      "[925]\ttrain-auc:1\teval-auc:0.877509\n",
      "[926]\ttrain-auc:1\teval-auc:0.877509\n",
      "[927]\ttrain-auc:1\teval-auc:0.877514\n",
      "[928]\ttrain-auc:1\teval-auc:0.877519\n",
      "[929]\ttrain-auc:1\teval-auc:0.877515\n",
      "[930]\ttrain-auc:1\teval-auc:0.877519\n",
      "[931]\ttrain-auc:1\teval-auc:0.877525\n",
      "[932]\ttrain-auc:1\teval-auc:0.877512\n",
      "[933]\ttrain-auc:1\teval-auc:0.877503\n",
      "[934]\ttrain-auc:1\teval-auc:0.877486\n",
      "[935]\ttrain-auc:1\teval-auc:0.877497\n",
      "[936]\ttrain-auc:1\teval-auc:0.877499\n",
      "[937]\ttrain-auc:1\teval-auc:0.877512\n",
      "[938]\ttrain-auc:1\teval-auc:0.877523\n",
      "[939]\ttrain-auc:1\teval-auc:0.877509\n",
      "[940]\ttrain-auc:1\teval-auc:0.877515\n",
      "[941]\ttrain-auc:1\teval-auc:0.877521\n",
      "[942]\ttrain-auc:1\teval-auc:0.877526\n",
      "[943]\ttrain-auc:1\teval-auc:0.877527\n",
      "[944]\ttrain-auc:1\teval-auc:0.877528\n",
      "[945]\ttrain-auc:1\teval-auc:0.877536\n",
      "[946]\ttrain-auc:1\teval-auc:0.877549\n",
      "[947]\ttrain-auc:1\teval-auc:0.877541\n",
      "[948]\ttrain-auc:1\teval-auc:0.877546\n",
      "[949]\ttrain-auc:1\teval-auc:0.877552\n",
      "[950]\ttrain-auc:1\teval-auc:0.877545\n",
      "[951]\ttrain-auc:1\teval-auc:0.877531\n",
      "[952]\ttrain-auc:1\teval-auc:0.877529\n",
      "[953]\ttrain-auc:1\teval-auc:0.877542\n",
      "[954]\ttrain-auc:1\teval-auc:0.877545\n",
      "[955]\ttrain-auc:1\teval-auc:0.877517\n",
      "[956]\ttrain-auc:1\teval-auc:0.877521\n",
      "[957]\ttrain-auc:1\teval-auc:0.877512\n",
      "[958]\ttrain-auc:1\teval-auc:0.877491\n",
      "[959]\ttrain-auc:1\teval-auc:0.877498\n",
      "[960]\ttrain-auc:1\teval-auc:0.877488\n",
      "[961]\ttrain-auc:1\teval-auc:0.877493\n",
      "[962]\ttrain-auc:1\teval-auc:0.877501\n",
      "[963]\ttrain-auc:1\teval-auc:0.877507\n",
      "[964]\ttrain-auc:1\teval-auc:0.877522\n",
      "[965]\ttrain-auc:1\teval-auc:0.877535\n",
      "[966]\ttrain-auc:1\teval-auc:0.877545\n",
      "[967]\ttrain-auc:1\teval-auc:0.877547\n",
      "[968]\ttrain-auc:1\teval-auc:0.877543\n",
      "[969]\ttrain-auc:1\teval-auc:0.877549\n",
      "[970]\ttrain-auc:1\teval-auc:0.877535\n",
      "[971]\ttrain-auc:1\teval-auc:0.877532\n",
      "[972]\ttrain-auc:1\teval-auc:0.877543\n",
      "[973]\ttrain-auc:1\teval-auc:0.877539\n",
      "[974]\ttrain-auc:1\teval-auc:0.877557\n",
      "[975]\ttrain-auc:1\teval-auc:0.877555\n",
      "[976]\ttrain-auc:1\teval-auc:0.877558\n",
      "[977]\ttrain-auc:1\teval-auc:0.877555\n",
      "[978]\ttrain-auc:1\teval-auc:0.877563\n",
      "[979]\ttrain-auc:1\teval-auc:0.877562\n",
      "[980]\ttrain-auc:1\teval-auc:0.877567\n",
      "[981]\ttrain-auc:1\teval-auc:0.877578\n",
      "[982]\ttrain-auc:1\teval-auc:0.877593\n",
      "[983]\ttrain-auc:1\teval-auc:0.877585\n",
      "[984]\ttrain-auc:1\teval-auc:0.877578\n",
      "[985]\ttrain-auc:1\teval-auc:0.877586\n",
      "[986]\ttrain-auc:1\teval-auc:0.877593\n",
      "[987]\ttrain-auc:1\teval-auc:0.877598\n",
      "[988]\ttrain-auc:1\teval-auc:0.877607\n",
      "[989]\ttrain-auc:1\teval-auc:0.877611\n",
      "[990]\ttrain-auc:1\teval-auc:0.877626\n",
      "[991]\ttrain-auc:1\teval-auc:0.877618\n",
      "[992]\ttrain-auc:1\teval-auc:0.877621\n",
      "[993]\ttrain-auc:1\teval-auc:0.877633\n",
      "[994]\ttrain-auc:1\teval-auc:0.877651\n",
      "[995]\ttrain-auc:1\teval-auc:0.877643\n",
      "[996]\ttrain-auc:1\teval-auc:0.877657\n",
      "[997]\ttrain-auc:1\teval-auc:0.877655\n",
      "[998]\ttrain-auc:1\teval-auc:0.877669\n",
      "[999]\ttrain-auc:1\teval-auc:0.877679\n",
      "[1000]\ttrain-auc:1\teval-auc:0.877684\n",
      "[1001]\ttrain-auc:1\teval-auc:0.877703\n",
      "[1002]\ttrain-auc:1\teval-auc:0.877717\n",
      "[1003]\ttrain-auc:1\teval-auc:0.877729\n",
      "[1004]\ttrain-auc:1\teval-auc:0.877736\n",
      "[1005]\ttrain-auc:1\teval-auc:0.877727\n",
      "[1006]\ttrain-auc:1\teval-auc:0.877717\n",
      "[1007]\ttrain-auc:1\teval-auc:0.877724\n",
      "[1008]\ttrain-auc:1\teval-auc:0.877711\n",
      "[1009]\ttrain-auc:1\teval-auc:0.877708\n",
      "[1010]\ttrain-auc:1\teval-auc:0.877695\n",
      "[1011]\ttrain-auc:1\teval-auc:0.877683\n",
      "[1012]\ttrain-auc:1\teval-auc:0.877686\n",
      "[1013]\ttrain-auc:1\teval-auc:0.877673\n",
      "[1014]\ttrain-auc:1\teval-auc:0.877675\n",
      "[1015]\ttrain-auc:1\teval-auc:0.877663\n",
      "[1016]\ttrain-auc:1\teval-auc:0.877652\n",
      "[1017]\ttrain-auc:1\teval-auc:0.877648\n",
      "[1018]\ttrain-auc:1\teval-auc:0.877645\n",
      "[1019]\ttrain-auc:1\teval-auc:0.877642\n",
      "[1020]\ttrain-auc:1\teval-auc:0.877649\n",
      "[1021]\ttrain-auc:1\teval-auc:0.877656\n",
      "[1022]\ttrain-auc:1\teval-auc:0.877651\n",
      "[1023]\ttrain-auc:1\teval-auc:0.87764\n",
      "[1024]\ttrain-auc:1\teval-auc:0.877645\n",
      "[1025]\ttrain-auc:1\teval-auc:0.877654\n",
      "[1026]\ttrain-auc:1\teval-auc:0.877643\n",
      "[1027]\ttrain-auc:1\teval-auc:0.87765\n",
      "[1028]\ttrain-auc:1\teval-auc:0.877654\n",
      "[1029]\ttrain-auc:1\teval-auc:0.877657\n",
      "[1030]\ttrain-auc:1\teval-auc:0.877664\n",
      "[1031]\ttrain-auc:1\teval-auc:0.877673\n",
      "[1032]\ttrain-auc:1\teval-auc:0.877664\n",
      "[1033]\ttrain-auc:1\teval-auc:0.877665\n",
      "[1034]\ttrain-auc:1\teval-auc:0.877658\n",
      "[1035]\ttrain-auc:1\teval-auc:0.877656\n",
      "[1036]\ttrain-auc:1\teval-auc:0.877652\n",
      "[1037]\ttrain-auc:1\teval-auc:0.87765\n",
      "[1038]\ttrain-auc:1\teval-auc:0.877642\n",
      "[1039]\ttrain-auc:1\teval-auc:0.877662\n",
      "[1040]\ttrain-auc:1\teval-auc:0.877675\n",
      "[1041]\ttrain-auc:1\teval-auc:0.877678\n",
      "[1042]\ttrain-auc:1\teval-auc:0.877697\n",
      "[1043]\ttrain-auc:1\teval-auc:0.877694\n",
      "[1044]\ttrain-auc:1\teval-auc:0.877692\n",
      "[1045]\ttrain-auc:1\teval-auc:0.877692\n",
      "[1046]\ttrain-auc:1\teval-auc:0.877691\n",
      "[1047]\ttrain-auc:1\teval-auc:0.87769\n",
      "[1048]\ttrain-auc:1\teval-auc:0.877689\n",
      "[1049]\ttrain-auc:1\teval-auc:0.877699\n",
      "[1050]\ttrain-auc:1\teval-auc:0.877691\n",
      "[1051]\ttrain-auc:1\teval-auc:0.877705\n",
      "[1052]\ttrain-auc:1\teval-auc:0.877704\n",
      "[1053]\ttrain-auc:1\teval-auc:0.877702\n",
      "[1054]\ttrain-auc:1\teval-auc:0.877703\n",
      "Stopping. Best iteration:\n",
      "[1004]\ttrain-auc:1\teval-auc:0.877736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbm = xgb.train(params, XGB_PartA_X, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_PartB_XGB = gbm.predict(xgb.DMatrix(PartB_X))\n",
    "y_pred_PartC_XGB = gbm.predict(xgb.DMatrix(PartC_X))\n",
    "y_pred_Xtest_XGB = gbm.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartB_Y_after_XGB_NN=pd.DataFrame(PartB_Y)\n",
    "PartC_Y_after_XGB_NN=pd.DataFrame(PartC_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartB_Y_after_XGB_NN['XGB_predict_B']=y_pred_PartB_XGB\n",
    "PartC_Y_after_XGB_NN['XGB_predict_C']=y_pred_PartC_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaller=MinMaxScaler()\n",
    "PartA_X=scaller.fit_transform(PartA_X)\n",
    "PartB_X=scaller.fit_transform(PartB_X)\n",
    "PartC_X=scaller.fit_transform(PartC_X)\n",
    "X_test=scaller.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(PartA_Y),PartA_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55540128, 5.01253133])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs=1000\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(lr=learning_rate, decay=decay_rate, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"model_stacking.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=2, save_best_only=True, mode='max', save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping=EarlyStopping(monitor='val_acc', min_delta=0.000001, patience=50,  verbose=0, mode='max', restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc=roc_auc_callback(training_data=(PartA_X, PartA_Y),validation_data=(PartB_X, PartB_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68000 samples, validate on 66000 samples\n",
      "Epoch 1/1000\n",
      " - 8s - loss: 0.7405 - acc: 0.5866 - val_loss: 0.5142 - val_acc: 0.7722\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.77221, saving model to model_stacking.h5\n",
      "roc_auc: 0.60601 - roc_auc_val: 0.58916 - norm_gini: 0.21202 - norm_gini_val: 0.17831          \n",
      "Epoch 2/1000\n",
      " - 7s - loss: 0.5444 - acc: 0.7299 - val_loss: 0.4115 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.77221 to 0.86794, saving model to model_stacking.h5\n",
      "roc_auc: 0.66969 - roc_auc_val: 0.65014 - norm_gini: 0.33938 - norm_gini_val: 0.30029          \n",
      "Epoch 3/1000\n",
      " - 7s - loss: 0.4521 - acc: 0.8068 - val_loss: 0.3576 - val_acc: 0.8925\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86794 to 0.89245, saving model to model_stacking.h5\n",
      "roc_auc: 0.71035 - roc_auc_val: 0.68835 - norm_gini: 0.42071 - norm_gini_val: 0.3767          \n",
      "Epoch 4/1000\n",
      " - 7s - loss: 0.4040 - acc: 0.8425 - val_loss: 0.3262 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.89245 to 0.89809, saving model to model_stacking.h5\n",
      "roc_auc: 0.7389 - roc_auc_val: 0.71514 - norm_gini: 0.4778 - norm_gini_val: 0.43028          \n",
      "Epoch 5/1000\n",
      " - 7s - loss: 0.3712 - acc: 0.8627 - val_loss: 0.3072 - val_acc: 0.9001\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.89809 to 0.90012, saving model to model_stacking.h5\n",
      "roc_auc: 0.76045 - roc_auc_val: 0.73527 - norm_gini: 0.52089 - norm_gini_val: 0.47054          \n",
      "Epoch 6/1000\n",
      " - 7s - loss: 0.3505 - acc: 0.8753 - val_loss: 0.2958 - val_acc: 0.9008\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.90012 to 0.90082, saving model to model_stacking.h5\n",
      "roc_auc: 0.77834 - roc_auc_val: 0.75185 - norm_gini: 0.55668 - norm_gini_val: 0.50371          \n",
      "Epoch 7/1000\n",
      " - 7s - loss: 0.3346 - acc: 0.8821 - val_loss: 0.2857 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.90082 to 0.90127, saving model to model_stacking.h5\n",
      "roc_auc: 0.79288 - roc_auc_val: 0.76761 - norm_gini: 0.58576 - norm_gini_val: 0.53521          \n",
      "Epoch 8/1000\n",
      " - 7s - loss: 0.3205 - acc: 0.8864 - val_loss: 0.2793 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.90127 to 0.90192, saving model to model_stacking.h5\n",
      "roc_auc: 0.80425 - roc_auc_val: 0.77891 - norm_gini: 0.60849 - norm_gini_val: 0.55783          \n",
      "Epoch 9/1000\n",
      " - 7s - loss: 0.3103 - acc: 0.8902 - val_loss: 0.2744 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.90192 to 0.90229, saving model to model_stacking.h5\n",
      "roc_auc: 0.81456 - roc_auc_val: 0.78878 - norm_gini: 0.62913 - norm_gini_val: 0.57755          \n",
      "Epoch 10/1000\n",
      " - 7s - loss: 0.3047 - acc: 0.8933 - val_loss: 0.2704 - val_acc: 0.9031\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.90229 to 0.90311, saving model to model_stacking.h5\n",
      "roc_auc: 0.82295 - roc_auc_val: 0.7969 - norm_gini: 0.6459 - norm_gini_val: 0.5938          \n",
      "Epoch 11/1000\n",
      " - 8s - loss: 0.2961 - acc: 0.8951 - val_loss: 0.2671 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.90311 to 0.90330, saving model to model_stacking.h5\n",
      "roc_auc: 0.82973 - roc_auc_val: 0.80367 - norm_gini: 0.65947 - norm_gini_val: 0.60733          \n",
      "Epoch 12/1000\n",
      " - 7s - loss: 0.2905 - acc: 0.8968 - val_loss: 0.2641 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.90330 to 0.90397, saving model to model_stacking.h5\n",
      "roc_auc: 0.83607 - roc_auc_val: 0.80998 - norm_gini: 0.67213 - norm_gini_val: 0.61995          \n",
      "Epoch 13/1000\n",
      " - 7s - loss: 0.2872 - acc: 0.8977 - val_loss: 0.2613 - val_acc: 0.9049\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.90397 to 0.90494, saving model to model_stacking.h5\n",
      "roc_auc: 0.84133 - roc_auc_val: 0.81502 - norm_gini: 0.68266 - norm_gini_val: 0.63004          \n",
      "Epoch 14/1000\n",
      " - 7s - loss: 0.2828 - acc: 0.8995 - val_loss: 0.2599 - val_acc: 0.9047\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.90494\n",
      "roc_auc: 0.84549 - roc_auc_val: 0.81915 - norm_gini: 0.69099 - norm_gini_val: 0.63829          \n",
      "Epoch 15/1000\n",
      " - 7s - loss: 0.2800 - acc: 0.8988 - val_loss: 0.2582 - val_acc: 0.9051\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.90494 to 0.90506, saving model to model_stacking.h5\n",
      "roc_auc: 0.84992 - roc_auc_val: 0.8228 - norm_gini: 0.69984 - norm_gini_val: 0.64559          \n",
      "Epoch 16/1000\n",
      " - 8s - loss: 0.2756 - acc: 0.9018 - val_loss: 0.2561 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.90506 to 0.90561, saving model to model_stacking.h5\n",
      "roc_auc: 0.85372 - roc_auc_val: 0.82638 - norm_gini: 0.70744 - norm_gini_val: 0.65277          \n",
      "Epoch 17/1000\n",
      " - 8s - loss: 0.2728 - acc: 0.9011 - val_loss: 0.2546 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.90561 to 0.90621, saving model to model_stacking.h5\n",
      "roc_auc: 0.85694 - roc_auc_val: 0.82922 - norm_gini: 0.71387 - norm_gini_val: 0.65845          \n",
      "Epoch 18/1000\n",
      " - 8s - loss: 0.2703 - acc: 0.9020 - val_loss: 0.2535 - val_acc: 0.9063\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.90621 to 0.90627, saving model to model_stacking.h5\n",
      "roc_auc: 0.85899 - roc_auc_val: 0.83158 - norm_gini: 0.71797 - norm_gini_val: 0.66316          \n",
      "Epoch 19/1000\n",
      " - 8s - loss: 0.2663 - acc: 0.9039 - val_loss: 0.2524 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.90627 to 0.90667, saving model to model_stacking.h5\n",
      "roc_auc: 0.86149 - roc_auc_val: 0.83373 - norm_gini: 0.72299 - norm_gini_val: 0.66746          \n",
      "Epoch 20/1000\n",
      " - 7s - loss: 0.2636 - acc: 0.9045 - val_loss: 0.2531 - val_acc: 0.9066\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90667\n",
      "roc_auc: 0.86343 - roc_auc_val: 0.83523 - norm_gini: 0.72686 - norm_gini_val: 0.67045          \n",
      "Epoch 21/1000\n",
      " - 7s - loss: 0.2632 - acc: 0.9040 - val_loss: 0.2519 - val_acc: 0.9070\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.90667 to 0.90702, saving model to model_stacking.h5\n",
      "roc_auc: 0.86564 - roc_auc_val: 0.83685 - norm_gini: 0.73129 - norm_gini_val: 0.6737          \n",
      "Epoch 22/1000\n",
      " - 7s - loss: 0.2608 - acc: 0.9043 - val_loss: 0.2510 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.90702 to 0.90738, saving model to model_stacking.h5\n",
      "roc_auc: 0.86714 - roc_auc_val: 0.83827 - norm_gini: 0.73428 - norm_gini_val: 0.67654          \n",
      "Epoch 23/1000\n",
      " - 7s - loss: 0.2590 - acc: 0.9057 - val_loss: 0.2498 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.90738 to 0.90773, saving model to model_stacking.h5\n",
      "roc_auc: 0.869 - roc_auc_val: 0.83965 - norm_gini: 0.73801 - norm_gini_val: 0.6793          \n",
      "Epoch 24/1000\n",
      " - 7s - loss: 0.2598 - acc: 0.9053 - val_loss: 0.2507 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90773\n",
      "roc_auc: 0.87026 - roc_auc_val: 0.84066 - norm_gini: 0.74051 - norm_gini_val: 0.68133          \n",
      "Epoch 25/1000\n",
      " - 7s - loss: 0.2562 - acc: 0.9068 - val_loss: 0.2492 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.90773 to 0.90829, saving model to model_stacking.h5\n",
      "roc_auc: 0.87165 - roc_auc_val: 0.84175 - norm_gini: 0.74331 - norm_gini_val: 0.6835          \n",
      "Epoch 26/1000\n",
      " - 7s - loss: 0.2537 - acc: 0.9076 - val_loss: 0.2496 - val_acc: 0.9082\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90829\n",
      "roc_auc: 0.873 - roc_auc_val: 0.84251 - norm_gini: 0.746 - norm_gini_val: 0.68503          \n",
      "Epoch 27/1000\n",
      " - 7s - loss: 0.2551 - acc: 0.9077 - val_loss: 0.2487 - val_acc: 0.9085\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.90829 to 0.90850, saving model to model_stacking.h5\n",
      "roc_auc: 0.87427 - roc_auc_val: 0.84312 - norm_gini: 0.74854 - norm_gini_val: 0.68625          \n",
      "Epoch 28/1000\n",
      " - 7s - loss: 0.2519 - acc: 0.9078 - val_loss: 0.2503 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90850\n",
      "roc_auc: 0.87529 - roc_auc_val: 0.84365 - norm_gini: 0.75058 - norm_gini_val: 0.6873          \n",
      "Epoch 29/1000\n",
      " - 7s - loss: 0.2512 - acc: 0.9075 - val_loss: 0.2485 - val_acc: 0.9087\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.90850 to 0.90870, saving model to model_stacking.h5\n",
      "roc_auc: 0.87617 - roc_auc_val: 0.84468 - norm_gini: 0.75234 - norm_gini_val: 0.68937          \n",
      "Epoch 30/1000\n",
      " - 7s - loss: 0.2510 - acc: 0.9087 - val_loss: 0.2484 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.90870 to 0.90895, saving model to model_stacking.h5\n",
      "roc_auc: 0.87692 - roc_auc_val: 0.84543 - norm_gini: 0.75385 - norm_gini_val: 0.69086          \n",
      "Epoch 31/1000\n",
      " - 7s - loss: 0.2501 - acc: 0.9076 - val_loss: 0.2475 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.90895 to 0.90917, saving model to model_stacking.h5\n",
      "roc_auc: 0.87793 - roc_auc_val: 0.84615 - norm_gini: 0.75586 - norm_gini_val: 0.69229          \n",
      "Epoch 32/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 7s - loss: 0.2494 - acc: 0.9091 - val_loss: 0.2483 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.90917\n",
      "roc_auc: 0.87877 - roc_auc_val: 0.84665 - norm_gini: 0.75754 - norm_gini_val: 0.69331          \n",
      "Epoch 33/1000\n",
      " - 7s - loss: 0.2468 - acc: 0.9098 - val_loss: 0.2483 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.90917 to 0.90921, saving model to model_stacking.h5\n",
      "roc_auc: 0.87969 - roc_auc_val: 0.84706 - norm_gini: 0.75937 - norm_gini_val: 0.69412          \n",
      "Epoch 34/1000\n",
      " - 7s - loss: 0.2481 - acc: 0.9087 - val_loss: 0.2480 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.90921 to 0.90926, saving model to model_stacking.h5\n",
      "roc_auc: 0.88042 - roc_auc_val: 0.8476 - norm_gini: 0.76084 - norm_gini_val: 0.69519          \n",
      "Epoch 35/1000\n",
      " - 7s - loss: 0.2481 - acc: 0.9092 - val_loss: 0.2459 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.90926 to 0.90968, saving model to model_stacking.h5\n",
      "roc_auc: 0.88128 - roc_auc_val: 0.84858 - norm_gini: 0.76256 - norm_gini_val: 0.69717          \n",
      "Epoch 36/1000\n",
      " - 7s - loss: 0.2451 - acc: 0.9103 - val_loss: 0.2467 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.90968\n",
      "roc_auc: 0.88182 - roc_auc_val: 0.84873 - norm_gini: 0.76363 - norm_gini_val: 0.69746          \n",
      "Epoch 37/1000\n",
      " - 7s - loss: 0.2464 - acc: 0.9101 - val_loss: 0.2470 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.90968\n",
      "roc_auc: 0.88234 - roc_auc_val: 0.84886 - norm_gini: 0.76467 - norm_gini_val: 0.69773          \n",
      "Epoch 38/1000\n",
      " - 7s - loss: 0.2438 - acc: 0.9108 - val_loss: 0.2470 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.90968\n",
      "roc_auc: 0.88289 - roc_auc_val: 0.84926 - norm_gini: 0.76579 - norm_gini_val: 0.69852          \n",
      "Epoch 39/1000\n",
      " - 7s - loss: 0.2447 - acc: 0.9111 - val_loss: 0.2465 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.90968\n",
      "roc_auc: 0.88357 - roc_auc_val: 0.84956 - norm_gini: 0.76714 - norm_gini_val: 0.69912          \n",
      "Epoch 40/1000\n",
      " - 7s - loss: 0.2443 - acc: 0.9104 - val_loss: 0.2467 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.90968\n",
      "roc_auc: 0.88417 - roc_auc_val: 0.8501 - norm_gini: 0.76833 - norm_gini_val: 0.70019          \n",
      "Epoch 41/1000\n",
      " - 7s - loss: 0.2431 - acc: 0.9120 - val_loss: 0.2458 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.90968 to 0.90982, saving model to model_stacking.h5\n",
      "roc_auc: 0.88499 - roc_auc_val: 0.85047 - norm_gini: 0.76998 - norm_gini_val: 0.70094          \n",
      "Epoch 42/1000\n",
      " - 7s - loss: 0.2414 - acc: 0.9112 - val_loss: 0.2464 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.90982\n",
      "roc_auc: 0.88546 - roc_auc_val: 0.85062 - norm_gini: 0.77091 - norm_gini_val: 0.70124          \n",
      "Epoch 43/1000\n",
      " - 7s - loss: 0.2425 - acc: 0.9106 - val_loss: 0.2463 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.90982\n",
      "roc_auc: 0.88615 - roc_auc_val: 0.85088 - norm_gini: 0.7723 - norm_gini_val: 0.70175          \n",
      "Epoch 44/1000\n",
      " - 7s - loss: 0.2399 - acc: 0.9111 - val_loss: 0.2461 - val_acc: 0.9096\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.90982\n",
      "roc_auc: 0.88657 - roc_auc_val: 0.8511 - norm_gini: 0.77314 - norm_gini_val: 0.70219          \n",
      "Epoch 45/1000\n",
      " - 7s - loss: 0.2395 - acc: 0.9120 - val_loss: 0.2462 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.90982 to 0.90995, saving model to model_stacking.h5\n",
      "roc_auc: 0.88719 - roc_auc_val: 0.85151 - norm_gini: 0.77437 - norm_gini_val: 0.70302          \n",
      "Epoch 46/1000\n",
      " - 7s - loss: 0.2394 - acc: 0.9124 - val_loss: 0.2458 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.90995 to 0.90997, saving model to model_stacking.h5\n",
      "roc_auc: 0.88767 - roc_auc_val: 0.85193 - norm_gini: 0.77533 - norm_gini_val: 0.70386          \n",
      "Epoch 47/1000\n",
      " - 7s - loss: 0.2403 - acc: 0.9115 - val_loss: 0.2462 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.90997 to 0.91002, saving model to model_stacking.h5\n",
      "roc_auc: 0.88811 - roc_auc_val: 0.85194 - norm_gini: 0.77623 - norm_gini_val: 0.70388          \n",
      "Epoch 48/1000\n",
      " - 7s - loss: 0.2398 - acc: 0.9120 - val_loss: 0.2461 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.91002 to 0.91005, saving model to model_stacking.h5\n",
      "roc_auc: 0.88865 - roc_auc_val: 0.85228 - norm_gini: 0.7773 - norm_gini_val: 0.70456          \n",
      "Epoch 49/1000\n",
      " - 7s - loss: 0.2383 - acc: 0.9127 - val_loss: 0.2456 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.91005 to 0.91027, saving model to model_stacking.h5\n",
      "roc_auc: 0.88914 - roc_auc_val: 0.85253 - norm_gini: 0.77829 - norm_gini_val: 0.70506          \n",
      "Epoch 50/1000\n",
      " - 7s - loss: 0.2361 - acc: 0.9128 - val_loss: 0.2449 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.91027\n",
      "roc_auc: 0.88941 - roc_auc_val: 0.85292 - norm_gini: 0.77883 - norm_gini_val: 0.70585          \n",
      "Epoch 51/1000\n",
      " - 7s - loss: 0.2367 - acc: 0.9136 - val_loss: 0.2449 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.91027 to 0.91038, saving model to model_stacking.h5\n",
      "roc_auc: 0.88983 - roc_auc_val: 0.85308 - norm_gini: 0.77966 - norm_gini_val: 0.70615          \n",
      "Epoch 52/1000\n",
      " - 7s - loss: 0.2373 - acc: 0.9129 - val_loss: 0.2458 - val_acc: 0.9101\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.91038\n",
      "roc_auc: 0.89022 - roc_auc_val: 0.85299 - norm_gini: 0.78044 - norm_gini_val: 0.70598          \n",
      "Epoch 53/1000\n",
      " - 7s - loss: 0.2354 - acc: 0.9123 - val_loss: 0.2455 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.91038 to 0.91045, saving model to model_stacking.h5\n",
      "roc_auc: 0.8905 - roc_auc_val: 0.85324 - norm_gini: 0.78101 - norm_gini_val: 0.70648          \n",
      "Epoch 54/1000\n",
      " - 7s - loss: 0.2354 - acc: 0.9130 - val_loss: 0.2450 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.91045\n",
      "roc_auc: 0.89084 - roc_auc_val: 0.85351 - norm_gini: 0.78169 - norm_gini_val: 0.70702          \n",
      "Epoch 55/1000\n",
      " - 7s - loss: 0.2365 - acc: 0.9128 - val_loss: 0.2458 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.91045\n",
      "roc_auc: 0.89126 - roc_auc_val: 0.85357 - norm_gini: 0.78251 - norm_gini_val: 0.70713          \n",
      "Epoch 56/1000\n",
      " - 7s - loss: 0.2343 - acc: 0.9132 - val_loss: 0.2459 - val_acc: 0.9101\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.91045\n",
      "roc_auc: 0.8916 - roc_auc_val: 0.85383 - norm_gini: 0.7832 - norm_gini_val: 0.70765          \n",
      "Epoch 57/1000\n",
      " - 7s - loss: 0.2349 - acc: 0.9140 - val_loss: 0.2461 - val_acc: 0.9102\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.91045\n",
      "roc_auc: 0.89216 - roc_auc_val: 0.85381 - norm_gini: 0.78432 - norm_gini_val: 0.70762          \n",
      "Epoch 58/1000\n",
      " - 7s - loss: 0.2345 - acc: 0.9136 - val_loss: 0.2463 - val_acc: 0.9102\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.91045\n",
      "roc_auc: 0.8924 - roc_auc_val: 0.85397 - norm_gini: 0.7848 - norm_gini_val: 0.70794          \n",
      "Epoch 59/1000\n",
      " - 7s - loss: 0.2353 - acc: 0.9136 - val_loss: 0.2456 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.91045\n",
      "roc_auc: 0.89278 - roc_auc_val: 0.85421 - norm_gini: 0.78556 - norm_gini_val: 0.70843          \n",
      "Epoch 60/1000\n",
      " - 7s - loss: 0.2325 - acc: 0.9137 - val_loss: 0.2443 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.91045\n",
      "roc_auc: 0.89305 - roc_auc_val: 0.8544 - norm_gini: 0.78609 - norm_gini_val: 0.70881          \n",
      "Epoch 61/1000\n",
      " - 7s - loss: 0.2342 - acc: 0.9143 - val_loss: 0.2449 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.91045\n",
      "roc_auc: 0.89346 - roc_auc_val: 0.85446 - norm_gini: 0.78691 - norm_gini_val: 0.70893          \n",
      "Epoch 62/1000\n",
      " - 7s - loss: 0.2340 - acc: 0.9138 - val_loss: 0.2445 - val_acc: 0.9107\n",
      "\n",
      "Epoch 00062: val_acc improved from 0.91045 to 0.91073, saving model to model_stacking.h5\n",
      "roc_auc: 0.89386 - roc_auc_val: 0.85455 - norm_gini: 0.78772 - norm_gini_val: 0.70909          \n",
      "Epoch 63/1000\n",
      " - 7s - loss: 0.2331 - acc: 0.9136 - val_loss: 0.2456 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.89438 - roc_auc_val: 0.85463 - norm_gini: 0.78875 - norm_gini_val: 0.70926          \n",
      "Epoch 64/1000\n",
      " - 7s - loss: 0.2319 - acc: 0.9143 - val_loss: 0.2451 - val_acc: 0.9106\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.89463 - roc_auc_val: 0.85491 - norm_gini: 0.78927 - norm_gini_val: 0.70982          \n",
      "Epoch 65/1000\n",
      " - 7s - loss: 0.2320 - acc: 0.9148 - val_loss: 0.2454 - val_acc: 0.9105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.89514 - roc_auc_val: 0.85476 - norm_gini: 0.79028 - norm_gini_val: 0.70953          \n",
      "Epoch 66/1000\n",
      " - 7s - loss: 0.2317 - acc: 0.9144 - val_loss: 0.2458 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.89523 - roc_auc_val: 0.85488 - norm_gini: 0.79047 - norm_gini_val: 0.70975          \n",
      "Epoch 67/1000\n",
      " - 7s - loss: 0.2306 - acc: 0.9144 - val_loss: 0.2457 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.8955 - roc_auc_val: 0.85492 - norm_gini: 0.79099 - norm_gini_val: 0.70984          \n",
      "Epoch 68/1000\n",
      " - 7s - loss: 0.2326 - acc: 0.9141 - val_loss: 0.2453 - val_acc: 0.9106\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.89571 - roc_auc_val: 0.8552 - norm_gini: 0.79142 - norm_gini_val: 0.71039          \n",
      "Epoch 69/1000\n",
      " - 7s - loss: 0.2301 - acc: 0.9144 - val_loss: 0.2470 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.89618 - roc_auc_val: 0.85508 - norm_gini: 0.79236 - norm_gini_val: 0.71016          \n",
      "Epoch 70/1000\n",
      " - 8s - loss: 0.2306 - acc: 0.9141 - val_loss: 0.2451 - val_acc: 0.9106\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.89642 - roc_auc_val: 0.85538 - norm_gini: 0.79285 - norm_gini_val: 0.71077          \n",
      "Epoch 71/1000\n",
      " - 7s - loss: 0.2305 - acc: 0.9149 - val_loss: 0.2464 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91073\n",
      "roc_auc: 0.89671 - roc_auc_val: 0.85561 - norm_gini: 0.79343 - norm_gini_val: 0.71123          \n",
      "Epoch 72/1000\n",
      " - 7s - loss: 0.2295 - acc: 0.9148 - val_loss: 0.2455 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00072: val_acc improved from 0.91073 to 0.91079, saving model to model_stacking.h5\n",
      "roc_auc: 0.89699 - roc_auc_val: 0.85564 - norm_gini: 0.79399 - norm_gini_val: 0.71128          \n",
      "Epoch 73/1000\n",
      " - 7s - loss: 0.2294 - acc: 0.9149 - val_loss: 0.2445 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00073: val_acc improved from 0.91079 to 0.91097, saving model to model_stacking.h5\n",
      "roc_auc: 0.8972 - roc_auc_val: 0.85571 - norm_gini: 0.7944 - norm_gini_val: 0.71142          \n",
      "Epoch 74/1000\n",
      " - 7s - loss: 0.2289 - acc: 0.9160 - val_loss: 0.2453 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91097\n",
      "roc_auc: 0.89756 - roc_auc_val: 0.85589 - norm_gini: 0.79512 - norm_gini_val: 0.71179          \n",
      "Epoch 75/1000\n",
      " - 7s - loss: 0.2283 - acc: 0.9155 - val_loss: 0.2453 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.91097\n",
      "roc_auc: 0.89787 - roc_auc_val: 0.85593 - norm_gini: 0.79575 - norm_gini_val: 0.71187          \n",
      "Epoch 76/1000\n",
      " - 7s - loss: 0.2295 - acc: 0.9158 - val_loss: 0.2458 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91097\n",
      "roc_auc: 0.89826 - roc_auc_val: 0.85591 - norm_gini: 0.79651 - norm_gini_val: 0.71182          \n",
      "Epoch 77/1000\n",
      " - 7s - loss: 0.2280 - acc: 0.9157 - val_loss: 0.2448 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00077: val_acc improved from 0.91097 to 0.91123, saving model to model_stacking.h5\n",
      "roc_auc: 0.89839 - roc_auc_val: 0.85625 - norm_gini: 0.79678 - norm_gini_val: 0.7125          \n",
      "Epoch 78/1000\n",
      " - 7s - loss: 0.2287 - acc: 0.9151 - val_loss: 0.2465 - val_acc: 0.9106\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.91123\n",
      "roc_auc: 0.89877 - roc_auc_val: 0.85609 - norm_gini: 0.79754 - norm_gini_val: 0.71218          \n",
      "Epoch 79/1000\n",
      " - 7s - loss: 0.2283 - acc: 0.9151 - val_loss: 0.2456 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.91123\n",
      "roc_auc: 0.89904 - roc_auc_val: 0.85625 - norm_gini: 0.79808 - norm_gini_val: 0.71249          \n",
      "Epoch 80/1000\n",
      " - 7s - loss: 0.2270 - acc: 0.9161 - val_loss: 0.2450 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.91123\n",
      "roc_auc: 0.89942 - roc_auc_val: 0.85634 - norm_gini: 0.79885 - norm_gini_val: 0.71267          \n",
      "Epoch 81/1000\n",
      " - 7s - loss: 0.2283 - acc: 0.9161 - val_loss: 0.2447 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.91123 to 0.91124, saving model to model_stacking.h5\n",
      "roc_auc: 0.89963 - roc_auc_val: 0.85635 - norm_gini: 0.79926 - norm_gini_val: 0.71271          \n",
      "Epoch 82/1000\n",
      " - 7s - loss: 0.2285 - acc: 0.9149 - val_loss: 0.2457 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.91124\n",
      "roc_auc: 0.89984 - roc_auc_val: 0.85635 - norm_gini: 0.79967 - norm_gini_val: 0.71271          \n",
      "Epoch 83/1000\n",
      " - 7s - loss: 0.2260 - acc: 0.9158 - val_loss: 0.2455 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91124\n",
      "roc_auc: 0.90019 - roc_auc_val: 0.85642 - norm_gini: 0.80038 - norm_gini_val: 0.71283          \n",
      "Epoch 84/1000\n",
      " - 7s - loss: 0.2273 - acc: 0.9156 - val_loss: 0.2452 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91124\n",
      "roc_auc: 0.90049 - roc_auc_val: 0.85645 - norm_gini: 0.80098 - norm_gini_val: 0.7129          \n",
      "Epoch 85/1000\n",
      " - 7s - loss: 0.2263 - acc: 0.9156 - val_loss: 0.2456 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91124\n",
      "roc_auc: 0.90069 - roc_auc_val: 0.85647 - norm_gini: 0.80137 - norm_gini_val: 0.71294          \n",
      "Epoch 86/1000\n",
      " - 7s - loss: 0.2268 - acc: 0.9159 - val_loss: 0.2452 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.91124\n",
      "roc_auc: 0.90095 - roc_auc_val: 0.85651 - norm_gini: 0.80191 - norm_gini_val: 0.71301          \n",
      "Epoch 87/1000\n",
      " - 7s - loss: 0.2274 - acc: 0.9152 - val_loss: 0.2463 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91124\n",
      "roc_auc: 0.90131 - roc_auc_val: 0.8566 - norm_gini: 0.80262 - norm_gini_val: 0.71321          \n",
      "Epoch 88/1000\n",
      " - 7s - loss: 0.2268 - acc: 0.9160 - val_loss: 0.2447 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00088: val_acc improved from 0.91124 to 0.91130, saving model to model_stacking.h5\n",
      "roc_auc: 0.90142 - roc_auc_val: 0.8568 - norm_gini: 0.80284 - norm_gini_val: 0.7136          \n",
      "Epoch 89/1000\n",
      " - 7s - loss: 0.2260 - acc: 0.9161 - val_loss: 0.2453 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90164 - roc_auc_val: 0.85685 - norm_gini: 0.80328 - norm_gini_val: 0.71369          \n",
      "Epoch 90/1000\n",
      " - 7s - loss: 0.2269 - acc: 0.9162 - val_loss: 0.2452 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90191 - roc_auc_val: 0.85683 - norm_gini: 0.80383 - norm_gini_val: 0.71366          \n",
      "Epoch 91/1000\n",
      " - 7s - loss: 0.2260 - acc: 0.9164 - val_loss: 0.2448 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90219 - roc_auc_val: 0.85698 - norm_gini: 0.80438 - norm_gini_val: 0.71396          \n",
      "Epoch 92/1000\n",
      " - 7s - loss: 0.2261 - acc: 0.9166 - val_loss: 0.2463 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90253 - roc_auc_val: 0.85699 - norm_gini: 0.80506 - norm_gini_val: 0.71399          \n",
      "Epoch 93/1000\n",
      " - 7s - loss: 0.2253 - acc: 0.9160 - val_loss: 0.2448 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90251 - roc_auc_val: 0.85714 - norm_gini: 0.80502 - norm_gini_val: 0.71427          \n",
      "Epoch 94/1000\n",
      " - 7s - loss: 0.2253 - acc: 0.9172 - val_loss: 0.2469 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90296 - roc_auc_val: 0.85691 - norm_gini: 0.80592 - norm_gini_val: 0.71382          \n",
      "Epoch 95/1000\n",
      " - 7s - loss: 0.2250 - acc: 0.9171 - val_loss: 0.2472 - val_acc: 0.9107\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90307 - roc_auc_val: 0.85716 - norm_gini: 0.80613 - norm_gini_val: 0.71432          \n",
      "Epoch 96/1000\n",
      " - 7s - loss: 0.2252 - acc: 0.9165 - val_loss: 0.2454 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90335 - roc_auc_val: 0.85705 - norm_gini: 0.8067 - norm_gini_val: 0.71411          \n",
      "Epoch 97/1000\n",
      " - 7s - loss: 0.2241 - acc: 0.9171 - val_loss: 0.2458 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.9035 - roc_auc_val: 0.85739 - norm_gini: 0.80701 - norm_gini_val: 0.71479          \n",
      "Epoch 98/1000\n",
      " - 8s - loss: 0.2247 - acc: 0.9161 - val_loss: 0.2456 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90371 - roc_auc_val: 0.85756 - norm_gini: 0.80741 - norm_gini_val: 0.71511          \n",
      "Epoch 99/1000\n",
      " - 7s - loss: 0.2235 - acc: 0.9170 - val_loss: 0.2453 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90396 - roc_auc_val: 0.85753 - norm_gini: 0.80792 - norm_gini_val: 0.71506          \n",
      "Epoch 100/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 7s - loss: 0.2234 - acc: 0.9174 - val_loss: 0.2460 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.90437 - roc_auc_val: 0.85748 - norm_gini: 0.80875 - norm_gini_val: 0.71496          \n",
      "Epoch 101/1000\n",
      " - 7s - loss: 0.2230 - acc: 0.9173 - val_loss: 0.2464 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91130\n",
      "roc_auc: 0.9046 - roc_auc_val: 0.85748 - norm_gini: 0.80919 - norm_gini_val: 0.71495          \n",
      "Epoch 102/1000\n",
      " - 7s - loss: 0.2232 - acc: 0.9167 - val_loss: 0.2450 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00102: val_acc improved from 0.91130 to 0.91138, saving model to model_stacking.h5\n",
      "roc_auc: 0.9047 - roc_auc_val: 0.85757 - norm_gini: 0.80939 - norm_gini_val: 0.71513          \n",
      "Epoch 103/1000\n",
      " - 7s - loss: 0.2219 - acc: 0.9173 - val_loss: 0.2467 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.91138\n",
      "roc_auc: 0.90506 - roc_auc_val: 0.85748 - norm_gini: 0.81013 - norm_gini_val: 0.71495          \n",
      "Epoch 104/1000\n",
      " - 7s - loss: 0.2232 - acc: 0.9168 - val_loss: 0.2461 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91138\n",
      "roc_auc: 0.90535 - roc_auc_val: 0.85763 - norm_gini: 0.8107 - norm_gini_val: 0.71525          \n",
      "Epoch 105/1000\n",
      " - 7s - loss: 0.2231 - acc: 0.9172 - val_loss: 0.2456 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91138\n",
      "roc_auc: 0.90544 - roc_auc_val: 0.85778 - norm_gini: 0.81087 - norm_gini_val: 0.71557          \n",
      "Epoch 106/1000\n",
      " - 7s - loss: 0.2220 - acc: 0.9175 - val_loss: 0.2463 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91138\n",
      "roc_auc: 0.90574 - roc_auc_val: 0.85772 - norm_gini: 0.81149 - norm_gini_val: 0.71543          \n",
      "Epoch 107/1000\n",
      " - 7s - loss: 0.2233 - acc: 0.9170 - val_loss: 0.2454 - val_acc: 0.9116\n",
      "\n",
      "Epoch 00107: val_acc improved from 0.91138 to 0.91156, saving model to model_stacking.h5\n",
      "roc_auc: 0.90584 - roc_auc_val: 0.85781 - norm_gini: 0.81168 - norm_gini_val: 0.71561          \n",
      "Epoch 108/1000\n",
      " - 7s - loss: 0.2220 - acc: 0.9173 - val_loss: 0.2461 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90629 - roc_auc_val: 0.85769 - norm_gini: 0.81257 - norm_gini_val: 0.71538          \n",
      "Epoch 109/1000\n",
      " - 7s - loss: 0.2213 - acc: 0.9179 - val_loss: 0.2454 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90647 - roc_auc_val: 0.85785 - norm_gini: 0.81294 - norm_gini_val: 0.71569          \n",
      "Epoch 110/1000\n",
      " - 7s - loss: 0.2224 - acc: 0.9173 - val_loss: 0.2462 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90676 - roc_auc_val: 0.85779 - norm_gini: 0.81351 - norm_gini_val: 0.71557          \n",
      "Epoch 111/1000\n",
      " - 7s - loss: 0.2234 - acc: 0.9168 - val_loss: 0.2468 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90704 - roc_auc_val: 0.85776 - norm_gini: 0.81407 - norm_gini_val: 0.71552          \n",
      "Epoch 112/1000\n",
      " - 7s - loss: 0.2217 - acc: 0.9181 - val_loss: 0.2460 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90711 - roc_auc_val: 0.85787 - norm_gini: 0.81423 - norm_gini_val: 0.71575          \n",
      "Epoch 113/1000\n",
      " - 7s - loss: 0.2221 - acc: 0.9170 - val_loss: 0.2479 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90749 - roc_auc_val: 0.85787 - norm_gini: 0.81498 - norm_gini_val: 0.71574          \n",
      "Epoch 114/1000\n",
      " - 7s - loss: 0.2199 - acc: 0.9186 - val_loss: 0.2458 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.9076 - roc_auc_val: 0.8579 - norm_gini: 0.8152 - norm_gini_val: 0.7158          \n",
      "Epoch 115/1000\n",
      " - 7s - loss: 0.2216 - acc: 0.9171 - val_loss: 0.2466 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90787 - roc_auc_val: 0.85793 - norm_gini: 0.81574 - norm_gini_val: 0.71586          \n",
      "Epoch 116/1000\n",
      " - 7s - loss: 0.2210 - acc: 0.9185 - val_loss: 0.2461 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.9081 - roc_auc_val: 0.85779 - norm_gini: 0.8162 - norm_gini_val: 0.71558          \n",
      "Epoch 117/1000\n",
      " - 7s - loss: 0.2216 - acc: 0.9173 - val_loss: 0.2466 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90839 - roc_auc_val: 0.85778 - norm_gini: 0.81679 - norm_gini_val: 0.71556          \n",
      "Epoch 118/1000\n",
      " - 7s - loss: 0.2209 - acc: 0.9174 - val_loss: 0.2469 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90853 - roc_auc_val: 0.85787 - norm_gini: 0.81705 - norm_gini_val: 0.71575          \n",
      "Epoch 119/1000\n",
      " - 8s - loss: 0.2215 - acc: 0.9168 - val_loss: 0.2463 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90868 - roc_auc_val: 0.85797 - norm_gini: 0.81735 - norm_gini_val: 0.71594          \n",
      "Epoch 120/1000\n",
      " - 7s - loss: 0.2206 - acc: 0.9175 - val_loss: 0.2471 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90894 - roc_auc_val: 0.85786 - norm_gini: 0.81788 - norm_gini_val: 0.71571          \n",
      "Epoch 121/1000\n",
      " - 8s - loss: 0.2195 - acc: 0.9175 - val_loss: 0.2467 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90928 - roc_auc_val: 0.85791 - norm_gini: 0.81855 - norm_gini_val: 0.71583          \n",
      "Epoch 122/1000\n",
      " - 8s - loss: 0.2209 - acc: 0.9171 - val_loss: 0.2475 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90955 - roc_auc_val: 0.85797 - norm_gini: 0.81909 - norm_gini_val: 0.71593          \n",
      "Epoch 123/1000\n",
      " - 8s - loss: 0.2195 - acc: 0.9188 - val_loss: 0.2473 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90956 - roc_auc_val: 0.85812 - norm_gini: 0.81912 - norm_gini_val: 0.71624          \n",
      "Epoch 124/1000\n",
      " - 8s - loss: 0.2197 - acc: 0.9181 - val_loss: 0.2466 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90988 - roc_auc_val: 0.85822 - norm_gini: 0.81976 - norm_gini_val: 0.71644          \n",
      "Epoch 125/1000\n",
      " - 8s - loss: 0.2197 - acc: 0.9179 - val_loss: 0.2459 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.90998 - roc_auc_val: 0.85816 - norm_gini: 0.81996 - norm_gini_val: 0.71631          \n",
      "Epoch 126/1000\n",
      " - 7s - loss: 0.2197 - acc: 0.9175 - val_loss: 0.2473 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.91037 - roc_auc_val: 0.8581 - norm_gini: 0.82075 - norm_gini_val: 0.7162          \n",
      "Epoch 127/1000\n",
      " - 8s - loss: 0.2192 - acc: 0.9183 - val_loss: 0.2471 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.9106 - roc_auc_val: 0.85813 - norm_gini: 0.8212 - norm_gini_val: 0.71627          \n",
      "Epoch 128/1000\n",
      " - 7s - loss: 0.2189 - acc: 0.9177 - val_loss: 0.2475 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.91084 - roc_auc_val: 0.85818 - norm_gini: 0.82168 - norm_gini_val: 0.71636          \n",
      "Epoch 129/1000\n",
      " - 9s - loss: 0.2190 - acc: 0.9190 - val_loss: 0.2477 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91156\n",
      "roc_auc: 0.911 - roc_auc_val: 0.85822 - norm_gini: 0.82199 - norm_gini_val: 0.71644          \n",
      "Epoch 130/1000\n",
      " - 8s - loss: 0.2182 - acc: 0.9192 - val_loss: 0.2462 - val_acc: 0.9116\n",
      "\n",
      "Epoch 00130: val_acc improved from 0.91156 to 0.91159, saving model to model_stacking.h5\n",
      "roc_auc: 0.91112 - roc_auc_val: 0.85833 - norm_gini: 0.82224 - norm_gini_val: 0.71666          \n",
      "Epoch 131/1000\n",
      " - 8s - loss: 0.2189 - acc: 0.9175 - val_loss: 0.2470 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91144 - roc_auc_val: 0.85821 - norm_gini: 0.82287 - norm_gini_val: 0.71641          \n",
      "Epoch 132/1000\n",
      " - 7s - loss: 0.2183 - acc: 0.9189 - val_loss: 0.2469 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91175 - roc_auc_val: 0.85821 - norm_gini: 0.82349 - norm_gini_val: 0.71643          \n",
      "Epoch 133/1000\n",
      " - 8s - loss: 0.2196 - acc: 0.9176 - val_loss: 0.2467 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91182 - roc_auc_val: 0.85824 - norm_gini: 0.82365 - norm_gini_val: 0.71647          \n",
      "Epoch 134/1000\n",
      " - 8s - loss: 0.2178 - acc: 0.9196 - val_loss: 0.2477 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91216 - roc_auc_val: 0.85829 - norm_gini: 0.82432 - norm_gini_val: 0.71659          \n",
      "Epoch 135/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 7s - loss: 0.2181 - acc: 0.9181 - val_loss: 0.2474 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.9122 - roc_auc_val: 0.85826 - norm_gini: 0.8244 - norm_gini_val: 0.71652          \n",
      "Epoch 136/1000\n",
      " - 8s - loss: 0.2171 - acc: 0.9188 - val_loss: 0.2480 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91265 - roc_auc_val: 0.85821 - norm_gini: 0.82529 - norm_gini_val: 0.71641          \n",
      "Epoch 137/1000\n",
      " - 8s - loss: 0.2188 - acc: 0.9171 - val_loss: 0.2473 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91282 - roc_auc_val: 0.85822 - norm_gini: 0.82563 - norm_gini_val: 0.71643          \n",
      "Epoch 138/1000\n",
      " - 8s - loss: 0.2187 - acc: 0.9185 - val_loss: 0.2479 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91315 - roc_auc_val: 0.85816 - norm_gini: 0.82631 - norm_gini_val: 0.71632          \n",
      "Epoch 139/1000\n",
      " - 7s - loss: 0.2178 - acc: 0.9186 - val_loss: 0.2478 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91336 - roc_auc_val: 0.85826 - norm_gini: 0.82671 - norm_gini_val: 0.71652          \n",
      "Epoch 140/1000\n",
      " - 10s - loss: 0.2167 - acc: 0.9187 - val_loss: 0.2482 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91348 - roc_auc_val: 0.8584 - norm_gini: 0.82696 - norm_gini_val: 0.7168          \n",
      "Epoch 141/1000\n",
      " - 11s - loss: 0.2176 - acc: 0.9192 - val_loss: 0.2480 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91363 - roc_auc_val: 0.85845 - norm_gini: 0.82727 - norm_gini_val: 0.71689          \n",
      "Epoch 142/1000\n",
      " - 10s - loss: 0.2163 - acc: 0.9182 - val_loss: 0.2465 - val_acc: 0.9116\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91369 - roc_auc_val: 0.8584 - norm_gini: 0.82737 - norm_gini_val: 0.7168          \n",
      "Epoch 143/1000\n",
      " - 9s - loss: 0.2172 - acc: 0.9190 - val_loss: 0.2481 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91401 - roc_auc_val: 0.85842 - norm_gini: 0.82803 - norm_gini_val: 0.71684          \n",
      "Epoch 144/1000\n",
      " - 9s - loss: 0.2177 - acc: 0.9181 - val_loss: 0.2467 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91427 - roc_auc_val: 0.85836 - norm_gini: 0.82853 - norm_gini_val: 0.71672          \n",
      "Epoch 145/1000\n",
      " - 9s - loss: 0.2161 - acc: 0.9188 - val_loss: 0.2504 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91482 - roc_auc_val: 0.85826 - norm_gini: 0.82963 - norm_gini_val: 0.71652          \n",
      "Epoch 146/1000\n",
      " - 9s - loss: 0.2166 - acc: 0.9188 - val_loss: 0.2484 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91489 - roc_auc_val: 0.85831 - norm_gini: 0.82978 - norm_gini_val: 0.71661          \n",
      "Epoch 147/1000\n",
      " - 10s - loss: 0.2154 - acc: 0.9196 - val_loss: 0.2478 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91491 - roc_auc_val: 0.85847 - norm_gini: 0.82983 - norm_gini_val: 0.71694          \n",
      "Epoch 148/1000\n",
      " - 9s - loss: 0.2175 - acc: 0.9191 - val_loss: 0.2480 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91524 - roc_auc_val: 0.85839 - norm_gini: 0.83048 - norm_gini_val: 0.71678          \n",
      "Epoch 149/1000\n",
      " - 9s - loss: 0.2157 - acc: 0.9185 - val_loss: 0.2474 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91545 - roc_auc_val: 0.85853 - norm_gini: 0.83089 - norm_gini_val: 0.71706          \n",
      "Epoch 150/1000\n",
      " - 9s - loss: 0.2159 - acc: 0.9191 - val_loss: 0.2494 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91573 - roc_auc_val: 0.85847 - norm_gini: 0.83146 - norm_gini_val: 0.71695          \n",
      "Epoch 151/1000\n",
      " - 9s - loss: 0.2154 - acc: 0.9184 - val_loss: 0.2484 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91598 - roc_auc_val: 0.85828 - norm_gini: 0.83196 - norm_gini_val: 0.71656          \n",
      "Epoch 152/1000\n",
      " - 8s - loss: 0.2146 - acc: 0.9195 - val_loss: 0.2475 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91606 - roc_auc_val: 0.85844 - norm_gini: 0.83212 - norm_gini_val: 0.71688          \n",
      "Epoch 153/1000\n",
      " - 8s - loss: 0.2145 - acc: 0.9192 - val_loss: 0.2477 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91631 - roc_auc_val: 0.85852 - norm_gini: 0.83262 - norm_gini_val: 0.71705          \n",
      "Epoch 154/1000\n",
      " - 8s - loss: 0.2151 - acc: 0.9190 - val_loss: 0.2476 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91647 - roc_auc_val: 0.85844 - norm_gini: 0.83294 - norm_gini_val: 0.71688          \n",
      "Epoch 155/1000\n",
      " - 8s - loss: 0.2144 - acc: 0.9189 - val_loss: 0.2487 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91685 - roc_auc_val: 0.85842 - norm_gini: 0.8337 - norm_gini_val: 0.71683          \n",
      "Epoch 156/1000\n",
      " - 8s - loss: 0.2141 - acc: 0.9197 - val_loss: 0.2483 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91697 - roc_auc_val: 0.85841 - norm_gini: 0.83395 - norm_gini_val: 0.71683          \n",
      "Epoch 157/1000\n",
      " - 8s - loss: 0.2132 - acc: 0.9208 - val_loss: 0.2493 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91723 - roc_auc_val: 0.85845 - norm_gini: 0.83446 - norm_gini_val: 0.7169          \n",
      "Epoch 158/1000\n",
      " - 8s - loss: 0.2137 - acc: 0.9199 - val_loss: 0.2500 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91768 - roc_auc_val: 0.85841 - norm_gini: 0.83536 - norm_gini_val: 0.71682          \n",
      "Epoch 159/1000\n",
      " - 7s - loss: 0.2143 - acc: 0.9193 - val_loss: 0.2502 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91779 - roc_auc_val: 0.85831 - norm_gini: 0.83557 - norm_gini_val: 0.71662          \n",
      "Epoch 160/1000\n",
      " - 8s - loss: 0.2148 - acc: 0.9195 - val_loss: 0.2504 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91816 - roc_auc_val: 0.85824 - norm_gini: 0.83633 - norm_gini_val: 0.71648          \n",
      "Epoch 161/1000\n",
      " - 8s - loss: 0.2122 - acc: 0.9209 - val_loss: 0.2501 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91821 - roc_auc_val: 0.85829 - norm_gini: 0.83642 - norm_gini_val: 0.71659          \n",
      "Epoch 162/1000\n",
      " - 7s - loss: 0.2132 - acc: 0.9202 - val_loss: 0.2502 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91849 - roc_auc_val: 0.85823 - norm_gini: 0.83698 - norm_gini_val: 0.71647          \n",
      "Epoch 163/1000\n",
      " - 8s - loss: 0.2124 - acc: 0.9208 - val_loss: 0.2493 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91859 - roc_auc_val: 0.85844 - norm_gini: 0.83719 - norm_gini_val: 0.71689          \n",
      "Epoch 164/1000\n",
      " - 8s - loss: 0.2140 - acc: 0.9200 - val_loss: 0.2498 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.9188 - roc_auc_val: 0.85829 - norm_gini: 0.8376 - norm_gini_val: 0.71657          \n",
      "Epoch 165/1000\n",
      " - 8s - loss: 0.2131 - acc: 0.9211 - val_loss: 0.2490 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91902 - roc_auc_val: 0.85837 - norm_gini: 0.83805 - norm_gini_val: 0.71674          \n",
      "Epoch 166/1000\n",
      " - 7s - loss: 0.2129 - acc: 0.9208 - val_loss: 0.2499 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91937 - roc_auc_val: 0.85838 - norm_gini: 0.83874 - norm_gini_val: 0.71676          \n",
      "Epoch 167/1000\n",
      " - 10s - loss: 0.2139 - acc: 0.9199 - val_loss: 0.2494 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91963 - roc_auc_val: 0.85849 - norm_gini: 0.83927 - norm_gini_val: 0.71698          \n",
      "Epoch 168/1000\n",
      " - 10s - loss: 0.2123 - acc: 0.9199 - val_loss: 0.2502 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91989 - roc_auc_val: 0.8583 - norm_gini: 0.83978 - norm_gini_val: 0.71661          \n",
      "Epoch 169/1000\n",
      " - 9s - loss: 0.2127 - acc: 0.9204 - val_loss: 0.2503 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.92004 - roc_auc_val: 0.85827 - norm_gini: 0.84009 - norm_gini_val: 0.71653          \n",
      "Epoch 170/1000\n",
      " - 7s - loss: 0.2116 - acc: 0.9205 - val_loss: 0.2504 - val_acc: 0.9110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00170: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.92033 - roc_auc_val: 0.85839 - norm_gini: 0.84066 - norm_gini_val: 0.71678          \n",
      "Epoch 171/1000\n",
      " - 7s - loss: 0.2126 - acc: 0.9206 - val_loss: 0.2500 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.92038 - roc_auc_val: 0.85826 - norm_gini: 0.84077 - norm_gini_val: 0.71652          \n",
      "Epoch 172/1000\n",
      " - 8s - loss: 0.2101 - acc: 0.9219 - val_loss: 0.2511 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.9207 - roc_auc_val: 0.8582 - norm_gini: 0.8414 - norm_gini_val: 0.71639          \n",
      "Epoch 173/1000\n",
      " - 8s - loss: 0.2120 - acc: 0.9196 - val_loss: 0.2497 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.92082 - roc_auc_val: 0.85819 - norm_gini: 0.84163 - norm_gini_val: 0.71638          \n",
      "Epoch 174/1000\n",
      " - 8s - loss: 0.2102 - acc: 0.9212 - val_loss: 0.2504 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.9211 - roc_auc_val: 0.85822 - norm_gini: 0.8422 - norm_gini_val: 0.71644          \n",
      "Epoch 175/1000\n",
      " - 8s - loss: 0.2109 - acc: 0.9200 - val_loss: 0.2499 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.92128 - roc_auc_val: 0.85807 - norm_gini: 0.84255 - norm_gini_val: 0.71615          \n",
      "Epoch 176/1000\n",
      " - 7s - loss: 0.2107 - acc: 0.9208 - val_loss: 0.2509 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.92158 - roc_auc_val: 0.85805 - norm_gini: 0.84315 - norm_gini_val: 0.71611          \n",
      "Epoch 177/1000\n",
      " - 7s - loss: 0.2113 - acc: 0.9205 - val_loss: 0.2512 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.92196 - roc_auc_val: 0.85811 - norm_gini: 0.84391 - norm_gini_val: 0.71621          \n",
      "Epoch 178/1000\n",
      " - 9s - loss: 0.2116 - acc: 0.9208 - val_loss: 0.2517 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.9222 - roc_auc_val: 0.8581 - norm_gini: 0.84441 - norm_gini_val: 0.7162          \n",
      "Epoch 179/1000\n",
      " - 8s - loss: 0.2098 - acc: 0.9215 - val_loss: 0.2519 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.92244 - roc_auc_val: 0.85812 - norm_gini: 0.84489 - norm_gini_val: 0.71624          \n",
      "Epoch 180/1000\n",
      " - 8s - loss: 0.2111 - acc: 0.9209 - val_loss: 0.2520 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.91159\n",
      "roc_auc: 0.91112 - roc_auc_val: 0.85833 - norm_gini: 0.82224 - norm_gini_val: 0.71666          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x185c1f7ce10>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(PartA_X, PartA_Y,validation_data=(PartB_X, PartB_Y), epochs=epochs, batch_size=100,verbose=2,\n",
    "          callbacks=[checkpoint,earlystopping,roc_auc],class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_PartB_NN = model.predict(PartB_X)\n",
    "y_pred_PartC_NN = model.predict(PartC_X)\n",
    "y_pred_Xtest_NN = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartB_Y_after_XGB_NN['NN_predict_B']=y_pred_PartB_NN\n",
    "PartC_Y_after_XGB_NN['NN_predict_C']=y_pred_PartC_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Xtest_XGB=pd.DataFrame(y_pred_Xtest_XGB,columns=['Xtest_XGB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Xtest_XGB['NN_predict_X_test']=y_pred_Xtest_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>XGB_predict_C</th>\n",
       "      <th>NN_predict_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29667</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.003368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target  XGB_predict_C  NN_predict_C\n",
       "29667       0       0.000016      0.003368"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PartC_Y_after_XGB_NN.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PartB_Y_after_XGB_NN\n",
    "# PartC_Y_after_XGB_NN\n",
    "# y_pred_Xtest_XGB\n",
    "\n",
    "part_B_target=PartB_Y_after_XGB_NN['target']\n",
    "part_C_target=PartC_Y_after_XGB_NN['target']\n",
    "part_B_X=PartB_Y_after_XGB_NN.drop('target',axis=1,inplace=False)\n",
    "part_C_X=PartC_Y_after_XGB_NN.drop('target',axis=1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xtest_XGB</th>\n",
       "      <th>NN_predict_X_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.103653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.073271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.015402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.146464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.065677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Xtest_XGB  NN_predict_X_test\n",
       "0   0.000443           0.103653\n",
       "1   0.001387           0.073271\n",
       "2   0.013989           0.015402\n",
       "3   0.019364           0.146464\n",
       "4   0.000307           0.065677"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_Xtest_XGB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd level models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd level models\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(200))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights2 = class_weight.compute_class_weight('balanced',np.unique(part_B_target),part_B_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55617353, 4.95049505])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
